diff --git a/drivers/net/ethernet/marvell/Kconfig b/drivers/net/ethernet/marvell/Kconfig
index fb94216..ffec047 100644
--- a/drivers/net/ethernet/marvell/Kconfig
+++ b/drivers/net/ethernet/marvell/Kconfig
@@ -170,5 +170,6 @@ config SKY2_DEBUG
 
 
 source "drivers/net/ethernet/marvell/octeontx2/Kconfig"
+source "drivers/net/ethernet/marvell/prestera_sw/Kconfig"
 
 endif # NET_VENDOR_MARVELL
diff --git a/drivers/net/ethernet/marvell/Makefile b/drivers/net/ethernet/marvell/Makefile
index 89dea72..b7b5a81 100644
--- a/drivers/net/ethernet/marvell/Makefile
+++ b/drivers/net/ethernet/marvell/Makefile
@@ -11,4 +11,5 @@ obj-$(CONFIG_MVPP2) += mvpp2/
 obj-$(CONFIG_PXA168_ETH) += pxa168_eth.o
 obj-$(CONFIG_SKGE) += skge.o
 obj-$(CONFIG_SKY2) += sky2.o
+obj-$(CONFIG_MRVL_PRESTERA_SW) += prestera_sw/
 obj-y		+= octeontx2/
diff --git a/drivers/net/ethernet/marvell/prestera_sw/Kconfig b/drivers/net/ethernet/marvell/prestera_sw/Kconfig
new file mode 100644
index 0000000..2d7eefe
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/Kconfig
@@ -0,0 +1,38 @@
+# SPDX-License-Identifier: GPL-2.0-only
+#
+# Marvell switch drivers configuration
+#
+
+config MRVL_PRESTERA_SW
+	tristate "Marvell Technologies Prestera switchdev support"
+	depends on NET_SWITCHDEV
+	depends on BRIDGE
+	default m
+	---help---
+	  This driver supports Marvell Technologies Prestera Switchdev
+	  Ethernet Switch ASICs.
+
+	  To compile this driver as a module, choose M here: the
+	  module will be called prestera_sw.
+
+config MRVL_PRESTERA_NL
+	tristate "Marvell's Prestera Switchdev IPC Netlink kernel module"
+	depends on MRVL_PRESTERA_SW
+	default m
+	---help---
+	  This driver supports Marvell's Prestera Switchdev IPC
+          Netlink kernel module
+
+	  To compile this driver as a module, choose M here: the
+	  module will be called prestera_nl.
+
+config MRVL_PRESTERA_PCI
+	tristate "Marvell's Prestera Switchdev IPC PCI kernel module"
+	depends on MRVL_PRESTERA_SW
+	default m
+	---help---
+	  This driver supports Marvell's Prestera Switchdev IPC
+          PCI kernel module
+
+	  To compile this driver as a module, choose M here: the
+	  module will be called prestera_nl.
diff --git a/drivers/net/ethernet/marvell/prestera_sw/Makefile b/drivers/net/ethernet/marvell/prestera_sw/Makefile
new file mode 100644
index 0000000..dc37999
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/Makefile
@@ -0,0 +1,24 @@
+# SPDX-License-Identifier: GPL-2.0
+#
+# Makefile for the Marvell Switch driver.
+#
+
+obj-$(CONFIG_MRVL_PRESTERA_SW) += prestera_sw.o
+prestera_sw-objs := prestera.o \
+	prestera_hw.o prestera_switchdev.o prestera_fw_log.o \
+	prestera_rxtx.o prestera_rxtx_eth.o prestera_rxtx_mvpp.o \
+	prestera_dsa.o
+
+prestera_sw-$(CONFIG_MRVL_PRESTERA_DEBUG) += prestera_log.o
+ccflags-$(CONFIG_MRVL_PRESTERA_DEBUG) += -DCONFIG_MRVL_PRESTERA_DEBUG
+
+obj-$(CONFIG_MRVL_PRESTERA_NL) += prestera_nl.o
+prestera_nl-objs := netlink.o
+
+obj-$(CONFIG_MRVL_PRESTERA_PCI) += prestera_pci.o
+
+# testing infrastructure
+obj-$(CONFIG_MRVL_PRESTERA_TESTS) += prestera_tests.o
+prestera_tests-objs := tests/prestera_tests.o tests/prestera_ipc_tests.o
+CFLAGS_prestera_ipc_tests.o += -I$(src)
+CFLAGS_prestera_tests.o += -I$(src) -I$(src)/tests
diff --git a/drivers/net/ethernet/marvell/prestera_sw/netlink.c b/drivers/net/ethernet/marvell/prestera_sw/netlink.c
new file mode 100644
index 0000000..5cc43ee
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/netlink.c
@@ -0,0 +1,399 @@
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+
+#include <linux/module.h>
+#include <linux/netlink.h>
+#include <linux/version.h>
+#include <net/genetlink.h>
+#include <linux/completion.h>
+#include <linux/types.h>
+
+#include "prestera.h"
+#include "prestera_hw.h"
+#include "prestera_log.h"
+
+/* Protects send_sync function */
+static DEFINE_MUTEX(mvsw_nl_send_mtx);
+static DECLARE_COMPLETION(mvsw_nl_receive_cmpl);
+
+#define NL_SYNC_TIMEOUT 500000
+
+#define MVSW_NL_MCGRP_NAME	"pgrp_nl"
+#define MVSW_NL_MCGRP_ID	0
+
+#define MVSW_NL_NAME    "prestera_nl"
+#define MVSW_NL_VERSION 0x1
+
+enum {
+	MVSW_NL_TYPE_NEW_MSG,
+	MVSW_NL_TYPE_REPLY,
+	MVSW_NL_TYPE_READY,
+	MVSW_NL_TYPE_EVENT,
+	MVSW_NL_TYPE_MAX,
+};
+
+enum {
+	MVSW_NL_ATTR_UNSPEC,
+	MVSW_NL_ATTR_TLV,
+	MVSW_NL_ATTR_READY_ACK,
+	__MVSW_NL_ATTR_MAX,
+	MVSW_NL_ATTR_MAX = __MVSW_NL_ATTR_MAX - 1
+};
+
+static struct work_struct nl_bus_work;
+
+/* TODO: change to use list in case of supporting more than 1 switch device */
+static struct mvsw_pr_device *dev;
+
+static int _reply_errno;
+static int _reply_msg_size;
+static u8 _reply_msg[MVSW_MSG_MAX_SIZE];
+
+static const struct genl_multicast_group mvsw_genl_mcgrps[] = {
+	[MVSW_NL_MCGRP_ID] = {.name = MVSW_NL_MCGRP_NAME}
+};
+
+static const struct nla_policy mvsw_nl_policy[__MVSW_NL_ATTR_MAX] = {
+	[MVSW_NL_ATTR_UNSPEC] = {.type = NLA_UNSPEC},
+	[MVSW_NL_ATTR_TLV] = {.type = NLA_UNSPEC},
+};
+
+static int mvsw_nl_handle_reply(struct sk_buff *skb, struct genl_info *info);
+static int mvsw_nl_handle_ready(struct sk_buff *skb, struct genl_info *info);
+static int mvsw_nl_handle_event(struct sk_buff *skb, struct genl_info *info);
+
+static const struct genl_ops mvsw_genl_ops[] = {
+	{
+	 .cmd = MVSW_NL_TYPE_REPLY,
+	 .validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,
+	 .doit = mvsw_nl_handle_reply},
+	{
+	 .cmd = MVSW_NL_TYPE_READY,
+	 .validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,
+	 .doit = mvsw_nl_handle_ready},
+	{
+	 .cmd = MVSW_NL_TYPE_EVENT,
+	 .validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,
+	 .doit = mvsw_nl_handle_event},
+};
+
+static struct genl_family mvsw_genl_family = {
+	.name = MVSW_NL_NAME,
+	.version = MVSW_NL_VERSION,
+	.maxattr = MVSW_NL_ATTR_MAX,
+	.module = THIS_MODULE,
+	.ops = mvsw_genl_ops,
+	.n_ops = ARRAY_SIZE(mvsw_genl_ops),
+	.netnsok = true,
+	.mcgrps = mvsw_genl_mcgrps,
+	.n_mcgrps = ARRAY_SIZE(mvsw_genl_mcgrps),
+	.policy = mvsw_nl_policy,
+};
+
+static int mvsw_nl_handle_reply(struct sk_buff *skb, struct genl_info *info)
+{
+	size_t size;
+
+	if (!info->attrs[MVSW_NL_ATTR_TLV]) {
+		MVSW_LOG_ERROR("Reply msg does not containt required nl attr");
+		_reply_errno = -EINVAL;
+		goto handler_reply_unlock;
+	}
+
+	size = nla_len(info->attrs[MVSW_NL_ATTR_TLV]);
+	if (size >= MVSW_MSG_MAX_SIZE) {
+		MVSW_LOG_ERROR("NL recv buffer overflow");
+		_reply_errno = -EMSGSIZE;
+		goto handler_reply_unlock;
+	}
+
+	nla_memcpy(_reply_msg, info->attrs[MVSW_NL_ATTR_TLV], size);
+	_reply_msg_size = size;
+	_reply_errno = 0;
+
+handler_reply_unlock:
+	complete(&mvsw_nl_receive_cmpl);
+	return 0;
+}
+
+static int mvsw_nl_get_reply(u32 timeout, u8 *msg, size_t size,
+			     size_t *recv_bytes)
+{
+	int err = 0;
+
+	err = wait_for_completion_timeout(&mvsw_nl_receive_cmpl,
+					  usecs_to_jiffies(timeout));
+	if (!err) {
+		MVSW_LOG_ERROR("Reply msg is missed");
+		return err;
+	}
+
+	if (_reply_msg_size > size) {
+		MVSW_LOG_ERROR("Reply msg overflow");
+		return -EMSGSIZE;
+	}
+
+	memcpy(msg, _reply_msg, _reply_msg_size);
+	*recv_bytes = _reply_msg_size;
+	err = _reply_errno;
+
+	return err;
+}
+
+static int mvsw_nl_send_sync(u32 timeout, u8 *in_msg, size_t in_size,
+			     u8 *out_msg, size_t out_size,
+			     size_t *out_data_size)
+{
+	int err = 0;
+	struct sk_buff *nl_msg;
+	void *hdr;
+
+	nl_msg = nlmsg_new(NLMSG_DEFAULT_SIZE, GFP_KERNEL);
+	if (!nl_msg) {
+		MVSW_LOG_ERROR("Failed to allocate a new nl msg");
+		return -ENOMEM;
+	}
+
+	hdr = genlmsg_put(nl_msg, 0, 0, &mvsw_genl_family, 0,
+			  MVSW_NL_TYPE_NEW_MSG);
+	if (!hdr) {
+		MVSW_LOG_ERROR("Failed to put cmd to nl msg");
+		err = -EMSGSIZE;
+		goto nl_send_err;
+	}
+
+	err = nla_put(nl_msg, MVSW_NL_ATTR_TLV, in_size, in_msg);
+	if (err) {
+		MVSW_LOG_ERROR("Failed to put data to nl msg");
+		goto nl_send_err;
+	}
+
+	mutex_lock(&mvsw_nl_send_mtx);
+
+	genlmsg_end(nl_msg, hdr);
+
+	err = genlmsg_multicast(&mvsw_genl_family, nl_msg, 0,
+				MVSW_NL_MCGRP_ID, GFP_ATOMIC);
+	if (err) {
+		mutex_unlock(&mvsw_nl_send_mtx);
+		MVSW_LOG_ERROR("Failed to send genl msg");
+		return err;
+	}
+
+	err = mvsw_nl_get_reply(timeout, out_msg, out_size, out_data_size);
+	if (err)
+		MVSW_LOG_ERROR("Failed to receive msg reply");
+
+	mutex_unlock(&mvsw_nl_send_mtx);
+	return err;
+
+nl_send_err:
+	genlmsg_cancel(nl_msg, hdr);
+	nlmsg_free(nl_msg);
+	return err;
+}
+
+static int mvsw_nl_send_async(u8 *in_msg, size_t in_size)
+{
+	int err = 0;
+	struct sk_buff *nl_msg;
+	void *hdr;
+
+	nl_msg = nlmsg_new(NLMSG_DEFAULT_SIZE, GFP_KERNEL);
+	if (!nl_msg) {
+		MVSW_LOG_ERROR("Failed to allocate a new nl msg");
+		return -ENOMEM;
+	}
+
+	hdr = genlmsg_put(nl_msg, 0, 0, &mvsw_genl_family, 0,
+			  MVSW_NL_TYPE_NEW_MSG);
+	if (!hdr) {
+		MVSW_LOG_ERROR("Failed to put cmd to nl msg");
+		err = -EMSGSIZE;
+		goto async_nl_send_err;
+	}
+
+	err = nla_put(nl_msg, MVSW_NL_ATTR_TLV, in_size, in_msg);
+	if (err) {
+		MVSW_LOG_ERROR("Failed to put data to nl msg");
+		goto async_nl_send_err;
+	}
+
+	genlmsg_end(nl_msg, hdr);
+
+	err = genlmsg_multicast(&mvsw_genl_family, nl_msg, 0,
+				MVSW_NL_MCGRP_ID, GFP_ATOMIC);
+	if (err)
+		MVSW_LOG_ERROR("Failed to send genl msg");
+
+	return err;
+
+async_nl_send_err:
+	genlmsg_cancel(nl_msg, hdr);
+	nlmsg_free(nl_msg);
+	return err;
+}
+
+static int mvsw_nl_send_req(struct mvsw_pr_device *dev, int mode,
+			    u8 *in_msg, size_t in_size, u8 *out_msg,
+			    size_t out_size, size_t *out_data_size)
+{
+	int err = 0;
+
+	switch (mode) {
+	case MVSW_BUS_SEND_SYNC:
+		err =
+		    mvsw_nl_send_sync(dev->bus->timeout, in_msg, in_size,
+				      out_msg, out_size, out_data_size);
+		break;
+	case MVSW_BUS_SEND_ASYNC:
+		err = mvsw_nl_send_async(in_msg, in_size);
+		break;
+	default:
+		MVSW_LOG_ERROR("NL bus unsupported mode %d", mode);
+		return -EOPNOTSUPP;
+	}
+
+	return err;
+}
+
+static struct mvsw_pr_bus nl_bus = {
+	.type = "netlink",
+	.timeout = NL_SYNC_TIMEOUT,
+	.send_req = mvsw_nl_send_req,
+};
+
+static int mvsw_nl_bus_register(void)
+{
+	int err;
+
+	err = genl_register_family(&mvsw_genl_family);
+	if (err) {
+		pr_err("Failed to initialize netlink channel\n");
+		return err;
+	}
+
+	return err;
+}
+
+static void mvsw_nl_bus_unregister(void)
+{
+	genl_unregister_family(&mvsw_genl_family);
+	/* Check if some dev is registered */
+	if (dev) {
+		mvsw_pr_device_unregister(dev);
+		kfree(dev->dev);
+		kfree(dev);
+		dev = NULL;
+	}
+}
+
+static void nl_register_device_work(struct work_struct *work)
+{
+	struct device *nl_device = kzalloc(sizeof(*nl_device), GFP_KERNEL);
+
+	dev = kzalloc(sizeof(*dev), GFP_KERNEL);
+	dev->name = "aldrin";
+	dev->bus = &nl_bus;
+
+	nl_device->init_name = mvsw_genl_family.name;
+	dev->dev = nl_device;
+
+	if (mvsw_pr_device_register(dev)) {
+		pr_err("Failed registering prestera device\n");
+		kfree(dev->dev);
+		kfree(dev);
+		dev = NULL;
+	}
+}
+
+static int mvsw_nl_handle_ready(struct sk_buff *skb, struct genl_info *info)
+{
+	int err;
+	struct sk_buff *nl_msg;
+	void *hdr;
+
+	nl_msg = nlmsg_new(NLMSG_DEFAULT_SIZE, GFP_KERNEL);
+	if (!nl_msg) {
+		MVSW_LOG_ERROR("Failed to allocate a new nl msg");
+		return -ENOMEM;
+	}
+
+	hdr = genlmsg_put_reply(nl_msg, info, &mvsw_genl_family, 0,
+				MVSW_NL_TYPE_READY);
+	if (!hdr) {
+		MVSW_LOG_ERROR("Failed to put cmd to nl msg");
+		err = -EMSGSIZE;
+		goto ready_reply_err;
+	}
+
+	err = nla_put_flag(nl_msg, MVSW_NL_ATTR_READY_ACK);
+	if (err) {
+		MVSW_LOG_ERROR("Failed to put ready ack");
+		goto ready_reply_err;
+	}
+
+	genlmsg_end(nl_msg, hdr);
+	err = genlmsg_reply(nl_msg, info);
+
+	if (err) {
+		MVSW_LOG_ERROR("Failed to send ready reply");
+		return 0;
+	}
+
+	INIT_WORK(&nl_bus_work, nl_register_device_work);
+	schedule_work(&nl_bus_work);
+
+	return 0;
+
+ready_reply_err:
+	genlmsg_cancel(nl_msg, hdr);
+	nlmsg_free(nl_msg);
+	return 0;
+}
+
+static int mvsw_nl_handle_event(struct sk_buff *skb, struct genl_info *info)
+{
+	size_t len;
+
+	if (!dev || !dev->recv_msg)
+		return 0;
+
+	if (!info->attrs[MVSW_NL_ATTR_TLV]) {
+		MVSW_LOG_ERROR("Msg missing MVSW_NL_ATTR_TLV attr");
+		return -EINVAL;
+	}
+
+	len = nla_len(info->attrs[MVSW_NL_ATTR_TLV]);
+
+	return dev->recv_msg(dev, nla_data(info->attrs[MVSW_NL_ATTR_TLV]), len);
+}
+
+static int __init mvsw_pr_nl_init(void)
+{
+	int err;
+
+	pr_info("Loading Marvell Prestera Netlink Driver\n");
+
+	err = mvsw_nl_bus_register();
+	if (err)
+		pr_err("Loading Marvell Prestera Netlink Driver failed!");
+
+	return err;
+}
+
+static void __exit mvsw_pr_nl_exit(void)
+{
+	mvsw_nl_bus_unregister();
+	pr_info("Unloading Marvell Prestera Netlink Driver\n");
+}
+
+module_init(mvsw_pr_nl_init);
+module_exit(mvsw_pr_nl_exit);
+
+MODULE_AUTHOR("Marvell Semi.");
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("Marvell Prestera netlink driver");
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera.c b/drivers/net/ethernet/marvell/prestera_sw/prestera.c
new file mode 100644
index 0000000..d7e9c53
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera.c
@@ -0,0 +1,1665 @@
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/list.h>
+#include <linux/netdevice.h>
+#include <linux/netdev_features.h>
+#include <linux/etherdevice.h>
+#include <linux/ethtool.h>
+#include <linux/jiffies.h>
+#include <net/switchdev.h>
+#include "prestera.h"
+#include "prestera_hw.h"
+#include "prestera_log.h"
+#include "prestera_fw_log.h"
+#include "prestera_rxtx.h"
+#include "prestera_drv_ver.h"
+
+#define MVSW_PR_MTU_DEFAULT 1536
+
+#define PORT_STATS_CACHE_TIMEOUT_MS (msecs_to_jiffies(1000))
+
+static struct list_head switches_registered;
+
+static const char mvsw_driver_kind[] = "prestera_sw";
+static const char mvsw_driver_name[] = "mvsw_switchdev";
+static const char mvsw_driver_version[] = PRESTERA_DRV_VER;
+
+static const struct mvsw_fw_rev fw_rev = {
+	.major = 1,
+	.minor = 0,
+	.subminor = 1,
+};
+
+static struct workqueue_struct *mvsw_pr_wq;
+
+struct mvsw_pr_link_mode {
+	enum ethtool_link_mode_bit_indices eth_mode;
+	u32 speed;
+	u64 pr_mask;
+	u8 duplex;
+	u8 port_type;
+};
+
+static const struct mvsw_pr_link_mode
+mvsw_pr_link_modes[MVSW_LINK_MODE_MAX] = {
+	[MVSW_LINK_MODE_10baseT_Half_BIT] = {
+		.eth_mode =  ETHTOOL_LINK_MODE_10baseT_Half_BIT,
+		.speed = 10,
+		.pr_mask = 1 << MVSW_LINK_MODE_10baseT_Half_BIT,
+		.duplex = MVSW_PORT_DUPLEX_HALF,
+		.port_type = MVSW_PORT_TYPE_TP,
+	},
+	[MVSW_LINK_MODE_10baseT_Full_BIT] = {
+		.eth_mode =  ETHTOOL_LINK_MODE_10baseT_Full_BIT,
+		.speed = 10,
+		.pr_mask = 1 << MVSW_LINK_MODE_10baseT_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_TP,
+	},
+	[MVSW_LINK_MODE_100baseT_Half_BIT] = {
+		.eth_mode =  ETHTOOL_LINK_MODE_100baseT_Half_BIT,
+		.speed = 100,
+		.pr_mask = 1 << MVSW_LINK_MODE_100baseT_Half_BIT,
+		.duplex = MVSW_PORT_DUPLEX_HALF,
+		.port_type = MVSW_PORT_TYPE_TP,
+	},
+	[MVSW_LINK_MODE_100baseT_Full_BIT] = {
+		.eth_mode =  ETHTOOL_LINK_MODE_100baseT_Full_BIT,
+		.speed = 100,
+		.pr_mask = 1 << MVSW_LINK_MODE_100baseT_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_TP,
+	},
+	[MVSW_LINK_MODE_1000baseT_Half_BIT] = {
+		.eth_mode =  ETHTOOL_LINK_MODE_1000baseT_Half_BIT,
+		.speed = 1000,
+		.pr_mask = 1 << MVSW_LINK_MODE_1000baseT_Half_BIT,
+		.duplex = MVSW_PORT_DUPLEX_HALF,
+		.port_type = MVSW_PORT_TYPE_TP,
+	},
+	[MVSW_LINK_MODE_1000baseT_Full_BIT] = {
+		.eth_mode =  ETHTOOL_LINK_MODE_1000baseT_Full_BIT,
+		.speed = 1000,
+		.pr_mask = 1 << MVSW_LINK_MODE_1000baseT_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_TP,
+	},
+	[MVSW_LINK_MODE_1000baseX_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_1000baseX_Full_BIT,
+		.speed = 1000,
+		.pr_mask = 1 << MVSW_LINK_MODE_1000baseX_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_FIBRE,
+	},
+	[MVSW_LINK_MODE_1000baseKX_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_1000baseKX_Full_BIT,
+		.speed = 1000,
+		.pr_mask = 1 << MVSW_LINK_MODE_1000baseKX_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_TP,
+	},
+	[MVSW_LINK_MODE_10GbaseKR_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_10000baseKR_Full_BIT,
+		.speed = 10000,
+		.pr_mask = 1 << MVSW_LINK_MODE_10GbaseKR_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_TP,
+	},
+	[MVSW_LINK_MODE_10GbaseSR_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_10000baseSR_Full_BIT,
+		.speed = 10000,
+		.pr_mask = 1 << MVSW_LINK_MODE_10GbaseSR_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_FIBRE,
+	},
+	[MVSW_LINK_MODE_10GbaseLR_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_10000baseLR_Full_BIT,
+		.speed = 10000,
+		.pr_mask = 1 << MVSW_LINK_MODE_10GbaseLR_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_FIBRE,
+	},
+	[MVSW_LINK_MODE_20GbaseKR2_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_20000baseKR2_Full_BIT,
+		.speed = 20000,
+		.pr_mask = 1 << MVSW_LINK_MODE_20GbaseKR2_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_TP,
+	},
+	[MVSW_LINK_MODE_25GbaseCR_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_25000baseCR_Full_BIT,
+		.speed = 25000,
+		.pr_mask = 1 << MVSW_LINK_MODE_25GbaseCR_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_DA,
+	},
+	[MVSW_LINK_MODE_25GbaseKR_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_25000baseKR_Full_BIT,
+		.speed = 25000,
+		.pr_mask = 1 << MVSW_LINK_MODE_25GbaseKR_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_TP,
+	},
+	[MVSW_LINK_MODE_25GbaseSR_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_25000baseSR_Full_BIT,
+		.speed = 25000,
+		.pr_mask = 1 << MVSW_LINK_MODE_25GbaseSR_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_FIBRE,
+	},
+	[MVSW_LINK_MODE_40GbaseKR4_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_40000baseKR4_Full_BIT,
+		.speed = 40000,
+		.pr_mask = 1 << MVSW_LINK_MODE_40GbaseKR4_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_TP,
+	},
+	[MVSW_LINK_MODE_40GbaseCR4_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_40000baseCR4_Full_BIT,
+		.speed = 40000,
+		.pr_mask = 1 << MVSW_LINK_MODE_40GbaseCR4_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_DA,
+	},
+	[MVSW_LINK_MODE_40GbaseSR4_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_40000baseSR4_Full_BIT,
+		.speed = 40000,
+		.pr_mask = 1 << MVSW_LINK_MODE_40GbaseSR4_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_FIBRE,
+	},
+	[MVSW_LINK_MODE_50GbaseCR2_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_50000baseCR2_Full_BIT,
+		.speed = 50000,
+		.pr_mask = 1 << MVSW_LINK_MODE_50GbaseCR2_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_DA,
+	},
+	[MVSW_LINK_MODE_50GbaseKR2_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_50000baseKR2_Full_BIT,
+		.speed = 50000,
+		.pr_mask = 1 << MVSW_LINK_MODE_50GbaseKR2_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_TP,
+	},
+	[MVSW_LINK_MODE_50GbaseSR2_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_50000baseSR2_Full_BIT,
+		.speed = 50000,
+		.pr_mask = 1 << MVSW_LINK_MODE_50GbaseSR2_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_FIBRE,
+	},
+	[MVSW_LINK_MODE_100GbaseKR4_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_100000baseKR4_Full_BIT,
+		.speed = 100000,
+		.pr_mask = 1 << MVSW_LINK_MODE_100GbaseKR4_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_TP,
+	},
+	[MVSW_LINK_MODE_100GbaseSR4_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_100000baseSR4_Full_BIT,
+		.speed = 100000,
+		.pr_mask = 1 << MVSW_LINK_MODE_100GbaseSR4_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_FIBRE,
+	},
+	[MVSW_LINK_MODE_100GbaseCR4_Full_BIT] = {
+		.eth_mode = ETHTOOL_LINK_MODE_100000baseCR4_Full_BIT,
+		.speed = 100000,
+		.pr_mask = 1 << MVSW_LINK_MODE_100GbaseCR4_Full_BIT,
+		.duplex = MVSW_PORT_DUPLEX_FULL,
+		.port_type = MVSW_PORT_TYPE_DA,
+	}
+};
+
+struct mvsw_pr_fec {
+	u32 eth_fec;
+	enum ethtool_link_mode_bit_indices eth_mode;
+	u8 pr_fec;
+};
+
+static const struct mvsw_pr_fec mvsw_pr_fec_caps[MVSW_PORT_FEC_MAX] = {
+	[MVSW_PORT_FEC_OFF_BIT] = {
+		.eth_fec = ETHTOOL_FEC_OFF,
+		.eth_mode = ETHTOOL_LINK_MODE_FEC_NONE_BIT,
+		.pr_fec = 1 << MVSW_PORT_FEC_OFF_BIT,
+	},
+	[MVSW_PORT_FEC_BASER_BIT] = {
+		.eth_fec = ETHTOOL_FEC_BASER,
+		.eth_mode = ETHTOOL_LINK_MODE_FEC_BASER_BIT,
+		.pr_fec = 1 << MVSW_PORT_FEC_BASER_BIT,
+	},
+	[MVSW_PORT_FEC_RS_BIT] = {
+		.eth_fec = ETHTOOL_FEC_RS,
+		.eth_mode = ETHTOOL_LINK_MODE_FEC_RS_BIT,
+		.pr_fec = 1 << MVSW_PORT_FEC_RS_BIT,
+	}
+};
+
+struct mvsw_pr_port_type {
+	enum ethtool_link_mode_bit_indices eth_mode;
+	u8 eth_type;
+};
+
+static const struct mvsw_pr_port_type
+mvsw_pr_port_types[MVSW_PORT_TYPE_MAX] = {
+	[MVSW_PORT_TYPE_NONE] = {
+		.eth_mode = __ETHTOOL_LINK_MODE_MASK_NBITS,
+		.eth_type = PORT_NONE,
+	},
+	[MVSW_PORT_TYPE_TP] = {
+		.eth_mode = ETHTOOL_LINK_MODE_TP_BIT,
+		.eth_type = PORT_TP,
+	},
+	[MVSW_PORT_TYPE_AUI] = {
+		.eth_mode = ETHTOOL_LINK_MODE_AUI_BIT,
+		.eth_type = PORT_AUI,
+	},
+	[MVSW_PORT_TYPE_MII] = {
+		.eth_mode = ETHTOOL_LINK_MODE_MII_BIT,
+		.eth_type = PORT_MII,
+	},
+	[MVSW_PORT_TYPE_FIBRE] = {
+		.eth_mode = ETHTOOL_LINK_MODE_FIBRE_BIT,
+		.eth_type = PORT_FIBRE,
+	},
+	[MVSW_PORT_TYPE_BNC] = {
+		.eth_mode = ETHTOOL_LINK_MODE_BNC_BIT,
+		.eth_type = PORT_BNC,
+	},
+	[MVSW_PORT_TYPE_DA] = {
+		.eth_mode = ETHTOOL_LINK_MODE_TP_BIT,
+		.eth_type = PORT_TP,
+	},
+	[MVSW_PORT_TYPE_OTHER] = {
+		.eth_mode = __ETHTOOL_LINK_MODE_MASK_NBITS,
+		.eth_type = PORT_OTHER,
+	}
+};
+
+static const char mvsw_pr_port_cnt_name[MVSW_PORT_CNT_MAX][ETH_GSTRING_LEN] = {
+	[MVSW_PORT_GOOD_OCTETS_RCV_CNT] = {"good_octets_received"},
+	[MVSW_PORT_BAD_OCTETS_RCV_CNT] = {"bad_octets_received"},
+	[MVSW_PORT_MAC_TRANSMIT_ERR_CNT] = {"mac_trans_error"},
+	[MVSW_PORT_BRDC_PKTS_RCV_CNT] = {"broadcast_frames_received"},
+	[MVSW_PORT_MC_PKTS_RCV_CNT] = {"multicast_frames_received"},
+	[MVSW_PORT_PKTS_64_OCTETS_CNT] = {"frames_64_octets"},
+	[MVSW_PORT_PKTS_65TO127_OCTETS_CNT] = {"frames_65_to_127_octets"},
+	[MVSW_PORT_PKTS_128TO255_OCTETS_CNT] = {"frames_128_to_255_octets"},
+	[MVSW_PORT_PKTS_256TO511_OCTETS_CNT] = {"frames_256_to_511_octets"},
+	[MVSW_PORT_PKTS_512TO1023_OCTETS_CNT] = {"frames_512_to_1023_octets"},
+	[MVSW_PORT_PKTS_1024TOMAX_OCTETS_CNT] = {"frames_1024_to_max_octets"},
+	[MVSW_PORT_EXCESSIVE_COLLISIONS_CNT] = {"excessive_collision"},
+	[MVSW_PORT_MC_PKTS_SENT_CNT] = {"multicast_frames_sent"},
+	[MVSW_PORT_BRDC_PKTS_SENT_CNT] = {"broadcast_frames_sent"},
+	[MVSW_PORT_FC_SENT_CNT] = {"fc_sent"},
+	[MVSW_PORT_GOOD_FC_RCV_CNT] = {"fc_received"},
+	[MVSW_PORT_DROP_EVENTS_CNT] = {"buffer_overrun"},
+	[MVSW_PORT_UNDERSIZE_PKTS_CNT] = {"undersize"},
+	[MVSW_PORT_FRAGMENTS_PKTS_CNT] = {"fragments"},
+	[MVSW_PORT_OVERSIZE_PKTS_CNT] = {"oversize"},
+	[MVSW_PORT_JABBER_PKTS_CNT] = {"jabber"},
+	[MVSW_PORT_MAC_RCV_ERROR_CNT] = {"rx_error_frame_received"},
+	[MVSW_PORT_BAD_CRC_CNT] = {"bad_crc"},
+	[MVSW_PORT_COLLISIONS_CNT] = {"collisions"},
+	[MVSW_PORT_LATE_COLLISIONS_CNT] = {"late_collision"},
+	[MVSW_PORT_GOOD_UC_PKTS_RCV_CNT] = {"unicast_frames_received"},
+	[MVSW_PORT_GOOD_UC_PKTS_SENT_CNT] = {"unicast_frames_sent"},
+	[MVSW_PORT_MULTIPLE_PKTS_SENT_CNT] = {"sent_multiple"},
+	[MVSW_PORT_DEFERRED_PKTS_SENT_CNT] = {"sent_deferred"},
+	[MVSW_PORT_PKTS_1024TO1518_OCTETS_CNT] = {"frames_1024_to_1518_octets"},
+	[MVSW_PORT_PKTS_1519TOMAX_OCTETS_CNT] = {"frames_1519_to_max_octets"},
+	[MVSW_PORT_GOOD_OCTETS_SENT_CNT] = {"good_octets_sent"}
+};
+
+static struct mvsw_pr_port *__find_pr_port(const struct mvsw_pr_switch *sw,
+					   u32 port_id)
+{
+	struct mvsw_pr_port *port;
+
+	list_for_each_entry(port, &sw->port_list, list) {
+		if (port->id == port_id)
+			return port;
+	}
+
+	return NULL;
+}
+
+static int mvsw_pr_port_state_set(struct net_device *dev, bool admin_state)
+{
+	int err;
+	struct mvsw_pr_port *port = netdev_priv(dev);
+
+	if (!admin_state)
+		netif_stop_queue(dev);
+
+	err = mvsw_pr_hw_port_state_set(port, admin_state);
+
+	if (admin_state && !err)
+		netif_start_queue(dev);
+
+	return err;
+}
+
+static int mvsw_pr_port_get_port_parent_id(struct net_device *dev,
+					   struct netdev_phys_item_id *ppid)
+{
+	const struct mvsw_pr_port *port = netdev_priv(dev);
+
+	ppid->id_len = sizeof(port->sw->id);
+	memcpy(&ppid->id, &port->sw->id, ppid->id_len);
+	return 0;
+}
+
+static int mvsw_pr_port_get_phys_port_name(struct net_device *dev,
+					   char *buf, size_t len)
+{
+	const struct mvsw_pr_port *port = netdev_priv(dev);
+
+	snprintf(buf, len, "%u", port->fp_id);
+	return 0;
+}
+
+static int mvsw_pr_port_open(struct net_device *dev)
+{
+	return mvsw_pr_port_state_set(dev, true);
+}
+
+static int mvsw_pr_port_close(struct net_device *dev)
+{
+	return mvsw_pr_port_state_set(dev, false);
+}
+
+static netdev_tx_t mvsw_pr_port_xmit(struct sk_buff *skb,
+				     struct net_device *dev)
+{
+	struct mvsw_pr_port *port = netdev_priv(dev);
+	struct mvsw_pr_rxtx_info rxtx_info = {
+		.port_id = port->id
+	};
+
+	return mvsw_pr_rxtx_xmit(skb, &rxtx_info);
+}
+
+static int mvsw_pr_setup_tc(struct net_device *dev, enum tc_setup_type type,
+			    void *type_data)
+{
+	/* TO DO: add implementation */
+	return 0;
+}
+
+static void mvsw_pr_set_rx_mode(struct net_device *dev)
+{
+	/* TO DO: add implementation */
+}
+
+static int mvsw_is_valid_mac_addr(struct mvsw_pr_port *port, u8 *addr)
+{
+	int err;
+
+	if (!is_valid_ether_addr(addr))
+		return -EADDRNOTAVAIL;
+
+	err = memcmp(port->sw->base_mac, addr, ETH_ALEN - 1);
+	if (err)
+		return -EINVAL;
+
+	return 0;
+}
+
+static int mvsw_pr_port_set_mac_address(struct net_device *dev, void *p)
+{
+	int err;
+	struct sockaddr *addr = p;
+	struct mvsw_pr_port *port = netdev_priv(dev);
+
+	err = mvsw_is_valid_mac_addr(port, addr->sa_data);
+	if (err)
+		return err;
+
+	err = mvsw_pr_hw_port_mac_set(port, addr->sa_data);
+	if (!err)
+		memcpy(dev->dev_addr, addr->sa_data, dev->addr_len);
+	return err;
+}
+
+static int mvsw_pr_port_change_mtu(struct net_device *dev, int mtu)
+{
+	int err;
+	struct mvsw_pr_port *port = netdev_priv(dev);
+
+	if (port->sw->mtu_min <= mtu && mtu <= port->sw->mtu_max)
+		err = mvsw_pr_hw_port_mtu_set(port, mtu);
+	else
+		err = -EINVAL;
+	if (!err)
+		dev->mtu = mtu;
+	return err;
+}
+
+static void mvsw_pr_port_get_stats64(struct net_device *dev,
+				     struct rtnl_link_stats64 *stats)
+{
+	struct mvsw_pr_port *port = netdev_priv(dev);
+	u64 *port_stats = port->cached_hw_stats.stats_arr;
+
+	stats->rx_packets = port_stats[MVSW_PORT_BRDC_PKTS_RCV_CNT] +
+				port_stats[MVSW_PORT_MC_PKTS_RCV_CNT] +
+				port_stats[MVSW_PORT_GOOD_UC_PKTS_RCV_CNT];
+
+	stats->tx_packets = port_stats[MVSW_PORT_BRDC_PKTS_SENT_CNT] +
+				port_stats[MVSW_PORT_MC_PKTS_SENT_CNT] +
+				port_stats[MVSW_PORT_GOOD_UC_PKTS_SENT_CNT];
+
+	stats->rx_bytes = port_stats[MVSW_PORT_GOOD_OCTETS_RCV_CNT];
+
+	stats->tx_bytes = port_stats[MVSW_PORT_GOOD_OCTETS_SENT_CNT];
+
+	stats->rx_errors = port_stats[MVSW_PORT_MAC_RCV_ERROR_CNT];
+	stats->tx_errors = port_stats[MVSW_PORT_MAC_TRANSMIT_ERR_CNT];
+
+	stats->rx_dropped = port_stats[MVSW_PORT_DROP_EVENTS_CNT];
+	stats->tx_dropped = 0;
+
+	stats->multicast = port_stats[MVSW_PORT_MC_PKTS_RCV_CNT];
+	stats->collisions = port_stats[MVSW_PORT_COLLISIONS_CNT];
+
+	stats->rx_crc_errors = port_stats[MVSW_PORT_BAD_CRC_CNT];
+}
+
+static void mvsw_pr_port_get_hw_stats64(struct mvsw_pr_port *port)
+{
+	u64 *port_stats = port->cached_hw_stats.stats_arr;
+	int err;
+
+	err = mvsw_pr_hw_port_stats_get
+		(port, port_stats, sizeof(*port_stats) * MVSW_PORT_CNT_MAX);
+	if (err) {
+		MVSW_LOG_ERROR("failed to get port stats");
+		return;
+	}
+}
+
+static void update_stats_cache(struct work_struct *work)
+{
+	struct mvsw_pr_port *port =
+		container_of(work, struct mvsw_pr_port,
+			     cached_hw_stats.caching_dw.work);
+
+	mvsw_pr_port_get_hw_stats64(port);
+
+	queue_delayed_work(mvsw_pr_wq, &port->cached_hw_stats.caching_dw,
+			   PORT_STATS_CACHE_TIMEOUT_MS);
+}
+
+static bool mvsw_pr_port_has_offload_stats(const struct net_device *dev,
+					   int attr_id)
+{
+	/* TO DO: add implementation */
+	return false;
+}
+
+static int mvsw_pr_port_get_offload_stats(int attr_id,
+					  const struct net_device *dev,
+					  void *sp)
+{
+	/* TO DO: add implementation */
+	return 0;
+}
+
+static int mvsw_pr_set_features(struct net_device *dev,
+				netdev_features_t features)
+{
+	/* TO DO: add implementation */
+	return 0;
+}
+
+static void mvsw_pr_port_get_drvinfo(struct net_device *dev,
+				     struct ethtool_drvinfo *drvinfo)
+{
+	struct mvsw_pr_port *port = netdev_priv(dev);
+	struct mvsw_pr_switch *sw = port->sw;
+
+	strlcpy(drvinfo->driver, mvsw_driver_kind, sizeof(drvinfo->driver));
+	strlcpy(drvinfo->bus_info, dev_name(sw->dev->dev),
+		sizeof(drvinfo->bus_info));
+	snprintf(drvinfo->fw_version, sizeof(drvinfo->fw_version),
+		 "%d.%d.%d",
+		 sw->info.fw_rev->major,
+		 sw->info.fw_rev->minor,
+		 sw->info.fw_rev->subminor);
+}
+
+static const struct net_device_ops mvsw_pr_netdev_ops = {
+	.ndo_open = mvsw_pr_port_open,
+	.ndo_stop = mvsw_pr_port_close,
+	.ndo_start_xmit = mvsw_pr_port_xmit,
+	.ndo_setup_tc = mvsw_pr_setup_tc,
+	.ndo_change_mtu = mvsw_pr_port_change_mtu,
+	.ndo_set_rx_mode = mvsw_pr_set_rx_mode,
+	.ndo_get_stats64 = mvsw_pr_port_get_stats64,
+	.ndo_set_features = mvsw_pr_set_features,
+	.ndo_set_mac_address = mvsw_pr_port_set_mac_address,
+	.ndo_has_offload_stats = mvsw_pr_port_has_offload_stats,
+	.ndo_get_offload_stats = mvsw_pr_port_get_offload_stats,
+	.ndo_get_phys_port_name = mvsw_pr_port_get_phys_port_name,
+	.ndo_get_port_parent_id = mvsw_pr_port_get_port_parent_id
+};
+
+bool mvsw_pr_netdev_check(const struct net_device *dev)
+{
+	return dev->netdev_ops == &mvsw_pr_netdev_ops;
+}
+
+static int mvsw_pr_lower_dev_walk(struct net_device *lower_dev, void *data)
+{
+	struct mvsw_pr_port **p_mvsw_pr_port = data;
+	int ret = 0;
+
+	if (mvsw_pr_netdev_check(lower_dev)) {
+		*p_mvsw_pr_port = netdev_priv(lower_dev);
+		ret = 1;
+	}
+
+	return ret;
+}
+
+struct mvsw_pr_port *mvsw_pr_port_dev_lower_find(struct net_device *dev)
+{
+	struct mvsw_pr_port *mvsw_pr_port;
+
+	if (mvsw_pr_netdev_check(dev))
+		return netdev_priv(dev);
+
+	mvsw_pr_port = NULL;
+	netdev_walk_all_lower_dev(dev, mvsw_pr_lower_dev_walk, &mvsw_pr_port);
+
+	return mvsw_pr_port;
+}
+
+struct mvsw_pr_switch *mvsw_pr_lower_get(struct net_device *dev)
+{
+	struct mvsw_pr_port *mvsw_pr_port;
+
+	mvsw_pr_port = mvsw_pr_port_dev_lower_find(dev);
+	return mvsw_pr_port ? mvsw_pr_port->sw : NULL;
+}
+
+static void mvsw_modes_to_eth(unsigned long *eth_modes, u64 link_modes, u8 fec,
+			      u8 type)
+{
+	u32 mode;
+
+	for (mode = 0; mode < MVSW_LINK_MODE_MAX; mode++) {
+		if ((mvsw_pr_link_modes[mode].pr_mask & link_modes) == 0)
+			continue;
+		if (type != MVSW_PORT_TYPE_NONE &&
+		    mvsw_pr_link_modes[mode].port_type != type)
+			continue;
+		__set_bit(mvsw_pr_link_modes[mode].eth_mode, eth_modes);
+	}
+
+	for (mode = 0; mode < MVSW_PORT_FEC_MAX; mode++) {
+		if ((mvsw_pr_fec_caps[mode].pr_fec & fec) == 0)
+			continue;
+		__set_bit(mvsw_pr_fec_caps[mode].eth_mode, eth_modes);
+	}
+}
+
+static void mvsw_modes_from_eth(const unsigned long *eth_modes, u64 *link_modes,
+				u8 *fec)
+{
+	u32 mode;
+
+	for (mode = 0; mode < MVSW_LINK_MODE_MAX; mode++) {
+		if (!test_bit(mvsw_pr_link_modes[mode].eth_mode, eth_modes))
+			continue;
+		*link_modes |= mvsw_pr_link_modes[mode].pr_mask;
+	}
+
+	for (mode = 0; mode < MVSW_PORT_FEC_MAX; mode++) {
+		if (!test_bit(mvsw_pr_fec_caps[mode].eth_mode, eth_modes))
+			continue;
+		*fec |= mvsw_pr_fec_caps[mode].pr_fec;
+	}
+}
+
+static void mvsw_pr_port_supp_types_get(struct ethtool_link_ksettings *ecmd,
+					struct mvsw_pr_port *port)
+{
+	u32 mode;
+	u8 ptype;
+
+	for (mode = 0; mode < MVSW_LINK_MODE_MAX; mode++) {
+		if ((mvsw_pr_link_modes[mode].pr_mask &
+		    port->supp_link_modes) == 0)
+			continue;
+		ptype = mvsw_pr_link_modes[mode].port_type;
+		__set_bit(mvsw_pr_port_types[ptype].eth_mode,
+			  ecmd->link_modes.supported);
+	}
+}
+
+static void mvsw_pr_port_speed_get(struct ethtool_link_ksettings *ecmd,
+				   struct mvsw_pr_port *port)
+{
+	int err;
+	u32 speed;
+
+	err = mvsw_pr_hw_port_speed_get(port, &speed);
+	ecmd->base.speed = !err ? speed : SPEED_UNKNOWN;
+}
+
+static int mvsw_pr_port_link_mode_set(struct mvsw_pr_port *port,
+				      u32 speed, u8 duplex, u8 type)
+{
+	int err;
+	u32 mode;
+	u32 new_mode = MVSW_LINK_MODE_MAX;
+
+	for (mode = 0; mode < MVSW_LINK_MODE_MAX; mode++) {
+		if (speed == mvsw_pr_link_modes[mode].speed &&
+		    duplex == mvsw_pr_link_modes[mode].duplex &&
+		    (mvsw_pr_link_modes[mode].pr_mask &
+		    port->supp_link_modes) &&
+		    type == mvsw_pr_link_modes[mode].port_type) {
+			new_mode = mode;
+			break;
+		}
+	}
+
+	if (new_mode == MVSW_LINK_MODE_MAX) {
+		pr_err("Unsupported speed/duplex requested");
+		return -EINVAL;
+	}
+
+	err = mvsw_pr_hw_port_link_mode_set(port, new_mode);
+
+	return err;
+}
+
+static int mvsw_pr_port_speed_duplex_set(const struct ethtool_link_ksettings
+					 *ecmd, struct mvsw_pr_port *port)
+{
+	int err;
+	u8 duplex;
+	u32 speed;
+	u32 curr_mode;
+
+	err = mvsw_pr_hw_port_link_mode_get(port, &curr_mode);
+	if (err || curr_mode >= MVSW_LINK_MODE_MAX)
+		return -EINVAL;
+
+	if (ecmd->base.duplex != DUPLEX_UNKNOWN)
+		duplex = ecmd->base.duplex == DUPLEX_FULL ?
+			 MVSW_PORT_DUPLEX_FULL : MVSW_PORT_DUPLEX_HALF;
+	else
+		duplex = mvsw_pr_link_modes[curr_mode].duplex;
+
+	if (ecmd->base.speed != SPEED_UNKNOWN)
+		speed = ecmd->base.speed;
+	else
+		speed = mvsw_pr_link_modes[curr_mode].speed;
+
+	return mvsw_pr_port_link_mode_set(port, speed, duplex, port->type);
+}
+
+static u8 mvsw_pr_port_type_get(struct mvsw_pr_port *port)
+{
+	if (port->type < MVSW_PORT_TYPE_MAX)
+		return mvsw_pr_port_types[port->type].eth_type;
+	return PORT_OTHER;
+}
+
+static int mvsw_pr_port_type_set(const struct ethtool_link_ksettings *ecmd,
+				 struct mvsw_pr_port *port)
+{
+	int err;
+	u32 type, mode;
+	u32 new_mode = MVSW_LINK_MODE_MAX;
+
+	for (type = 0; type < MVSW_PORT_TYPE_MAX; type++) {
+		if (mvsw_pr_port_types[type].eth_type == ecmd->base.port &&
+		    test_bit(mvsw_pr_port_types[type].eth_mode,
+			     ecmd->link_modes.supported)) {
+			break;
+		}
+	}
+
+	if (type == port->type)
+		return 0;
+
+	if (type == MVSW_PORT_TYPE_MAX) {
+		pr_err("Unsupported port type requested\n");
+		return -EINVAL;
+	}
+
+	for (mode = 0; mode < MVSW_LINK_MODE_MAX; mode++) {
+		if ((mvsw_pr_link_modes[mode].pr_mask &
+		    port->supp_link_modes) &&
+		    type == mvsw_pr_link_modes[mode].port_type) {
+			new_mode = mode;
+		}
+	}
+
+	if (new_mode < MVSW_LINK_MODE_MAX)
+		err = mvsw_pr_hw_port_link_mode_set(port, new_mode);
+	else
+		err = -EINVAL;
+
+	if (!err)
+		port->type = type;
+
+	return err;
+}
+
+static void mvsw_pr_port_remote_cap_get(struct ethtool_link_ksettings *ecmd,
+					struct mvsw_pr_port *port)
+{
+	int err;
+	u64 link_mode_bitmap;
+
+	err = mvsw_pr_hw_port_remote_cap_get(port, &link_mode_bitmap);
+	if (!err)
+		mvsw_modes_to_eth(ecmd->link_modes.lp_advertising,
+				  link_mode_bitmap, 0, MVSW_PORT_TYPE_NONE);
+}
+
+static void mvsw_pr_port_duplex_get(struct ethtool_link_ksettings *ecmd,
+				    struct mvsw_pr_port *port)
+{
+	int err;
+	u8 duplex;
+
+	err = mvsw_pr_hw_port_duplex_get(port, &duplex);
+	if (!err)
+		ecmd->base.duplex = duplex == MVSW_PORT_DUPLEX_FULL ?
+				    DUPLEX_FULL : DUPLEX_HALF;
+	else
+		ecmd->base.duplex = DUPLEX_UNKNOWN;
+}
+
+static int mvsw_pr_port_autoneg_set(struct mvsw_pr_port *port, bool enable,
+				    u64 link_modes, u8 fec)
+{
+	int err = 0;
+	bool refresh = false;
+
+	if (port->type != MVSW_PORT_TYPE_TP)
+		return enable ? -EINVAL : 0;
+
+	if (port->adver_link_modes != link_modes || port->adver_fec != fec) {
+		port->adver_link_modes = link_modes;
+		port->adver_fec = fec != 0 ? fec : 1 << MVSW_PORT_FEC_OFF_BIT;
+		refresh = true;
+	}
+
+	if (port->autoneg == enable && !(port->autoneg && refresh))
+		return 0;
+
+	err = mvsw_pr_hw_port_autoneg_set(port, enable,
+					  port->adver_link_modes,
+					  port->adver_fec);
+	if (err) {
+		MVSW_LOG_ERROR("Failed to configure autoneg [port=%u]",
+			       port->id);
+		return -EINVAL;
+	}
+
+	port->autoneg = enable;
+	return 0;
+}
+
+static void mvsw_pr_port_mdix_get(struct ethtool_link_ksettings *ecmd,
+				  struct mvsw_pr_port *port)
+{
+	int err;
+	u8 mode;
+
+	err = mvsw_pr_hw_port_mdix_get(port, &mode);
+	if (err)
+		return;
+
+	ecmd->base.eth_tp_mdix = mode;
+}
+
+static int mvsw_pr_port_mdix_set(const struct ethtool_link_ksettings *ecmd,
+				 struct mvsw_pr_port *port)
+{
+	if (ecmd->base.eth_tp_mdix_ctrl) {
+		/* TODO: For now setting mdix is not supported. */
+		/* Waiting for changes in mpd driver. */
+		return -EOPNOTSUPP;
+	}
+
+	return 0;
+}
+
+static int mvsw_pr_port_get_link_ksettings(struct net_device *dev,
+					   struct ethtool_link_ksettings *ecmd)
+{
+	struct mvsw_pr_port *port = netdev_priv(dev);
+
+	ethtool_link_ksettings_zero_link_mode(ecmd, supported);
+	ethtool_link_ksettings_zero_link_mode(ecmd, advertising);
+	ethtool_link_ksettings_zero_link_mode(ecmd, lp_advertising);
+
+	ecmd->base.autoneg = port->autoneg ? AUTONEG_ENABLE : AUTONEG_DISABLE;
+
+	if (port->type == MVSW_PORT_TYPE_TP) {
+		ethtool_link_ksettings_add_link_mode(ecmd, supported, Autoneg);
+		if (netif_running(dev) &&
+		    (port->autoneg ||
+		     port->transceiver == MVSW_PORT_TRANSCEIVER_COPPER))
+			ethtool_link_ksettings_add_link_mode(ecmd, advertising,
+							     Autoneg);
+	}
+
+	mvsw_modes_to_eth(ecmd->link_modes.supported, port->supp_link_modes,
+			  port->supp_fec, port->type);
+
+	mvsw_pr_port_supp_types_get(ecmd, port);
+
+	if (netif_carrier_ok(dev)) {
+		mvsw_pr_port_speed_get(ecmd, port);
+		mvsw_pr_port_duplex_get(ecmd, port);
+	} else {
+		ecmd->base.speed = SPEED_UNKNOWN;
+		ecmd->base.duplex = DUPLEX_UNKNOWN;
+	}
+
+	ecmd->base.port = mvsw_pr_port_type_get(port);
+
+	if (port->autoneg) {
+		if (netif_running(dev))
+			mvsw_modes_to_eth(ecmd->link_modes.advertising,
+					  port->adver_link_modes,
+					  port->adver_fec,
+					  port->type);
+
+		if (netif_carrier_ok(dev) &&
+		    port->transceiver == MVSW_PORT_TRANSCEIVER_COPPER) {
+			ethtool_link_ksettings_add_link_mode(ecmd,
+							     lp_advertising,
+							     Autoneg);
+			mvsw_pr_port_remote_cap_get(ecmd, port);
+		}
+	}
+
+	if (port->type == MVSW_PORT_TYPE_TP &&
+	    port->transceiver == MVSW_PORT_TRANSCEIVER_COPPER)
+		mvsw_pr_port_mdix_get(ecmd, port);
+
+	return 0;
+}
+
+static bool mvsw_pr_check_supp_modes(const struct mvsw_pr_port *port,
+				     u64 adver_modes, u8 adver_fec)
+{
+	if ((port->supp_link_modes & adver_modes) == 0)
+		return true;
+	if ((adver_fec & ~port->supp_fec) != 0)
+		return true;
+
+	return false;
+}
+
+static int mvsw_pr_port_set_link_ksettings(struct net_device *dev,
+					   const struct ethtool_link_ksettings
+					   *ecmd)
+{
+	int err, err1;
+	u8 adver_fec = 0;
+	u64 adver_modes = 0;
+	struct mvsw_pr_port *port = netdev_priv(dev);
+	bool is_up = netif_running(dev);
+
+	if (is_up) {
+		err = mvsw_pr_port_state_set(dev, false);
+		if (err)
+			return -EINVAL;
+	}
+
+	err = mvsw_pr_port_type_set(ecmd, port);
+	if (err)
+		goto fini_link_ksettings;
+
+	if (port->transceiver == MVSW_PORT_TRANSCEIVER_COPPER) {
+		err = mvsw_pr_port_mdix_set(ecmd, port);
+		if (err)
+			goto fini_link_ksettings;
+	}
+
+	mvsw_modes_from_eth(ecmd->link_modes.advertising, &adver_modes,
+			    &adver_fec);
+
+	if (ecmd->base.autoneg == AUTONEG_ENABLE &&
+	    mvsw_pr_check_supp_modes(port, adver_modes, adver_fec)) {
+		pr_err("Unsupported link mode requested");
+		err = -EINVAL;
+		goto fini_link_ksettings;
+	}
+
+	err = mvsw_pr_port_autoneg_set(port,
+				       ecmd->base.autoneg == AUTONEG_ENABLE,
+				       adver_modes, adver_fec);
+	if (err)
+		goto fini_link_ksettings;
+
+	if (ecmd->base.autoneg == AUTONEG_DISABLE) {
+		err = mvsw_pr_port_speed_duplex_set(ecmd, port);
+		if (err)
+			goto fini_link_ksettings;
+	}
+
+fini_link_ksettings:
+	err1 = mvsw_pr_port_state_set(dev, is_up);
+	if (err1)
+		return err1;
+
+	return err;
+}
+
+static int mvsw_pr_port_get_fecparam(struct net_device *dev,
+				     struct ethtool_fecparam *fecparam)
+{
+	int err;
+	u8 active;
+	u32 mode;
+	struct mvsw_pr_port *port = netdev_priv(dev);
+
+	err = mvsw_pr_hw_port_fec_get(port, &active);
+	if (err)
+		return err;
+
+	fecparam->fec = 0;
+	for (mode = 0; mode < MVSW_PORT_FEC_MAX; mode++) {
+		if ((mvsw_pr_fec_caps[mode].pr_fec & port->supp_fec) == 0)
+			continue;
+		fecparam->fec |= mvsw_pr_fec_caps[mode].eth_fec;
+	}
+
+	if (active < MVSW_PORT_FEC_MAX)
+		fecparam->active_fec = mvsw_pr_fec_caps[active].eth_fec;
+	else
+		fecparam->active_fec = ETHTOOL_FEC_AUTO;
+
+	return 0;
+}
+
+static int mvsw_pr_port_set_fecparam(struct net_device *dev,
+				     struct ethtool_fecparam *fecparam)
+{
+	int err;
+	u32 mode;
+	u8 fec, active;
+	struct mvsw_pr_port *port = netdev_priv(dev);
+
+	if (port->autoneg) {
+		pr_err("fec setting is not allowed while autoneg is on\n");
+		return -EINVAL;
+	}
+
+	err = mvsw_pr_hw_port_fec_get(port, &active);
+	if (err)
+		return err;
+
+	fec = MVSW_PORT_FEC_MAX;
+	for (mode = 0; mode < MVSW_PORT_FEC_MAX; mode++) {
+		if (((mvsw_pr_fec_caps[mode].eth_fec & fecparam->fec) != 0) &&
+		    ((mvsw_pr_fec_caps[mode].pr_fec & port->supp_fec) != 0)) {
+			fec = mode;
+			break;
+		}
+	}
+
+	if (fec == active)
+		return 0;
+
+	if (fec == MVSW_PORT_FEC_MAX) {
+		pr_err("Unsupported FEC requested");
+		return -EINVAL;
+	}
+
+	err = mvsw_pr_hw_port_fec_set(port, fec);
+
+	return err;
+}
+
+static void mvsw_pr_port_get_ethtool_stats(struct net_device *dev,
+					   struct ethtool_stats *stats,
+					   u64 *data)
+{
+	struct mvsw_pr_port *port = netdev_priv(dev);
+	u64 *port_stats = port->cached_hw_stats.stats_arr;
+
+	memcpy((u8 *)data, port_stats, sizeof(*port_stats) * MVSW_PORT_CNT_MAX);
+}
+
+static void mvsw_pr_port_get_strings(struct net_device *dev,
+				     u32 stringset, u8 *data)
+{
+	if (stringset != ETH_SS_STATS)
+		return;
+
+	memcpy(data, *mvsw_pr_port_cnt_name, sizeof(mvsw_pr_port_cnt_name));
+}
+
+static int mvsw_pr_port_get_sset_count(struct net_device *dev, int sset)
+{
+	switch (sset) {
+	case ETH_SS_STATS:
+		return MVSW_PORT_CNT_MAX;
+	default:
+		return -EOPNOTSUPP;
+	}
+}
+
+static const struct ethtool_ops mvsw_pr_ethtool_ops = {
+	.get_drvinfo = mvsw_pr_port_get_drvinfo,
+	.get_link_ksettings = mvsw_pr_port_get_link_ksettings,
+	.set_link_ksettings = mvsw_pr_port_set_link_ksettings,
+	.get_fecparam = mvsw_pr_port_get_fecparam,
+	.set_fecparam = mvsw_pr_port_set_fecparam,
+	.get_sset_count = mvsw_pr_port_get_sset_count,
+	.get_strings = mvsw_pr_port_get_strings,
+	.get_ethtool_stats = mvsw_pr_port_get_ethtool_stats,
+	.get_link = ethtool_op_get_link
+};
+
+int mvsw_pr_port_learning_set(struct mvsw_pr_port *mvsw_pr_port,
+			      bool learn_enable)
+{
+	return mvsw_pr_hw_port_learning_set(mvsw_pr_port, learn_enable);
+}
+
+int mvsw_pr_port_flood_set(struct mvsw_pr_port *mvsw_pr_port, bool flood)
+{
+	return mvsw_pr_hw_port_flood_set(mvsw_pr_port, flood);
+}
+
+int mvsw_pr_port_pvid_set(struct mvsw_pr_port *mvsw_pr_port, u16 vid)
+{
+	int err;
+
+	if (!vid) {
+		err = mvsw_pr_hw_port_accept_frame_type_set
+		    (mvsw_pr_port, MVSW_ACCEPT_FRAME_TYPE_TAGGED);
+		if (err)
+			return err;
+	} else {
+		err = mvsw_pr_hw_vlan_port_vid_set(mvsw_pr_port, vid);
+		if (err)
+			return err;
+		err = mvsw_pr_hw_port_accept_frame_type_set
+		    (mvsw_pr_port, MVSW_ACCEPT_FRAME_TYPE_ALL);
+		if (err)
+			goto err_port_allow_untagged_set;
+	}
+
+	mvsw_pr_port->pvid = vid;
+	return 0;
+
+err_port_allow_untagged_set:
+	mvsw_pr_hw_vlan_port_vid_set(mvsw_pr_port, mvsw_pr_port->pvid);
+	return err;
+}
+
+struct mvsw_pr_port_vlan*
+mvsw_pr_port_vlan_find_by_vid(const struct mvsw_pr_port *mvsw_pr_port, u16 vid)
+{
+	struct mvsw_pr_port_vlan *mvsw_pr_port_vlan;
+
+	list_for_each_entry(mvsw_pr_port_vlan, &mvsw_pr_port->vlans_list,
+			    list) {
+		if (mvsw_pr_port_vlan->vid == vid)
+			return mvsw_pr_port_vlan;
+	}
+
+	return NULL;
+}
+
+struct mvsw_pr_port_vlan*
+mvsw_pr_port_vlan_create(struct mvsw_pr_port *mvsw_pr_port, u16 vid)
+{
+	struct mvsw_pr_port_vlan *mvsw_pr_port_vlan;
+	bool untagged = vid == MVSW_PR_DEFAULT_VID;
+	int err;
+
+	mvsw_pr_port_vlan = mvsw_pr_port_vlan_find_by_vid(mvsw_pr_port, vid);
+	if (mvsw_pr_port_vlan)
+		return ERR_PTR(-EEXIST);
+
+	err = mvsw_pr_port_vlan_set(mvsw_pr_port, vid, true, untagged);
+	if (err)
+		return ERR_PTR(err);
+
+	mvsw_pr_port_vlan = kzalloc(sizeof(*mvsw_pr_port_vlan), GFP_KERNEL);
+	if (!mvsw_pr_port_vlan) {
+		err = -ENOMEM;
+		goto err_port_vlan_alloc;
+	}
+
+	mvsw_pr_port_vlan->mvsw_pr_port = mvsw_pr_port;
+	mvsw_pr_port_vlan->vid = vid;
+	list_add(&mvsw_pr_port_vlan->list, &mvsw_pr_port->vlans_list);
+
+	return mvsw_pr_port_vlan;
+
+err_port_vlan_alloc:
+	mvsw_pr_port_vlan_set(mvsw_pr_port, vid, false, false);
+	return ERR_PTR(err);
+}
+
+static void
+mvsw_pr_port_vlan_cleanup(struct mvsw_pr_port_vlan *mvsw_pr_port_vlan)
+{
+	if (mvsw_pr_port_vlan->bridge_port)
+		mvsw_pr_port_vlan_bridge_leave(mvsw_pr_port_vlan);
+}
+
+void mvsw_pr_port_vlan_destroy(struct mvsw_pr_port_vlan *mvsw_pr_port_vlan)
+{
+	struct mvsw_pr_port *mvsw_pr_port = mvsw_pr_port_vlan->mvsw_pr_port;
+	u16 vid = mvsw_pr_port_vlan->vid;
+
+	mvsw_pr_port_vlan_cleanup(mvsw_pr_port_vlan);
+	list_del(&mvsw_pr_port_vlan->list);
+	kfree(mvsw_pr_port_vlan);
+	mvsw_pr_hw_vlan_port_set(mvsw_pr_port, vid, false, false);
+}
+
+int mvsw_pr_port_vlan_set(struct mvsw_pr_port *mvsw_pr_port, u16 vid,
+			  bool is_member, bool untagged)
+{
+	return mvsw_pr_hw_vlan_port_set(mvsw_pr_port, vid, is_member, untagged);
+}
+
+static int mvsw_pr_port_create(struct mvsw_pr_switch *sw, u32 port)
+{
+	int err;
+	char *mac;
+	struct net_device *net_dev;
+	struct mvsw_pr_port *pr_port;
+
+	net_dev = alloc_etherdev(sizeof(*pr_port));
+	if (!net_dev)
+		return -ENOMEM;
+
+	pr_port = netdev_priv(net_dev);
+	pr_port->net_dev = net_dev;
+	pr_port->id = port;
+	pr_port->sw = sw;
+	pr_port->pvid = MVSW_PR_DEFAULT_VID;
+	INIT_LIST_HEAD(&pr_port->vlans_list);
+
+	pr_port->cached_hw_stats.stats_arr =
+		kcalloc(MVSW_PORT_CNT_MAX,
+			sizeof(*pr_port->cached_hw_stats.stats_arr),
+			GFP_KERNEL);
+
+	if (!pr_port->cached_hw_stats.stats_arr) {
+		MVSW_LOG_ERROR("Failed to alloc hw stats arr [index=%u]", port);
+		err = -ENOMEM;
+		goto err_register_netdev;
+	}
+
+	err = mvsw_pr_hw_port_info_get(pr_port, &pr_port->fp_id,
+				       &pr_port->hw_id, &pr_port->dev_id);
+	if (err) {
+		MVSW_LOG_ERROR("Failed to fetch port info [index=%u]", port);
+		goto err_register_netdev;
+	}
+
+	net_dev->netdev_ops = &mvsw_pr_netdev_ops;
+	net_dev->ethtool_ops = &mvsw_pr_ethtool_ops;
+	net_dev->features |= NETIF_F_NETNS_LOCAL | NETIF_F_HW_L2FW_DOFFLOAD;
+
+	netif_carrier_off(net_dev);
+	net_dev->link_mode = IF_LINK_MODE_DEFAULT;
+
+	net_dev->min_mtu = sw->mtu_min;
+	net_dev->max_mtu = sw->mtu_max;
+	net_dev->mtu = (sw->mtu_max > MVSW_PR_MTU_DEFAULT) ?
+			MVSW_PR_MTU_DEFAULT : sw->mtu_max;
+
+	err = mvsw_pr_hw_port_mtu_set(pr_port, net_dev->mtu);
+	if (err) {
+		MVSW_LOG_ERROR("Failed to set port mtu [port=%u]\n",
+			       pr_port->id);
+		goto err_register_netdev;
+	}
+
+	/* Only 0xFF mac addrs are supported */
+	if (pr_port->fp_id >= 0xFF)
+		goto err_register_netdev;
+	mac = net_dev->dev_addr;
+	memcpy(mac, sw->base_mac, net_dev->addr_len - 1);
+	mac[net_dev->addr_len - 1] = (char)pr_port->fp_id;
+
+	err = mvsw_pr_hw_port_mac_set(pr_port, mac);
+	if (err) {
+		MVSW_LOG_ERROR("Failed to set port mac addr [port=%u]\n",
+			       pr_port->id);
+		goto err_register_netdev;
+	}
+
+	err = mvsw_pr_hw_port_cap_get(pr_port);
+	if (err) {
+		MVSW_LOG_ERROR("Failed to get port capability [port_id=%u]\n",
+			       pr_port->id);
+		goto err_register_netdev;
+	}
+
+	pr_port->adver_link_modes = 0;
+	pr_port->adver_fec = 1 << MVSW_PORT_FEC_OFF_BIT;
+	pr_port->autoneg = false;
+	mvsw_pr_port_autoneg_set(pr_port, true,
+				 pr_port->supp_link_modes,
+				 pr_port->supp_fec);
+
+	err = mvsw_pr_hw_port_state_set(pr_port, false);
+	if (err) {
+		MVSW_LOG_ERROR("Failed to set port admin down [port=%u]\n",
+			       pr_port->id);
+		goto err_register_netdev;
+	}
+
+	INIT_DELAYED_WORK(&pr_port->cached_hw_stats.caching_dw,
+			  &update_stats_cache);
+
+	err = register_netdev(net_dev);
+	if (err) {
+		pr_err("failed to register netdev [id=%u]\n", pr_port->id);
+		goto err_register_netdev;
+	}
+	list_add(&pr_port->list, &sw->port_list);
+
+	MVSW_LOG_INFO("Created mvsw port [id=%u]", pr_port->id);
+	return 0;
+
+err_register_netdev:
+	free_netdev(net_dev);
+	return err;
+}
+
+static void mvsw_pr_port_vlan_flush(struct mvsw_pr_port *mvsw_pr_port,
+				    bool flush_default)
+{
+	struct mvsw_pr_port_vlan *mvsw_pr_port_vlan, *tmp;
+
+	list_for_each_entry_safe(mvsw_pr_port_vlan, tmp,
+				 &mvsw_pr_port->vlans_list, list) {
+		if (!flush_default &&
+		    mvsw_pr_port_vlan->vid == MVSW_PR_DEFAULT_VID)
+			continue;
+		mvsw_pr_port_vlan_destroy(mvsw_pr_port_vlan);
+	}
+}
+
+int mvsw_pr_8021d_bridge_create(struct mvsw_pr_switch *sw, u16 *bridge_id)
+{
+	return mvsw_pr_hw_bridge_create(sw, bridge_id);
+}
+
+int mvsw_pr_8021d_bridge_delete(struct mvsw_pr_switch *sw, u16 bridge_id)
+{
+	return mvsw_pr_hw_bridge_delete(sw, bridge_id);
+}
+
+int mvsw_pr_8021d_bridge_port_add(struct mvsw_pr_port *mvsw_pr_port,
+				  u16 bridge_id)
+{
+	return mvsw_pr_hw_bridge_port_add(mvsw_pr_port, bridge_id);
+}
+
+int mvsw_pr_8021d_bridge_port_delete(struct mvsw_pr_port *mvsw_pr_port,
+				     u16 bridge_id)
+{
+	return mvsw_pr_hw_bridge_port_delete(mvsw_pr_port, bridge_id);
+}
+
+int mvsw_pr_switch_ageing_set(struct mvsw_pr_switch *sw, u32 ageing_time)
+{
+	return mvsw_pr_hw_switch_ageing_set(sw, ageing_time);
+}
+
+int mvsw_pr_fdb_flush_vlan(struct mvsw_pr_switch *sw, u16 vid,
+			   enum mvsw_pr_fdb_flush_mode mode)
+{
+	return mvsw_pr_hw_fdb_flush_vlan(sw, vid, mode);
+}
+
+int mvsw_pr_fdb_flush_port_vlan(struct mvsw_pr_port *port, u16 vid,
+				enum mvsw_pr_fdb_flush_mode mode)
+{
+	return mvsw_pr_hw_fdb_flush_port_vlan(port, vid, mode);
+}
+
+int mvsw_pr_fdb_flush_port(struct mvsw_pr_port *port,
+			   enum mvsw_pr_fdb_flush_mode mode)
+{
+	return mvsw_pr_hw_fdb_flush_port(port, mode);
+}
+
+static int mvsw_pr_clear_ports(struct mvsw_pr_switch *sw)
+{
+	struct list_head *pos, *n;
+	struct net_device *net_dev;
+	struct mvsw_pr_port *pr_port;
+
+	list_for_each_safe(pos, n, &sw->port_list) {
+		pr_port = list_entry(pos, typeof(*pr_port), list);
+		MVSW_LOG_INFO("Removed mvsw port [id=%u]", pr_port->id);
+		cancel_delayed_work_sync(&pr_port->cached_hw_stats.caching_dw);
+		kfree(pr_port->cached_hw_stats.stats_arr);
+		net_dev = pr_port->net_dev;
+		unregister_netdev(net_dev);
+		mvsw_pr_port_vlan_flush(pr_port, true);
+		WARN_ON_ONCE(!list_empty(&pr_port->vlans_list));
+		free_netdev(net_dev);
+		list_del(pos);
+	}
+	return (!list_empty(&sw->port_list));
+}
+
+static void mvsw_pr_port_handle_event(struct mvsw_pr_switch *sw,
+				      struct mvsw_pr_event *evt)
+{
+	struct mvsw_pr_port *port;
+	struct delayed_work *caching_dw;
+
+	port = __find_pr_port(sw, evt->port_evt.port_id);
+	if (!port) {
+		MVSW_LOG_ERROR("failed to find port [id=%u]",
+			       evt->port_evt.port_id);
+		return;
+	}
+
+	caching_dw = &port->cached_hw_stats.caching_dw;
+
+	switch (evt->id) {
+	case MVSW_PORT_EVENT_STATE_CHANGED:
+		if (evt->port_evt.data.oper_state) {
+			netif_carrier_on(port->net_dev);
+			if (!delayed_work_pending(caching_dw))
+				queue_delayed_work(mvsw_pr_wq, caching_dw, 0);
+		} else {
+			netif_carrier_off(port->net_dev);
+			if (delayed_work_pending(caching_dw))
+				cancel_delayed_work(caching_dw);
+		}
+		break;
+	default:
+		MVSW_LOG_ERROR("Unknown port event [evt id = %u]",
+			       evt->id);
+		return;
+	}
+}
+
+static void mvsw_pr_fdb_handle_event(struct mvsw_pr_switch *sw,
+				     struct mvsw_pr_event *evt)
+{
+	struct mvsw_pr_port *port;
+	struct switchdev_notifier_fdb_info info;
+
+	port = __find_pr_port(sw, evt->fdb_evt.port_id);
+	if (!port) {
+		MVSW_LOG_ERROR("failed to find port [id=%u]",
+			       evt->fdb_evt.port_id);
+		return;
+	}
+	info.addr = evt->fdb_evt.data.mac;
+	info.vid = evt->fdb_evt.vid;
+	info.offloaded = true;
+
+	rtnl_lock();
+	switch (evt->id) {
+	case MVSW_FDB_EVENT_LEARNED:
+		call_switchdev_notifiers(SWITCHDEV_FDB_ADD_TO_BRIDGE,
+					 port->net_dev, &info.info, NULL);
+		break;
+	case MVSW_FDB_EVENT_AGED:
+		call_switchdev_notifiers(SWITCHDEV_FDB_DEL_TO_BRIDGE,
+					 port->net_dev, &info.info, NULL);
+		break;
+	default:
+		MVSW_LOG_ERROR("Unknown fdb event [evt id = %u]",
+			       evt->id);
+	}
+	rtnl_unlock();
+	return;
+}
+
+int mvsw_pr_fdb_add(struct mvsw_pr_port *mvsw_pr_port, const unsigned char *mac,
+		    u16 vid, bool dynamic)
+{
+	return mvsw_pr_hw_fdb_add(mvsw_pr_port, mac, vid, dynamic);
+}
+
+int mvsw_pr_fdb_del(struct mvsw_pr_port *mvsw_pr_port, const unsigned char *mac,
+		    u16 vid)
+{
+	return mvsw_pr_hw_fdb_del(mvsw_pr_port, mac, vid);
+}
+
+static void mvsw_pr_fdb_event_handler_unregister(struct mvsw_pr_switch *sw)
+{
+	struct mvsw_pr_event_handler eh;
+
+	eh.type = MVSW_EVENT_TYPE_FDB;
+	eh.func = mvsw_pr_fdb_handle_event;
+	mvsw_pr_hw_event_handler_unregister(sw, &eh);
+}
+
+static void mvsw_pr_port_event_handler_unregister(struct mvsw_pr_switch *sw)
+{
+	struct mvsw_pr_event_handler eh;
+
+	eh.type = MVSW_EVENT_TYPE_PORT;
+	eh.func = mvsw_pr_port_handle_event;
+	mvsw_pr_hw_event_handler_unregister(sw, &eh);
+}
+
+static void mvsw_pr_event_handlers_unregister(struct mvsw_pr_switch *sw)
+{
+	mvsw_pr_fdb_event_handler_unregister(sw);
+	mvsw_pr_port_event_handler_unregister(sw);
+}
+
+static int mvsw_pr_fdb_event_handler_register(struct mvsw_pr_switch *sw)
+{
+	struct mvsw_pr_event_handler eh;
+
+	eh.type = MVSW_EVENT_TYPE_FDB;
+	eh.func = mvsw_pr_fdb_handle_event;
+	return mvsw_pr_hw_event_handler_register(sw, &eh);
+}
+
+static int mvsw_pr_port_event_handler_register(struct mvsw_pr_switch *sw)
+{
+	struct mvsw_pr_event_handler eh;
+
+	eh.type = MVSW_EVENT_TYPE_PORT;
+	eh.func = mvsw_pr_port_handle_event;
+	return mvsw_pr_hw_event_handler_register(sw, &eh);
+}
+
+static int mvsw_pr_event_handlers_register(struct mvsw_pr_switch *sw)
+{
+	int err;
+
+	err = mvsw_pr_port_event_handler_register(sw);
+	if (err) {
+		MVSW_LOG_ERROR("failed to register port event handler");
+		return err;
+	}
+
+	err = mvsw_pr_fdb_event_handler_register(sw);
+	if (err) {
+		MVSW_LOG_ERROR("failed to register FDB event handler");
+		goto err_fdb_handler_register;
+	}
+
+	return 0;
+
+err_fdb_handler_register:
+	mvsw_pr_port_event_handler_unregister(sw);
+	return err;
+}
+
+const struct mvsw_pr_port *mvsw_pr_port_find(u32 dev_hw_id, u32 port_hw_id)
+{
+	struct mvsw_pr_port *port = NULL;
+	struct mvsw_pr_switch *sw;
+
+	list_for_each_entry(sw, &switches_registered, list) {
+		list_for_each_entry(port, &sw->port_list, list) {
+			if (port->hw_id == port_hw_id &&
+			    port->dev_id == dev_hw_id)
+				return port;
+		}
+	}
+	return NULL;
+}
+
+static int mvsw_pr_init(struct mvsw_pr_switch *sw)
+{
+	int err;
+	u32 port;
+
+	err = mvsw_pr_hw_switch_init(sw);
+	if (err) {
+		pr_err("Failed to init switch device\n");
+		return err;
+	}
+
+	pr_info("Initialized prestera switch device (%s)\n", sw->dev->name);
+
+	sw->info.device_kind = mvsw_driver_kind;
+	sw->info.device_version = mvsw_driver_version;
+	sw->info.fw_rev = &fw_rev;
+
+	err = mvsw_pr_switchdev_register(sw);
+	if (err) {
+		MVSW_LOG_ERROR("Failed to register sw notifiers");
+		return err;
+	}
+
+	INIT_LIST_HEAD(&sw->port_list);
+
+	for (port = 0; port < sw->port_count; port++) {
+		err = mvsw_pr_port_create(sw, port);
+		if (err) {
+			MVSW_LOG_ERROR("Failed to create port [id=%u]", port);
+			goto err_ports_init;
+		}
+	}
+
+	err = mvsw_pr_event_handlers_register(sw);
+	if (err) {
+		MVSW_LOG_ERROR("Failed to register evt handlers\n");
+		goto err_ports_init;
+	}
+
+	err = mvsw_pr_fw_log_init(sw);
+	if (err) {
+		MVSW_LOG_ERROR("Failed to init FW log interface\n");
+		goto err_fw_log_init;
+	}
+
+	return 0;
+
+err_fw_log_init:
+	mvsw_pr_event_handlers_unregister(sw);
+err_ports_init:
+	mvsw_pr_clear_ports(sw);
+	return err;
+}
+
+static void mvsw_pr_fini(struct mvsw_pr_switch *sw)
+{
+	mvsw_pr_event_handlers_unregister(sw);
+
+	mvsw_pr_fw_log_fini(sw);
+
+	mvsw_pr_switchdev_unregister(sw);
+	mvsw_pr_clear_ports(sw);
+}
+
+int mvsw_pr_device_register(struct mvsw_pr_device *dev)
+{
+	struct mvsw_pr_switch *sw;
+	int err;
+
+	sw = kzalloc(sizeof(*sw), GFP_KERNEL);
+	if (!sw)
+		return -ENOMEM;
+
+	dev->priv = sw;
+	sw->dev = dev;
+
+	err = mvsw_pr_init(sw);
+	if (err) {
+		kfree(sw);
+		return err;
+	}
+
+	list_add(&sw->list, &switches_registered);
+
+	return 0;
+}
+EXPORT_SYMBOL(mvsw_pr_device_register);
+
+void mvsw_pr_device_unregister(struct mvsw_pr_device *dev)
+{
+	struct mvsw_pr_switch *sw = dev->priv;
+
+	list_del(&sw->list);
+	mvsw_pr_fini(sw);
+	kfree(sw);
+}
+EXPORT_SYMBOL(mvsw_pr_device_unregister);
+
+static int __init mvsw_pr_module_init(void)
+{
+	int err;
+
+	INIT_LIST_HEAD(&switches_registered);
+
+	mvsw_pr_wq = alloc_workqueue(mvsw_driver_name, 0, 0);
+
+	if (!mvsw_pr_wq)
+		return -ENOMEM;
+
+	err = mvsw_pr_rxtx_init();
+	if (err) {
+		pr_err("failed to initialize prestera rxtx\n");
+		destroy_workqueue(mvsw_pr_wq);
+		return err;
+	}
+
+	pr_info("Loading Marvell Prestera Switch Driver\n");
+	return 0;
+}
+
+static void __exit mvsw_pr_module_exit(void)
+{
+	destroy_workqueue(mvsw_pr_wq);
+	mvsw_pr_rxtx_fini();
+
+	pr_info("Unloading Marvell Prestera Switch Driver\n");
+}
+
+module_init(mvsw_pr_module_init);
+module_exit(mvsw_pr_module_exit);
+
+MODULE_AUTHOR("Marvell Semi.");
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("Marvell Prestera switch driver");
+MODULE_VERSION(PRESTERA_DRV_VER);
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera.h b/drivers/net/ethernet/marvell/prestera_sw/prestera.h
new file mode 100644
index 0000000..b29f809
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera.h
@@ -0,0 +1,241 @@
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+
+#ifndef _MVSW_PRESTERA_H_
+#define _MVSW_PRESTERA_H_
+
+#include <linux/skbuff.h>
+#include <linux/notifier.h>
+#include <uapi/linux/if_ether.h>
+#include <linux/workqueue.h>
+
+#define MVSW_MSG_MAX_SIZE 1500
+
+#define MVSW_PR_DEFAULT_VID 1
+
+#define MVSW_PR_MIN_AGEING_TIME 10
+#define MVSW_PR_MAX_AGEING_TIME 1000000
+#define MVSW_PR_DEFAULT_AGEING_TIME 300
+
+struct mvsw_fw_rev {
+	u16 major;
+	u16 minor;
+	u16 subminor;
+};
+
+struct mvsw_pr_info {
+	const char *device_kind;
+	const char *device_name;
+	const char *device_version;
+	const struct mvsw_fw_rev *fw_rev;
+};
+
+struct mvsw_pr_bridge_port;
+
+struct mvsw_pr_port_vlan {
+	struct list_head list;
+	struct mvsw_pr_port *mvsw_pr_port;
+	u16 vid;
+	struct mvsw_pr_bridge_port *bridge_port;
+	struct list_head bridge_vlan_node;
+};
+
+struct mvsw_pr_port {
+	struct net_device *net_dev;
+	struct mvsw_pr_switch *sw;
+	u32 id;
+	u32 hw_id;
+	u32 dev_id;
+	u16 fp_id;
+	u16 pvid;
+	bool autoneg;
+	u8 supp_fec;
+	u8 adver_fec;
+	u8 type;
+	u8 transceiver;
+	u64 supp_link_modes;
+	u64 adver_link_modes;
+	struct list_head list;
+	struct list_head vlans_list;
+	struct {
+		u64 *stats_arr;
+		struct delayed_work caching_dw;
+	} cached_hw_stats;
+};
+
+struct mvsw_pr_switchdev {
+	struct mvsw_pr_switch *sw;
+	struct notifier_block swdev_n;
+	struct notifier_block swdev_blocking_n;
+};
+
+struct mvsw_pr_fib {
+	struct mvsw_pr_switch *sw;
+	struct notifier_block fib_nb;
+	struct notifier_block netevent_nb;
+};
+
+struct mvsw_pr_bus;
+
+struct mvsw_pr_device {
+	const char *name;
+	struct mvsw_pr_bus *bus;
+	struct device *dev;
+	int (*recv_msg)(struct mvsw_pr_device *dev, u8 *msg, size_t size);
+	struct workqueue_struct *dev_wq;
+	void *priv;
+};
+
+enum {
+	MVSW_BUS_SEND_SYNC,
+	MVSW_BUS_SEND_ASYNC,
+	MVSW_BUS_SEND_MAX,
+};
+
+enum mvsw_pr_event_type {
+	MVSW_EVENT_TYPE_UNSPEC,
+	MVSW_EVENT_TYPE_PORT,
+	MVSW_EVENT_TYPE_FDB,
+	MVSW_EVENT_TYPE_FW_LOG,
+
+	MVSW_EVENT_TYPE_MAX,
+};
+
+enum mvsw_pr_port_event_id {
+	MVSW_PORT_EVENT_UNSPEC,
+	MVSW_PORT_EVENT_STATE_CHANGED,
+
+	MVSW_PORT_EVENT_MAX,
+};
+
+enum mvsw_pr_fdb_event_id {
+	MVSW_FDB_EVENT_UNSPEC,
+	MVSW_FDB_EVENT_LEARNED,
+	MVSW_FDB_EVENT_AGED,
+
+	MVSW_FDB_EVENT_MAX,
+};
+
+struct mvsw_pr_fdb_event {
+	u32 port_id;
+	u32 vid;
+	union {
+		u8 mac[ETH_ALEN];
+	} data;
+};
+
+struct mvsw_pr_port_event {
+	u32 port_id;
+	union {
+		u32 oper_state;
+	} data;
+};
+
+struct mvsw_pr_fw_log_event {
+	u32 log_len;
+	u8 *data;
+};
+
+struct mvsw_pr_event {
+	u16 id;
+	union {
+		struct mvsw_pr_port_event port_evt;
+		struct mvsw_pr_fdb_event fdb_evt;
+		struct mvsw_pr_fw_log_event fw_log_evt;
+	};
+};
+
+typedef void (*mvsw_pr_event_handler_cb) (struct mvsw_pr_switch *sw,
+				       struct mvsw_pr_event *evt);
+
+struct mvsw_pr_event_handler {
+	mvsw_pr_event_handler_cb func;
+	enum mvsw_pr_event_type type;
+	void *ctx;
+};
+
+struct mvsw_pr_bus {
+	const char *type;
+	u32 timeout;		/* usec */
+	int (*send_req)(struct mvsw_pr_device *dev, int mode, u8 *in_msg,
+			size_t in_size, u8 *out_msg, size_t out_size,
+			size_t *out_data_size);
+};
+
+struct mvsw_pr_bridge;
+
+struct mvsw_pr_switch {
+	struct list_head list;
+	struct mvsw_pr_device *dev;
+	struct mvsw_pr_info info;
+	struct list_head event_handlers;
+	char base_mac[ETH_ALEN];
+	struct list_head port_list;
+	u32 port_count;
+	u32 mtu_min;
+	u32 mtu_max;
+	u8 id;
+	struct mvsw_pr_bridge *bridge;
+	struct mvsw_pr_switchdev *switchdev;
+	struct mvsw_pr_fib *fib;
+	struct notifier_block netdevice_nb;
+};
+
+enum mvsw_pr_fdb_flush_mode {
+	MVSW_PR_FDB_FLUSH_MODE_DYNAMIC = BIT(0),
+	MVSW_PR_FDB_FLUSH_MODE_STATIC = BIT(1),
+	MVSW_PR_FDB_FLUSH_MODE_ALL = MVSW_PR_FDB_FLUSH_MODE_DYNAMIC
+				   | MVSW_PR_FDB_FLUSH_MODE_STATIC,
+};
+
+int mvsw_pr_switch_ageing_set(struct mvsw_pr_switch *sw, u32 ageing_time);
+
+int mvsw_pr_port_learning_set(struct mvsw_pr_port *mvsw_pr_port,
+			      bool learn_enable);
+int mvsw_pr_port_flood_set(struct mvsw_pr_port *mvsw_pr_port, bool flood);
+int mvsw_pr_port_pvid_set(struct mvsw_pr_port *mvsw_pr_port, u16 vid);
+struct mvsw_pr_port_vlan *
+mvsw_pr_port_vlan_create(struct mvsw_pr_port *mvsw_pr_port, u16 vid);
+void mvsw_pr_port_vlan_destroy(struct mvsw_pr_port_vlan *mvsw_pr_port_vlan);
+int mvsw_pr_port_vlan_set(struct mvsw_pr_port *mvsw_pr_port, u16 vid,
+			  bool is_member, bool untagged);
+
+int mvsw_pr_8021d_bridge_create(struct mvsw_pr_switch *sw, u16 *bridge_id);
+int mvsw_pr_8021d_bridge_delete(struct mvsw_pr_switch *sw, u16 bridge_id);
+int mvsw_pr_8021d_bridge_port_add(struct mvsw_pr_port *mvsw_pr_port,
+				  u16 bridge_id);
+int mvsw_pr_8021d_bridge_port_delete(struct mvsw_pr_port *mvsw_pr_port,
+				     u16 bridge_id);
+
+int mvsw_pr_fdb_add(struct mvsw_pr_port *mvsw_pr_port, const unsigned char *mac,
+		    u16 vid, bool dynamic);
+int mvsw_pr_fdb_del(struct mvsw_pr_port *mvsw_pr_port, const unsigned char *mac,
+		    u16 vid);
+int mvsw_pr_fdb_flush_vlan(struct mvsw_pr_switch *sw, u16 vid,
+			   enum mvsw_pr_fdb_flush_mode mode);
+int mvsw_pr_fdb_flush_port_vlan(struct mvsw_pr_port *port, u16 vid,
+				enum mvsw_pr_fdb_flush_mode mode);
+int mvsw_pr_fdb_flush_port(struct mvsw_pr_port *port,
+			   enum mvsw_pr_fdb_flush_mode mode);
+
+struct mvsw_pr_port_vlan *
+mvsw_pr_port_vlan_find_by_vid(const struct mvsw_pr_port *mvsw_pr_port, u16 vid);
+void
+mvsw_pr_port_vlan_bridge_leave(struct mvsw_pr_port_vlan *mvsw_pr_port_vlan);
+
+int mvsw_pr_switchdev_register(struct mvsw_pr_switch *sw);
+void mvsw_pr_switchdev_unregister(struct mvsw_pr_switch *sw);
+
+int mvsw_pr_device_register(struct mvsw_pr_device *dev);
+void mvsw_pr_device_unregister(struct mvsw_pr_device *dev);
+
+bool mvsw_pr_netdev_check(const struct net_device *dev);
+struct mvsw_pr_switch *mvsw_pr_lower_get(struct net_device *dev);
+struct mvsw_pr_port *mvsw_pr_port_dev_lower_find(struct net_device *dev);
+
+const struct mvsw_pr_port *mvsw_pr_port_find(u32 dev_hw_id, u32 port_hw_id);
+
+#endif /* _MVSW_PRESTERA_H_ */
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_drv_ver.h b/drivers/net/ethernet/marvell/prestera_sw/prestera_drv_ver.h
new file mode 100644
index 0000000..fee6782
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_drv_ver.h
@@ -0,0 +1,23 @@
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+#ifndef _PRESTERA_DRV_VER_H_
+#define _PRESTERA_DRV_VER_H_
+
+#include <linux/stringify.h>
+
+/* Prestera driver version */
+#define PRESTERA_DRV_VER_MAJOR	1
+#define PRESTERA_DRV_VER_MINOR	0
+#define PRESTERA_DRV_VER_PATCH	0
+#define PRESTERA_DRV_VER_EXTRA	-LD200218
+
+#define PRESTERA_DRV_VER \
+		__stringify(PRESTERA_DRV_VER_MAJOR)  "." \
+		__stringify(PRESTERA_DRV_VER_MINOR)  "." \
+		__stringify(PRESTERA_DRV_VER_PATCH)  \
+		__stringify(PRESTERA_DRV_VER_EXTRA)
+
+#endif  /* _PRESTERA_DRV_VER_H_ */
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_dsa.c b/drivers/net/ethernet/marvell/prestera_sw/prestera_dsa.c
new file mode 100644
index 0000000..43b8bd1
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_dsa.c
@@ -0,0 +1,305 @@
+// SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+/*
+ * Copyright (c) 2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+#include "prestera_dsa.h"
+
+#include <linux/string.h>
+#include <linux/bitops.h>
+#include <linux/bitfield.h>
+#include <linux/errno.h>
+
+#define W0_MASK_IS_TAGGED	BIT(29)
+
+/* TrgDev[4:0] = {Word0[28:24]} */
+#define W0_MASK_HW_DEV_NUM	GENMASK(28, 24)
+
+/* SrcPort/TrgPort extended to 8b
+ * SrcPort/TrgPort[7:0] = {Word2[20], Word1[11:10], Word0[23:19]}
+ */
+#define W0_MASK_IFACE_PORT_NUM	GENMASK(23, 19)
+
+/* bits 30:31 - TagCommand 1 = FROM_CPU */
+#define W0_MASK_DSA_CMD		GENMASK(31, 30)
+
+/* bits 13:15 -- UP */
+#define W0_MASK_VPT		GENMASK(15, 13)
+
+#define W0_MASK_EXT_BIT		BIT(12)
+#define W0_MASK_OPCODE		GENMASK(18, 16)
+
+/* bit 16 - CFI */
+#define W0_MASK_CFI_BIT		BIT(16)
+
+/* bits 0:11 -- VID */
+#define W0_MASK_VID		GENMASK(11, 0)
+
+#define W1_MASK_SRC_IS_TARNK	BIT(27)
+
+/* SrcPort/TrgPort extended to 8b
+ * SrcPort/TrgPort[7:0] = {Word2[20], Word1[11:10], Word0[23:19]}
+ */
+#define W1_MASK_IFACE_PORT_NUM	GENMASK(11, 10)
+
+#define W1_MASK_EXT_BIT		BIT(31)
+#define W1_MASK_CFI_BIT		BIT(30)
+
+/* bit 30 -- EgressFilterEn */
+#define W1_MASK_EGR_FILTER_EN	BIT(30)
+
+/* bit 28 -- egrFilterRegistered */
+#define W1_MASK_EGR_FILTER_REG	BIT(28)
+
+/* bits 20-24 -- Src-ID */
+#define W1_MASK_SRC_ID		GENMASK(24, 20)
+
+/* bits 15-19 -- SrcDev */
+#define W1_MASK_SRC_DEV		GENMASK(19, 15)
+
+/* SrcTrunk is extended to 12b
+ * SrcTrunk[11:0] = {Word2[14:3]
+ */
+#define W2_MASK_SRC_TRANK_ID	GENMASK(14, 3)
+
+/* SRCePort[16:0]/TRGePort[16:0]/ = {Word2[19:3]} */
+#define W2_MASK_IFACE_EPORT	GENMASK(19, 3)
+
+/* SrcPort/TrgPort extended to 8b
+ * SrcPort/TrgPort[7:0] = {Word2[20], Word1[11:10], Word0[23:19]}
+ */
+#define W2_MASK_IFACE_PORT_NUM	BIT(20)
+
+#define W2_MASK_EXT_BIT		BIT(31)
+
+/* 5b SrcID is extended to 12 bits
+ * SrcID[11:0] = {Word2[27:21], Word1[24:20]}
+ */
+#define W2_MASK_SRC_ID		GENMASK(27, 21)
+
+/* 5b SrcDev is extended to 12b
+ * SrcDev[11:0] = {Word2[20:14], Word1[19:15]}
+ */
+#define W2_MASK_SRC_DEV		GENMASK(20, 14)
+
+/* trgHwDev and trgPort
+ * TrgDev[11:5] = {Word3[6:0]}
+ */
+#define W3_MASK_HW_DEV_NUM	GENMASK(6, 0)
+
+/* VID becomes 16b eVLAN. eVLAN[15:0] = {Word3[30:27], Word0[11:0]} */
+#define W3_MASK_VID		GENMASK(30, 27)
+
+/* TRGePort[16:0] = {Word3[23:7]} */
+#define W3_MASK_DST_EPORT	GENMASK(23, 7)
+
+#define DEV_NUM_MASK		GENMASK(11, 5)
+#define VID_MASK		GENMASK(15, 12)
+
+static int net_if_dsa_to_cpu_parse(const u32 *words_ptr,
+				   struct mvsw_pr_dsa *dsa_info_ptr)
+{
+	u32 get_value;	/* used to get needed bits from the DSA */
+	struct mvsw_pr_dsa_to_cpu *to_cpu_ptr;
+
+	to_cpu_ptr = &dsa_info_ptr->dsa_info.to_cpu;
+	to_cpu_ptr->is_tagged =
+	    (bool)FIELD_GET(W0_MASK_IS_TAGGED, words_ptr[0]);
+	to_cpu_ptr->hw_dev_num = FIELD_GET(W0_MASK_HW_DEV_NUM, words_ptr[0]);
+	to_cpu_ptr->src_is_trunk =
+	    (bool)FIELD_GET(W1_MASK_SRC_IS_TARNK, words_ptr[1]);
+
+	/* set hw dev num */
+	get_value = FIELD_GET(W3_MASK_HW_DEV_NUM, words_ptr[3]);
+	to_cpu_ptr->hw_dev_num &= W3_MASK_HW_DEV_NUM;
+	to_cpu_ptr->hw_dev_num |= FIELD_PREP(DEV_NUM_MASK, get_value);
+
+	if (to_cpu_ptr->src_is_trunk) {
+		to_cpu_ptr->iface.src_trunk_id =
+		    (u16)FIELD_GET(W2_MASK_SRC_TRANK_ID, words_ptr[2]);
+	} else {
+		/* When to_cpu_ptr->is_egress_pipe = false:
+		 *   this field indicates the source ePort number assigned by
+		 *   the ingress device.
+		 * When to_cpu_ptr->is_egress_pipe = true:
+		 *   this field indicates the target ePort number assigned by
+		 *   the ingress device.
+		 */
+		to_cpu_ptr->iface.eport =
+		    FIELD_GET(W2_MASK_IFACE_EPORT, words_ptr[2]);
+	}
+	to_cpu_ptr->iface.port_num =
+	    (FIELD_GET(W0_MASK_IFACE_PORT_NUM, words_ptr[0])) |
+	    (FIELD_GET(W1_MASK_IFACE_PORT_NUM, words_ptr[1])) |
+	    (FIELD_GET(W2_MASK_IFACE_PORT_NUM, words_ptr[2]));
+
+	return 0;
+}
+
+int mvsw_pr_dsa_parse(const u8 *dsa_bytes_ptr, struct mvsw_pr_dsa *dsa_info_ptr)
+{
+	u32 get_value;		/* used to get needed bits from the DSA */
+	u32 words_ptr[4] = { 0 };	/* DSA tag can be up to 4 words */
+	u32 *dsa_words_ptr = (u32 *)dsa_bytes_ptr;
+
+	/* sanity */
+	if (unlikely(!dsa_info_ptr || !dsa_bytes_ptr))
+		return -EINVAL;
+
+	/* zero results */
+	memset(dsa_info_ptr, 0, sizeof(struct mvsw_pr_dsa));
+
+	/* copy the data of the first word */
+	words_ptr[0] = ntohl((__force __be32)dsa_words_ptr[0]);
+
+	/* set the common parameters */
+	dsa_info_ptr->dsa_cmd =
+	    (enum mvsw_pr_dsa_cmd)FIELD_GET(W0_MASK_DSA_CMD, words_ptr[0]);
+	dsa_info_ptr->common_params.vpt =
+	    (u8)FIELD_GET(W0_MASK_VPT, words_ptr[0]);
+
+	/* only to CPU is supported */
+	if (unlikely(dsa_info_ptr->dsa_cmd != MVSW_NET_DSA_CMD_TO_CPU_E))
+		return -EINVAL;
+
+	/* check extended bit */
+	if (FIELD_GET(W0_MASK_EXT_BIT, words_ptr[0]) == 0)
+		/* 1 words DSA tag is not supported */
+		return -EINVAL;
+
+	/* check that the "old" cpu opcode is set the 0xF
+	 * (with the extended bit)
+	 */
+	if (FIELD_GET(W0_MASK_OPCODE, words_ptr[0]) != 0x07)
+		return -EINVAL;
+
+	/* copy the data of the second word */
+	words_ptr[1] = ntohl((__force __be32)dsa_words_ptr[1]);
+
+	/* check the extended bit */
+	if (FIELD_GET(W1_MASK_EXT_BIT, words_ptr[1]) == 0)
+		/* 2 words DSA tag is not supported */
+		return -EINVAL;
+
+	/* copy the data of the third word */
+	words_ptr[2] = ntohl((__force __be32)dsa_words_ptr[2]);
+
+	/* check the extended bit */
+	if (FIELD_GET(W2_MASK_EXT_BIT, words_ptr[1]) == 0)
+		/* 3 words DSA tag is not supported */
+		return -EINVAL;
+
+	/* copy the data of the forth word */
+	words_ptr[3] = ntohl((__force __be32)dsa_words_ptr[3]);
+
+	/* VID */
+	get_value = FIELD_GET(W3_MASK_VID, words_ptr[3]);
+	dsa_info_ptr->common_params.vid &= VID_MASK;
+	dsa_info_ptr->common_params.vid |= FIELD_PREP(VID_MASK, get_value);
+
+	dsa_info_ptr->common_params.cfi_bit =
+	    (u8)FIELD_GET(W1_MASK_CFI_BIT, words_ptr[1]);
+
+	return net_if_dsa_to_cpu_parse(words_ptr, dsa_info_ptr);
+}
+
+static int net_if_dsa_tag_from_cpu_build(const struct mvsw_pr_dsa *dsa_info_ptr,
+					 u32 *words_ptr)
+{
+	u32 trg_hw_dev = 0;
+	u32 trg_port = 0;
+	const struct mvsw_pr_dsa_from_cpu *from_cpu_ptr =
+	    &dsa_info_ptr->dsa_info.from_cpu;
+
+	if (unlikely(from_cpu_ptr->dst_iface.type != MVSW_IF_PORT_E))
+		/* only sending to port interface is supported */
+		return -EINVAL;
+
+	words_ptr[0] |=
+	    FIELD_PREP(W0_MASK_DSA_CMD, MVSW_NET_DSA_CMD_FROM_CPU_E);
+
+	trg_hw_dev = from_cpu_ptr->dst_iface.dev_port.hw_dev_num;
+	trg_port = from_cpu_ptr->dst_iface.dev_port.port_num;
+
+	if (trg_hw_dev >= BIT(12))
+		return -EINVAL;
+
+	if (trg_port >= BIT(8) || trg_port >= BIT(10))
+		return -EINVAL;
+
+	words_ptr[0] |= FIELD_PREP(W0_MASK_HW_DEV_NUM, trg_hw_dev);
+	words_ptr[3] |= FIELD_PREP(W3_MASK_HW_DEV_NUM, (trg_hw_dev >> 5));
+
+	if (dsa_info_ptr->common_params.cfi_bit == 1)
+		words_ptr[0] |= FIELD_PREP(W0_MASK_CFI_BIT, 1);
+
+	words_ptr[0] |= FIELD_PREP(W0_MASK_VPT,
+				   dsa_info_ptr->common_params.vpt);
+	words_ptr[0] |= FIELD_PREP(W0_MASK_VID,
+				   dsa_info_ptr->common_params.vid);
+
+	/* set extended bits */
+	words_ptr[0] |= FIELD_PREP(W0_MASK_EXT_BIT, 1);
+	words_ptr[1] |= FIELD_PREP(W1_MASK_EXT_BIT, 1);
+	words_ptr[2] |= FIELD_PREP(W2_MASK_EXT_BIT, 1);
+
+	if (from_cpu_ptr->egr_filter_en)
+		words_ptr[1] |= FIELD_PREP(W1_MASK_EGR_FILTER_EN, 1);
+
+	if (from_cpu_ptr->egr_filter_registered)
+		words_ptr[1] |= FIELD_PREP(W1_MASK_EGR_FILTER_REG, 1);
+
+	/* check src_id & src_hw_dev */
+	if (from_cpu_ptr->src_id >= BIT(12) ||
+	    from_cpu_ptr->src_hw_dev >= BIT(12)) {
+		return -EINVAL;
+	}
+
+	words_ptr[1] |= FIELD_PREP(W1_MASK_SRC_ID, from_cpu_ptr->src_id);
+	words_ptr[1] |= FIELD_PREP(W1_MASK_SRC_DEV, from_cpu_ptr->src_hw_dev);
+
+	words_ptr[2] |= FIELD_PREP(W2_MASK_SRC_ID, from_cpu_ptr->src_id >> 5);
+	words_ptr[2] |= FIELD_PREP(W2_MASK_SRC_DEV,
+				   from_cpu_ptr->src_hw_dev >> 5);
+
+	/* bits 0:9 -- reserved with value 0 */
+	if (from_cpu_ptr->dst_eport >= BIT(17))
+		return -EINVAL;
+
+	words_ptr[3] |= FIELD_PREP(W3_MASK_DST_EPORT, from_cpu_ptr->dst_eport);
+	words_ptr[3] |= FIELD_PREP(W3_MASK_VID,
+				   (dsa_info_ptr->common_params.vid >> 12));
+
+	return 0;
+}
+
+int mvsw_pr_dsa_build(const struct mvsw_pr_dsa *dsa_info_ptr,
+		      u8 *dsa_bytes_ptr)
+{
+	int rc;
+	u32 words_ptr[4] = { 0 };	/* 4 words of DSA tag */
+	__be32 *dsa_words_ptr = (__be32 *)dsa_bytes_ptr;
+
+	if (unlikely(!dsa_info_ptr || !dsa_bytes_ptr))
+		return -EINVAL;
+
+	if (dsa_info_ptr->common_params.cfi_bit >= BIT(1) ||
+	    dsa_info_ptr->common_params.vpt >= BIT(3)) {
+		return -EINVAL;
+	}
+
+	if (unlikely(dsa_info_ptr->dsa_cmd != MVSW_NET_DSA_CMD_FROM_CPU_E))
+		return -EINVAL;
+
+	/* build form CPU DSA tag */
+	rc = net_if_dsa_tag_from_cpu_build(dsa_info_ptr, words_ptr);
+	if (rc != 0)
+		return rc;
+
+	dsa_words_ptr[0] = htonl(words_ptr[0]);
+	dsa_words_ptr[1] = htonl(words_ptr[1]);
+	dsa_words_ptr[2] = htonl(words_ptr[2]);
+	dsa_words_ptr[3] = htonl(words_ptr[3]);
+
+	return 0;
+}
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_dsa.h b/drivers/net/ethernet/marvell/prestera_sw/prestera_dsa.h
new file mode 100644
index 0000000..f65d110
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_dsa.h
@@ -0,0 +1,88 @@
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+#ifndef _MVSW_PRESTERA_DSA_H_
+#define _MVSW_PRESTERA_DSA_H_
+
+#include <linux/types.h>
+
+#define MVSW_PR_DSA_HLEN	16
+
+enum mvsw_pr_dsa_cmd {
+	/* DSA command is "To CPU" */
+	MVSW_NET_DSA_CMD_TO_CPU_E = 0,
+
+	/* DSA command is "FROM CPU" */
+	MVSW_NET_DSA_CMD_FROM_CPU_E,
+};
+
+enum mvsw_pr_if_type {
+	/* the interface is of port type (dev,port) */
+	MVSW_IF_PORT_E = 0,
+
+	/* the interface is of trunk type (trunkId) */
+	MVSW_IF_TRUNK_E = 1,
+
+	/* the interface is of Vid type (vlan-id) */
+	MVSW_IF_VID_E = 3,
+};
+
+struct mvsw_pr_dsa_common {
+	/* the value vlan priority tag (APPLICABLE RANGES: 0..7) */
+	u8 vpt;
+
+	/* CFI bit of the vlan tag (APPLICABLE RANGES: 0..1) */
+	u8 cfi_bit;
+
+	/* Vlan id */
+	u16 vid;
+};
+
+struct mvsw_pr_iface_info {
+	enum mvsw_pr_if_type type;
+	struct {
+		u32 hw_dev_num;
+		u32 port_num;
+	} dev_port;
+	u16 trunk_id;
+	u16 vlan_id;
+	u32 hw_dev_num;
+};
+
+struct mvsw_pr_dsa_to_cpu {
+	bool is_tagged;
+	u32 hw_dev_num;
+	bool src_is_trunk;
+	struct {
+		u16 src_trunk_id;
+		u32 port_num;
+		u32 eport;
+	} iface;
+};
+
+struct mvsw_pr_dsa_from_cpu {
+	struct mvsw_pr_iface_info dst_iface;	/* vid/port */
+	bool egr_filter_en;
+	bool egr_filter_registered;
+	u32 src_id;
+	u32 src_hw_dev;
+	u32 dst_eport;	/* for port but not for vid */
+};
+
+struct mvsw_pr_dsa {
+	struct mvsw_pr_dsa_common common_params;
+	enum mvsw_pr_dsa_cmd dsa_cmd;
+	union {
+		struct mvsw_pr_dsa_to_cpu to_cpu;
+		struct mvsw_pr_dsa_from_cpu from_cpu;
+	} dsa_info;
+};
+
+int mvsw_pr_dsa_parse(const u8 *dsa_bytes_ptr,
+		      struct mvsw_pr_dsa *dsa_info_ptr);
+int mvsw_pr_dsa_build(const struct mvsw_pr_dsa *dsa_info_ptr,
+		      u8 *dsa_bytes_ptr);
+
+#endif /* _MVSW_PRESTERA_DSA_H_ */
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_fw_log.c b/drivers/net/ethernet/marvell/prestera_sw/prestera_fw_log.c
new file mode 100644
index 0000000..803adee
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_fw_log.c
@@ -0,0 +1,445 @@
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+#include <linux/sysfs.h>
+#include <linux/fs.h>
+#include <linux/etherdevice.h>
+#include <linux/string.h>
+#include <linux/ctype.h>
+#include <linux/debugfs.h>
+
+#include "prestera.h"
+#include "prestera_hw.h"
+#include "prestera_log.h"
+#include "prestera_fw_log.h"
+
+#define FW_LOG_DBGFS_CFG_DIR "mvsw_pr_fw_log"
+#define FW_LOG_DBGFS_CFG_NAME "cfg"
+#define FW_LOG_DBGFS_MAX_STR_LEN 64
+#define FW_LOG_PR_LOG_PREFIX "[mvsw_pr_fw_log]"
+#define FW_LOG_PR_LIB_SIZE 32
+#define FW_LOG_PR_READ_BUF_STATIC_SIZE 4095
+#define MVSW_FW_LOG_INFO(fmt, ...) \
+	pr_info(fmt, ##__VA_ARGS__)
+
+#define FW_LOG_SETTINGS_GET(ptr, lib, type)				\
+	(((ptr[lib] & (1 << type)) >> type) & 1)
+
+#define FW_LOG_SETTINGS_TOG_TYPE(ptr, lib, type)			\
+	(ptr[lib] ^= (1 << type))
+
+#define FW_LOG_SETTINGS_CLR(ptr)					\
+	memset(ptr, 0, FW_LOG_LIB_MAX * 2)
+
+#define FW_LOG_SETTINGS_CLR_LIB(ptr, lib)				\
+	(ptr[lib] = 0)
+
+#define FW_LOG_SETTINGS_CLR_TYPE(ptr, lib, type)			\
+	(ptr[lib] &= ~(1 << type))
+
+static void mvsw_pr_fw_log_evt_handler(struct mvsw_pr_switch *,
+				       struct mvsw_pr_event *);
+static ssize_t mvsw_pr_fw_log_debugfs_read(struct file *file,
+					   char __user *ubuf,
+					   size_t count, loff_t *ppos);
+static ssize_t mvsw_pr_fw_log_debugfs_write(struct file *file,
+					    const char __user *ubuf,
+					    size_t count, loff_t *ppos);
+static inline int mvsw_pr_fw_log_get_type_from_str(const char *str);
+static inline int mvsw_pr_fw_log_get_lib_from_str(const char *str);
+
+static int mvsw_pr_fw_log_event_handler_register(struct mvsw_pr_switch *sw);
+static void mvsw_pr_fw_log_event_handler_unregister(struct mvsw_pr_switch *sw);
+
+enum {
+	FW_LOG_LIB_BRIDGE,
+	FW_LOG_LIB_CNC,
+	FW_LOG_LIB_CONFIG,
+	FW_LOG_LIB_COS,
+	FW_LOG_LIB_HW_INIT,
+	FW_LOG_LIB_CSCD,
+	FW_LOG_LIB_CUT_THROUGH,
+	FW_LOG_LIB_DIAG,
+	FW_LOG_LIB_FABRIC,
+	FW_LOG_LIB_IP,
+	FW_LOG_LIB_IPFIX,
+	FW_LOG_LIB_IP_LPM,
+	FW_LOG_LIB_L2_MLL,
+	FW_LOG_LIB_LOGICAL_TARGET,
+	FW_LOG_LIB_LPM,
+	FW_LOG_LIB_MIRROR,
+	FW_LOG_LIB_MULTI_PORT_GROUP,
+	FW_LOG_LIB_NETWORK_IF,
+	FW_LOG_LIB_NST,
+	FW_LOG_LIB_OAM,
+	FW_LOG_LIB_PCL,
+	FW_LOG_LIB_PHY,
+	FW_LOG_LIB_POLICER,
+	FW_LOG_LIB_PORT,
+	FW_LOG_LIB_PROTECTION,
+	FW_LOG_LIB_PTP,
+	FW_LOG_LIB_SYSTEM_RECOVERY,
+	FW_LOG_LIB_TCAM,
+	FW_LOG_LIB_TM_GLUE,
+	FW_LOG_LIB_TRUNK,
+	FW_LOG_LIB_TTI,
+	FW_LOG_LIB_TUNNEL,
+	FW_LOG_LIB_VNT,
+	FW_LOG_LIB_RESOURCE_MANAGER,
+	FW_LOG_LIB_VERSION,
+	FW_LOG_LIB_TM,
+	FW_LOG_LIB_SMI,
+	FW_LOG_LIB_INIT,
+	FW_LOG_LIB_DRAGONITE,
+	FW_LOG_LIB_VIRTUAL_TCAM,
+	FW_LOG_LIB_INGRESS,
+	FW_LOG_LIB_EGRESS,
+	FW_LOG_LIB_LATENCY_MONITORING,
+	FW_LOG_LIB_TAM,
+	FW_LOG_LIB_EXACT_MATCH,
+	FW_LOG_LIB_PHA,
+	FW_LOG_LIB_PACKET_ANALYZER,
+	FW_LOG_LIB_FLOW_MANAGER,
+	FW_LOG_LIB_BRIDGE_FDB_MANAGER,
+	FW_LOG_LIB_I2C,
+	FW_LOG_LIB_ALL,
+
+	FW_LOG_LIB_MAX
+};
+
+enum {
+	FW_LOG_TYPE_INFO,
+	FW_LOG_TYPE_ENTRY_LEVEL_FUNCTION,
+	FW_LOG_TYPE_ERROR,
+	FW_LOG_TYPE_ALL,
+	FW_LOG_TYPE_NONE,
+
+	FW_LOG_TYPE_MAX
+};
+
+struct mvsw_pr_fw_log_prv_debugfs {
+	struct dentry *cfg_dir;
+	struct dentry *cfg;
+	const struct file_operations cfg_fops;
+	char *read_buf;
+	ssize_t read_buf_size;
+};
+
+static u16 fw_log_lib_type_settings[FW_LOG_LIB_MAX] = { 0 };
+
+static struct mvsw_pr_fw_log_prv_debugfs fw_log_debugfs_handle = {
+	.cfg_dir = NULL,
+	.cfg_fops = {
+		.read = mvsw_pr_fw_log_debugfs_read,
+		.write = mvsw_pr_fw_log_debugfs_write,
+		.open = simple_open,
+		.llseek = default_llseek,
+	}
+};
+
+static const char *mvsw_pr_fw_log_prv_lib_str_enu[FW_LOG_LIB_MAX] = {
+	[FW_LOG_LIB_ALL] =  "all",
+	[FW_LOG_LIB_BRIDGE] =  "bridge",
+	[FW_LOG_LIB_CNC] =  "cnc",
+	[FW_LOG_LIB_CONFIG] =  "config",
+	[FW_LOG_LIB_COS] =  "cos",
+	[FW_LOG_LIB_CSCD] =  "cscd",
+	[FW_LOG_LIB_CUT_THROUGH] =  "cut-through",
+	[FW_LOG_LIB_DIAG] =  "diag",
+	[FW_LOG_LIB_DRAGONITE] =  "dragonite",
+	[FW_LOG_LIB_EGRESS] =  "egress",
+	[FW_LOG_LIB_EXACT_MATCH] =  "exact-match",
+	[FW_LOG_LIB_FABRIC] =  "fabric",
+	[FW_LOG_LIB_BRIDGE_FDB_MANAGER] =  "fdb-manager",
+	[FW_LOG_LIB_FLOW_MANAGER] =  "flow-manager",
+	[FW_LOG_LIB_HW_INIT] =  "hw-init",
+	[FW_LOG_LIB_I2C] =  "I2C_UNDEFINED",
+	[FW_LOG_LIB_INGRESS] =  "ingress",
+	[FW_LOG_LIB_INIT] =  "init",
+	[FW_LOG_LIB_IPFIX] =  "ipfix",
+	[FW_LOG_LIB_IP] =  "ip",
+	[FW_LOG_LIB_IP_LPM] =  "ip-lpm",
+	[FW_LOG_LIB_L2_MLL] =  "l2-mll",
+	[FW_LOG_LIB_LATENCY_MONITORING] =  "latency-monitoring",
+	[FW_LOG_LIB_LOGICAL_TARGET] =  "logical-target",
+	[FW_LOG_LIB_LPM] =  "lpm",
+	[FW_LOG_LIB_MIRROR] =  "mirror",
+	[FW_LOG_LIB_MULTI_PORT_GROUP] =  "multi-port-group",
+	[FW_LOG_LIB_NETWORK_IF] =  "network-if",
+	[FW_LOG_LIB_NST] =  "nst",
+	[FW_LOG_LIB_OAM] =  "oam",
+	[FW_LOG_LIB_PACKET_ANALYZER] =  "packet-analyzer",
+	[FW_LOG_LIB_PCL] =  "pcl",
+	[FW_LOG_LIB_PHA] =  "pha",
+	[FW_LOG_LIB_PHY] =  "phy",
+	[FW_LOG_LIB_POLICER] =  "policer",
+	[FW_LOG_LIB_PROTECTION] =  "protection",
+	[FW_LOG_LIB_PTP] =  "ptp",
+	[FW_LOG_LIB_RESOURCE_MANAGER] =  "resource-manager",
+	[FW_LOG_LIB_SMI] =  "smi",
+	[FW_LOG_LIB_SYSTEM_RECOVERY] =  "system-recovery",
+	[FW_LOG_LIB_TAM] =  "tam",
+	[FW_LOG_LIB_TCAM] =  "tcam",
+	[FW_LOG_LIB_TM] =  "tm",
+	[FW_LOG_LIB_TM_GLUE] =  "tm-glue",
+	[FW_LOG_LIB_TRUNK] =  "trunk",
+	[FW_LOG_LIB_TTI] =  "tti",
+	[FW_LOG_LIB_TUNNEL] =  "tunnel",
+	[FW_LOG_LIB_VERSION] =  "version",
+	[FW_LOG_LIB_VIRTUAL_TCAM] =  "virtual-tcam",
+	[FW_LOG_LIB_VNT] =  "vnt"
+};
+
+static const char *mvsw_pr_fw_log_prv_type_str_enu[FW_LOG_TYPE_MAX] = {
+	[FW_LOG_TYPE_INFO] = "info",
+	[FW_LOG_TYPE_ENTRY_LEVEL_FUNCTION] = "entry-level-function",
+	[FW_LOG_TYPE_ERROR] = "error",
+	[FW_LOG_TYPE_ALL] = "all",
+	[FW_LOG_TYPE_NONE]  = "none",
+};
+
+static void mvsw_pr_fw_log_evt_handler(struct mvsw_pr_switch *sw,
+				       struct mvsw_pr_event *evt)
+{
+	u32 log_len = evt->fw_log_evt.log_len;
+	u8 *buf = evt->fw_log_evt.data;
+
+	buf[log_len] = '\0';
+
+	MVSW_FW_LOG_INFO(FW_LOG_PR_LOG_PREFIX "%s\n", buf);
+}
+
+static ssize_t mvsw_pr_fw_log_debugfs_read(struct file *file,
+					   char __user *ubuf,
+					   size_t count, loff_t *ppos)
+{
+	char *buf = fw_log_debugfs_handle.read_buf;
+	int lib, type;
+
+	memset(fw_log_debugfs_handle.read_buf, 0,
+	       fw_log_debugfs_handle.read_buf_size);
+
+	for (type = 0; type < FW_LOG_TYPE_MAX; ++type) {
+		if (type == FW_LOG_TYPE_NONE || type == FW_LOG_TYPE_ALL)
+			continue;
+		strcat(buf, mvsw_pr_fw_log_prv_type_str_enu[type]);
+		strcat(buf, "\t");
+	}
+
+	strcat(buf, "\n");
+
+	for (lib = 0; lib < FW_LOG_LIB_MAX; ++lib) {
+		if (lib == FW_LOG_LIB_MAX ||
+		    !mvsw_pr_fw_log_prv_lib_str_enu[lib])
+			continue;
+		strcat(buf, mvsw_pr_fw_log_prv_lib_str_enu[lib]);
+
+		for (type = 0; type < FW_LOG_TYPE_MAX; ++type) {
+			if (type == FW_LOG_TYPE_NONE || type == FW_LOG_TYPE_ALL)
+				continue;
+			strcat(buf, "\t");
+			strcat(buf,
+			       FW_LOG_SETTINGS_GET(fw_log_lib_type_settings,
+						   lib, type) ? "+" : "-");
+		}
+		strcat(buf, "\n");
+	}
+
+	return simple_read_from_buffer(ubuf, count, ppos, buf, strlen(buf));
+}
+
+static ssize_t mvsw_pr_fw_log_debugfs_write(struct file *file,
+					    const char __user *ubuf,
+					    size_t count, loff_t *ppos)
+{
+	u8 tmp_buf[FW_LOG_DBGFS_MAX_STR_LEN] = { 0 };
+	u8 lib_str[FW_LOG_PR_LIB_SIZE] = { 0 };
+	u8 type_str[FW_LOG_PR_LIB_SIZE] = { 0 };
+	u8 *ppos_lib, *ppos_type;
+	int err;
+	int lib, type;
+	int lib_name;
+	int log_type;
+	ssize_t len_to_copy = count - 1;
+	struct mvsw_pr_switch *sw = file->private_data;
+	char *end = tmp_buf;
+
+	if (len_to_copy > FW_LOG_DBGFS_MAX_STR_LEN) {
+		MVSW_LOG_ERROR("Len is > than max(%zu vs max possible %d)\n",
+			       count, FW_LOG_DBGFS_MAX_STR_LEN);
+		return -EMSGSIZE;
+	}
+
+	err = copy_from_user(tmp_buf, ubuf, len_to_copy);
+	if (err) {
+		err = -EINVAL;
+		goto error;
+	}
+
+	ppos_lib  = strsep(&end, " \t");
+	ppos_type = strsep(&end, " \t\0");
+
+	if (!ppos_lib || !ppos_type) {
+		err = -EINVAL;
+		goto error;
+	}
+
+	strcpy(lib_str, ppos_lib);
+
+	strcpy(type_str, ppos_type);
+
+	if (iscntrl(lib_str[0]) || isspace(lib_str[0]) || lib_str[0] == '\0' ||
+	    iscntrl(type_str[0]) || isspace(type_str[0]) ||
+	    type_str[0] == '\0') {
+		err = -EINVAL;
+		goto error;
+	}
+
+	lib_name = mvsw_pr_fw_log_get_lib_from_str(lib_str);
+	log_type = mvsw_pr_fw_log_get_type_from_str(type_str);
+
+	if (lib_name >= FW_LOG_LIB_MAX || log_type >= FW_LOG_TYPE_MAX) {
+		err = -EINVAL;
+		goto error;
+	}
+
+	if (lib_name == FW_LOG_LIB_ALL && log_type == FW_LOG_TYPE_NONE) {
+		FW_LOG_SETTINGS_CLR(fw_log_lib_type_settings);
+	} else if (lib_name == FW_LOG_LIB_ALL && log_type == FW_LOG_TYPE_ALL) {
+		FW_LOG_SETTINGS_CLR(fw_log_lib_type_settings);
+		for (lib = 0; lib < FW_LOG_LIB_MAX; ++lib) {
+			for (type = 0; type < FW_LOG_TYPE_ALL; ++type) {
+				FW_LOG_SETTINGS_TOG_TYPE
+				    (fw_log_lib_type_settings, lib, type);
+			}
+		}
+	} else if (lib_name == FW_LOG_LIB_ALL) {
+		for (lib = 0; lib < FW_LOG_LIB_MAX; ++lib) {
+			FW_LOG_SETTINGS_CLR_TYPE(fw_log_lib_type_settings,
+						 lib, log_type);
+			FW_LOG_SETTINGS_TOG_TYPE(fw_log_lib_type_settings,
+						 lib, log_type);
+		}
+	} else if (log_type == FW_LOG_TYPE_ALL) {
+		FW_LOG_SETTINGS_CLR_LIB(fw_log_lib_type_settings, lib_name);
+		for (type = 0; type < FW_LOG_TYPE_ALL; ++type) {
+			FW_LOG_SETTINGS_TOG_TYPE(fw_log_lib_type_settings,
+						 lib_name, type);
+		}
+	} else {
+		FW_LOG_SETTINGS_TOG_TYPE(fw_log_lib_type_settings,
+					 lib_name, log_type);
+	}
+
+	mvsw_pr_hw_fw_log_level_set(sw, lib_name, log_type);
+
+	return count;
+
+error:
+	MVSW_LOG_ERROR("Invalid str received, make sure request is valid\n");
+	MVSW_LOG_ERROR("Valid fmt consists of: \"lib type\" string, e.g:\n");
+	MVSW_LOG_ERROR("\"phy error\" for 'phy' lib 'error' logs enabled\n");
+
+	return err;
+}
+
+static inline int mvsw_pr_fw_log_get_type_from_str(const char *str)
+{
+	int i;
+
+	for (i = 0; i < FW_LOG_TYPE_MAX; ++i) {
+		if (!mvsw_pr_fw_log_prv_type_str_enu[i])
+			continue;
+		if (strcmp(mvsw_pr_fw_log_prv_type_str_enu[i], str) == 0)
+			return i;
+	}
+
+	return FW_LOG_TYPE_MAX;
+}
+
+static inline int mvsw_pr_fw_log_get_lib_from_str(const char *str)
+{
+	int i;
+
+	for (i = 0; i < FW_LOG_LIB_MAX; ++i) {
+		if (!mvsw_pr_fw_log_prv_lib_str_enu[i])
+			continue;
+		if (strcmp(mvsw_pr_fw_log_prv_lib_str_enu[i], str) == 0)
+			return i;
+	}
+
+	return FW_LOG_LIB_MAX;
+}
+
+static int mvsw_pr_fw_log_event_handler_register(struct mvsw_pr_switch *sw)
+{
+	struct mvsw_pr_event_handler evt_handler = {
+		.func = mvsw_pr_fw_log_evt_handler,
+		.type = MVSW_EVENT_TYPE_FW_LOG,
+		.ctx = NULL,
+	};
+
+	return mvsw_pr_hw_event_handler_register(sw, &evt_handler);
+}
+
+static void mvsw_pr_fw_log_event_handler_unregister(struct mvsw_pr_switch *sw)
+{
+	struct mvsw_pr_event_handler eh;
+
+	eh.type = MVSW_EVENT_TYPE_FW_LOG;
+	eh.func = mvsw_pr_fw_log_evt_handler;
+
+	mvsw_pr_hw_event_handler_unregister(sw, &eh);
+}
+
+int mvsw_pr_fw_log_init(struct mvsw_pr_switch *sw)
+{
+	fw_log_debugfs_handle.cfg_dir =
+		debugfs_create_dir(FW_LOG_DBGFS_CFG_DIR, NULL);
+
+	if (!fw_log_debugfs_handle.cfg_dir) {
+		MVSW_LOG_ERROR("Failed to create debugfs dir entry");
+		return -1;
+	}
+
+	fw_log_debugfs_handle.cfg =
+		debugfs_create_file(FW_LOG_DBGFS_CFG_NAME, 0644,
+				    fw_log_debugfs_handle.cfg_dir, sw,
+				    &fw_log_debugfs_handle.cfg_fops);
+
+	if (!fw_log_debugfs_handle.cfg) {
+		MVSW_LOG_ERROR("Failed to create debugfs dir entry");
+		debugfs_remove(fw_log_debugfs_handle.cfg_dir);
+		return -1;
+	}
+
+	if (mvsw_pr_fw_log_event_handler_register(sw))
+		goto error;
+
+	fw_log_debugfs_handle.read_buf_size = FW_LOG_PR_READ_BUF_STATIC_SIZE;
+
+	// +1 for null term char '\0'
+	fw_log_debugfs_handle.read_buf =
+		kzalloc(fw_log_debugfs_handle.read_buf_size + 1, GFP_KERNEL);
+
+	if (!fw_log_debugfs_handle.read_buf)
+		goto error;
+
+	return 0;
+error:
+	debugfs_remove(fw_log_debugfs_handle.cfg);
+	debugfs_remove(fw_log_debugfs_handle.cfg_dir);
+	return -1;
+}
+
+void mvsw_pr_fw_log_fini(struct mvsw_pr_switch *sw)
+{
+	mvsw_pr_fw_log_event_handler_unregister(sw);
+
+	kfree(fw_log_debugfs_handle.read_buf);
+
+	debugfs_remove(fw_log_debugfs_handle.cfg);
+	debugfs_remove(fw_log_debugfs_handle.cfg_dir);
+}
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_fw_log.h b/drivers/net/ethernet/marvell/prestera_sw/prestera_fw_log.h
new file mode 100644
index 0000000..ccd5514
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_fw_log.h
@@ -0,0 +1,15 @@
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+
+#ifndef _MVSW_PRESTERA_FW_LOG_H_
+#define _MVSW_PRESTERA_FW_LOG_H_
+
+#include "prestera.h"
+
+int  mvsw_pr_fw_log_init(struct mvsw_pr_switch *sw);
+void mvsw_pr_fw_log_fini(struct mvsw_pr_switch *sw);
+
+#endif /* _MVSW_PRESTERA_FW_LOG_H_ */
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_hw.c b/drivers/net/ethernet/marvell/prestera_sw/prestera_hw.c
new file mode 100644
index 0000000..cab7f61
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_hw.c
@@ -0,0 +1,1062 @@
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+#include <linux/etherdevice.h>
+#include <linux/ethtool.h>
+#include <linux/netdevice.h>
+#include <linux/list.h>
+
+#include "prestera_hw.h"
+#include "prestera.h"
+#include "prestera_log.h"
+#include "prestera_fw_log.h"
+
+#define MVSW_PR_INIT_TIMEOUT 30000000	/* 30sec */
+#define MVSW_PR_MIN_MTU 64
+
+enum mvsw_msg_type {
+	MVSW_MSG_TYPE_SWITCH_UNSPEC,
+	MVSW_MSG_TYPE_SWITCH_INIT,
+
+	MVSW_MSG_TYPE_AGEING_TIMEOUT_SET,
+
+	MVSW_MSG_TYPE_PORT_ATTR_SET,
+	MVSW_MSG_TYPE_PORT_ATTR_GET,
+	MVSW_MSG_TYPE_PORT_INFO_GET,
+
+	MVSW_MSG_TYPE_VLAN_CREATE,
+	MVSW_MSG_TYPE_VLAN_DELETE,
+	MVSW_MSG_TYPE_VLAN_PORT_SET,
+	MVSW_MSG_TYPE_VLAN_PVID_SET,
+
+	MVSW_MSG_TYPE_FDB_ADD,
+	MVSW_MSG_TYPE_FDB_DELETE,
+	MVSW_MSG_TYPE_FDB_FLUSH_PORT,
+	MVSW_MSG_TYPE_FDB_FLUSH_VLAN,
+	MVSW_MSG_TYPE_FDB_FLUSH_PORT_VLAN,
+
+	MVSW_MSG_TYPE_LOG_LEVEL_SET,
+
+	MVSW_MSG_TYPE_BRIDGE_CREATE,
+	MVSW_MSG_TYPE_BRIDGE_DELETE,
+	MVSW_MSG_TYPE_BRIDGE_PORT_ADD,
+	MVSW_MSG_TYPE_BRIDGE_PORT_DELETE,
+
+	MVSW_MSG_TYPE_ACK,
+	MVSW_MSG_TYPE_MAX
+};
+
+enum mvsw_msg_port_attr {
+	MVSW_MSG_PORT_ATTR_ADMIN_STATE,
+	MVSW_MSG_PORT_ATTR_OPER_STATE,
+	MVSW_MSG_PORT_ATTR_MTU,
+	MVSW_MSG_PORT_ATTR_MAC,
+	MVSW_MSG_PORT_ATTR_SPEED,
+	MVSW_MSG_PORT_ATTR_ACCEPT_FRAME_TYPE,
+	MVSW_MSG_PORT_ATTR_LEARNING,
+	MVSW_MSG_PORT_ATTR_FLOOD,
+	MVSW_MSG_PORT_ATTR_CAPABILITY,
+	MVSW_MSG_PORT_ATTR_REMOTE_CAPABILITY,
+	MVSW_MSG_PORT_ATTR_LINK_MODE,
+	MVSW_MSG_PORT_ATTR_TYPE,
+	MVSW_MSG_PORT_ATTR_FEC,
+	MVSW_MSG_PORT_ATTR_AUTONEG,
+	MVSW_MSG_PORT_ATTR_DUPLEX,
+	MVSW_MSG_PORT_ATTR_STATS,
+	MVSW_MSG_PORT_ATTR_MDIX,
+	MVSW_MSG_PORT_ATTR_MAX
+};
+
+enum {
+	MVSW_MSG_ACK_OK,
+	MVSW_MSG_ACK_FAILED,
+	MVSW_MSG_ACK_MAX
+};
+
+enum {
+	MVSW_MODE_FORCED_MDI,
+	MVSW_MODE_FORCED_MDIX,
+	MVSW_MODE_AUTO_MDI,
+	MVSW_MODE_AUTO_MDIX,
+	MVSW_MODE_AUTO
+};
+
+struct mvsw_msg_cmd {
+	u32 type;
+} __packed __aligned(4);
+
+struct mvsw_msg_ret {
+	struct mvsw_msg_cmd cmd;
+	u32 status;
+} __packed __aligned(4);
+
+struct mvsw_msg_common_request {
+	struct mvsw_msg_cmd cmd;
+} __packed __aligned(4);
+
+struct mvsw_msg_common_response {
+	struct mvsw_msg_ret ret;
+} __packed __aligned(4);
+
+union mvsw_msg_switch_param {
+	u32 ageing_timeout;
+};
+
+struct mvsw_msg_switch_attr_cmd {
+	struct mvsw_msg_cmd cmd;
+	union mvsw_msg_switch_param param;
+} __packed __aligned(4);
+
+struct mvsw_msg_switch_init_ret {
+	struct mvsw_msg_ret ret;
+	u32 port_count;
+	u32 mtu_max;
+	u8  switch_id;
+	u8  mac[ETH_ALEN];
+} __packed __aligned(4);
+
+struct mvsw_msg_port_autoneg_param {
+	u64 link_mode;
+	u8  enable;
+	u8  fec;
+};
+
+struct mvsw_msg_port_cap_param {
+	u64 link_mode;
+	u8  type;
+	u8  fec;
+	u8  transceiver;
+};
+
+union mvsw_msg_port_param {
+	u8  admin_state;
+	u8  oper_state;
+	u32 mtu;
+	u8  mac[ETH_ALEN];
+	u8  accept_frm_type;
+	u8  learning;
+	u32 speed;
+	u8  flood;
+	u32 link_mode;
+	u8  type;
+	u8  duplex;
+	u8  fec;
+	u8  mdix;
+	struct mvsw_msg_port_autoneg_param autoneg;
+	struct mvsw_msg_port_cap_param cap;
+};
+
+struct mvsw_msg_port_attr_cmd {
+	struct mvsw_msg_cmd cmd;
+	u32 attr;
+	u32 port;
+	u32 dev;
+	union mvsw_msg_port_param param;
+} __packed __aligned(4);
+
+struct mvsw_msg_port_attr_ret {
+	struct mvsw_msg_ret ret;
+	union mvsw_msg_port_param param;
+} __packed __aligned(4);
+
+struct mvsw_msg_port_stats_ret {
+	struct mvsw_msg_ret ret;
+	u64 stats[MVSW_PORT_CNT_MAX];
+} __packed __aligned(4);
+
+struct mvsw_msg_port_info_cmd {
+	struct mvsw_msg_cmd cmd;
+	u32 port;
+} __packed __aligned(4);
+
+struct mvsw_msg_port_info_ret {
+	struct mvsw_msg_ret ret;
+	u32 hw_id;
+	u32 dev_id;
+	u16 fp_id;
+} __packed __aligned(4);
+
+struct mvsw_msg_vlan_cmd {
+	struct mvsw_msg_cmd cmd;
+	u32 port;
+	u32 dev;
+	u16 vid;
+	u8  is_member;
+	u8  is_tagged;
+} __packed __aligned(4);
+
+struct mvsw_msg_fdb_cmd {
+	struct mvsw_msg_cmd cmd;
+	u32 port;
+	u32 dev;
+	u8  mac[ETH_ALEN];
+	u16 vid;
+	u8  dynamic;
+	u32 flush_mode;
+} __packed __aligned(4);
+
+struct mvsw_msg_log_cmd {
+	struct mvsw_msg_cmd cmd;
+	u32 lib;
+	u32 level;
+} __packed __aligned(4);
+
+struct mvsw_msg_event {
+	u16 type;
+	u16 id;
+} __packed __aligned(4);
+
+struct mvsw_msg_event_log {
+	struct mvsw_msg_event id;
+	u32 log_string_size;
+	u8 log_string[0];
+} __packed __aligned(4);
+
+union mvsw_msg_event_fdb_param {
+	u8 mac[ETH_ALEN];
+};
+
+struct mvsw_msg_event_fdb {
+	struct mvsw_msg_event id;
+	u32 port_id;
+	u32 vid;
+	union mvsw_msg_event_fdb_param param;
+} __packed __aligned(4);
+
+union mvsw_msg_event_port_param {
+	u32 oper_state;
+};
+
+struct mvsw_msg_event_port {
+	struct mvsw_msg_event id;
+	u32 port_id;
+	union mvsw_msg_event_port_param param;
+} __packed __aligned(4);
+
+struct mvsw_msg_bridge_cmd {
+	struct mvsw_msg_cmd cmd;
+	u32 port;
+	u32 dev;
+	u16 bridge;
+} __packed __aligned(4);
+
+struct mvsw_msg_bridge_ret {
+	struct mvsw_msg_ret ret;
+	u16 bridge;
+} __packed __aligned(4);
+
+#define mvsw_pr_hw_check_resp(_response)			\
+({								\
+	int __er = 0;						\
+	typeof(_response) __r = (_response);			\
+	if (__r->ret.cmd.type != MVSW_MSG_TYPE_ACK)		\
+		__er = -EBADE;					\
+	else if (__r->ret.status != MVSW_MSG_ACK_OK)		\
+		__er = -EINVAL;					\
+	(__er);							\
+})
+
+#define mvsw_pr_hw_send_req_resp(_switch, _type, _request, _response)	\
+({								\
+	int __e;						\
+	size_t __s;						\
+	typeof(_switch) __sw = (_switch);			\
+	typeof(_request) __req = (_request);			\
+	typeof(_response) __resp = (_response);			\
+	__req->cmd.type = (_type);				\
+	__e = __sw->dev->bus->send_req(				\
+		__sw->dev, MVSW_BUS_SEND_SYNC,			\
+		(u8 *)__req, sizeof(*__req),			\
+		(u8 *)__resp, sizeof(*__resp),			\
+		&__s);						\
+	if (!__e)						\
+		__e = mvsw_pr_hw_check_resp(__resp);		\
+	(__e);							\
+})
+
+#define mvsw_pr_hw_send_req(_sw, _t, _req)			\
+({								\
+	struct mvsw_msg_common_response __re;			\
+	(mvsw_pr_hw_send_req_resp(_sw, _t, _req, &__re));	\
+})
+
+struct mvsw_pr_event_handler_item {
+	struct list_head list;
+	struct mvsw_pr_event_handler eh;
+};
+
+static int mvsw_pr_hw_parse_port_evt(u8 *msg, struct mvsw_pr_event *evt)
+{
+	struct mvsw_msg_event_port *hw_evt = (struct mvsw_msg_event_port *)msg;
+
+	evt->port_evt.port_id		= hw_evt->port_id;
+
+	switch (evt->id) {
+	case MVSW_PORT_EVENT_STATE_CHANGED:
+		evt->port_evt.data.oper_state	= hw_evt->param.oper_state;
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static int mvsw_pr_hw_parse_fdb_evt(u8 *msg, struct mvsw_pr_event *evt)
+{
+	struct mvsw_msg_event_fdb *hw_evt = (struct mvsw_msg_event_fdb *)msg;
+
+	evt->fdb_evt.port_id	= hw_evt->port_id;
+	evt->fdb_evt.vid	= hw_evt->vid;
+
+	memcpy(&evt->fdb_evt.data, &hw_evt->param, sizeof(u8) * ETH_ALEN);
+
+	return 0;
+}
+
+static int mvsw_pr_hw_parse_log_evt(u8 *msg, struct mvsw_pr_event *evt)
+{
+	struct mvsw_msg_event_log *hw_evt = (struct mvsw_msg_event_log *)msg;
+
+	evt->fw_log_evt.log_len	= hw_evt->log_string_size;
+	evt->fw_log_evt.data	= hw_evt->log_string;
+
+	return 0;
+}
+
+struct mvsw_pr_hw_evt_parser {
+	int (*func)(u8 *msg, struct mvsw_pr_event *evt);
+};
+
+static struct mvsw_pr_hw_evt_parser __event_parsers[MVSW_EVENT_TYPE_MAX] = {
+	[MVSW_EVENT_TYPE_PORT] = {.func = mvsw_pr_hw_parse_port_evt},
+	[MVSW_EVENT_TYPE_FDB] = {.func = mvsw_pr_hw_parse_fdb_evt},
+	[MVSW_EVENT_TYPE_FW_LOG] = {.func = mvsw_pr_hw_parse_log_evt}
+};
+
+static int mvsw_pr_device_recv(struct mvsw_pr_device *dev, u8 *buf, size_t size)
+{
+	struct mvsw_pr_event_handler_item *item;
+	struct mvsw_pr_switch *sw = dev->priv;
+	mvsw_pr_event_handler_cb cb = NULL;
+	struct mvsw_pr_event evt;
+	struct mvsw_msg_event *msg = (struct mvsw_msg_event *)buf;
+	int err;
+
+	if (msg->type >= MVSW_EVENT_TYPE_MAX)
+		return -EINVAL;
+
+	rcu_read_lock();
+	list_for_each_entry_rcu(item, &sw->event_handlers, list) {
+		if (msg->type == item->eh.type) {
+			cb = item->eh.func;
+			break;
+		}
+	}
+	rcu_read_unlock();
+
+	if (!cb || !__event_parsers[msg->type].func)
+		return 0;
+
+	evt.id = msg->id;
+
+	err = __event_parsers[msg->type].func(buf, &evt);
+	if (!err)
+		cb(sw, &evt);
+
+	return err;
+}
+
+int mvsw_pr_hw_port_info_get(const struct mvsw_pr_port *port,
+			     u16 *fp_id, u32 *hw_id, u32 *dev_id)
+{
+	int err;
+	struct mvsw_msg_port_info_ret resp;
+	struct mvsw_msg_port_info_cmd req = {
+		.port = port->id
+	};
+
+	err = mvsw_pr_hw_send_req_resp(port->sw, MVSW_MSG_TYPE_PORT_INFO_GET,
+				       &req, &resp);
+	if (err)
+		return err;
+
+	*hw_id = resp.hw_id;
+	*dev_id = resp.dev_id;
+	*fp_id = resp.fp_id;
+
+	return 0;
+}
+
+int mvsw_pr_hw_switch_init(struct mvsw_pr_switch *sw)
+{
+	int err = 0;
+	u32 default_timeout;
+	struct mvsw_pr_bus *bus = sw->dev->bus;
+	struct mvsw_msg_switch_init_ret resp;
+	struct mvsw_msg_common_request req;
+
+	INIT_LIST_HEAD(&sw->event_handlers);
+
+	/* Init might take a lot of time, the timeout should be increased */
+	default_timeout = bus->timeout;
+	bus->timeout = MVSW_PR_INIT_TIMEOUT;
+	err = mvsw_pr_hw_send_req_resp(sw, MVSW_MSG_TYPE_SWITCH_INIT,
+				       &req, &resp);
+	bus->timeout = default_timeout;
+	if (err)
+		return err;
+
+	sw->id = resp.switch_id;
+	sw->port_count = resp.port_count;
+	sw->mtu_min = MVSW_PR_MIN_MTU;
+	sw->mtu_max = resp.mtu_max;
+	sw->dev->recv_msg = mvsw_pr_device_recv;
+	memcpy(sw->base_mac, resp.mac, ETH_ALEN);
+
+	return err;
+}
+
+int mvsw_pr_hw_switch_ageing_set(const struct mvsw_pr_switch *sw,
+				 u32 ageing_time)
+{
+	struct mvsw_msg_switch_attr_cmd req = {
+		.param = {.ageing_timeout = ageing_time}
+	};
+
+	return mvsw_pr_hw_send_req(sw, MVSW_MSG_TYPE_AGEING_TIMEOUT_SET, &req);
+}
+
+int mvsw_pr_hw_port_state_set(const struct mvsw_pr_port *port,
+			      bool admin_state)
+{
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_ADMIN_STATE,
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.param = {.admin_state = admin_state ? 1 : 0}
+	};
+
+	return mvsw_pr_hw_send_req(port->sw, MVSW_MSG_TYPE_PORT_ATTR_SET, &req);
+}
+
+int mvsw_pr_hw_port_state_get(const struct mvsw_pr_port *port,
+			      bool *admin_state, bool *oper_state)
+{
+	int err;
+	struct mvsw_msg_port_attr_ret resp;
+	struct mvsw_msg_port_attr_cmd req = {
+		.port = port->hw_id,
+		.dev = port->dev_id
+	};
+
+	if (admin_state) {
+		req.attr = MVSW_MSG_PORT_ATTR_ADMIN_STATE;
+		err = mvsw_pr_hw_send_req_resp(port->sw,
+					       MVSW_MSG_TYPE_PORT_ATTR_GET,
+					       &req, &resp);
+		if (err)
+			return err;
+		*admin_state = resp.param.admin_state != 0;
+	}
+
+	if (oper_state) {
+		req.attr = MVSW_MSG_PORT_ATTR_OPER_STATE;
+		err = mvsw_pr_hw_send_req_resp(port->sw,
+					       MVSW_MSG_TYPE_PORT_ATTR_GET,
+					       &req, &resp);
+		if (err)
+			return err;
+		*oper_state = resp.param.oper_state != 0;
+	}
+
+	return 0;
+}
+
+int mvsw_pr_hw_port_mtu_set(const struct mvsw_pr_port *port, u32 mtu)
+{
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_MTU,
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.param = {.mtu = mtu}
+	};
+
+	return mvsw_pr_hw_send_req(port->sw, MVSW_MSG_TYPE_PORT_ATTR_SET, &req);
+}
+
+int mvsw_pr_hw_port_mtu_get(const struct mvsw_pr_port *port, u32 *mtu)
+{
+	int err;
+	struct mvsw_msg_port_attr_ret resp;
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_MTU,
+		.port = port->hw_id,
+		.dev = port->dev_id
+	};
+
+	err = mvsw_pr_hw_send_req_resp(port->sw, MVSW_MSG_TYPE_PORT_ATTR_GET,
+				       &req, &resp);
+	if (err)
+		return err;
+
+	*mtu = resp.param.mtu;
+
+	return err;
+}
+
+int mvsw_pr_hw_port_mac_set(const struct mvsw_pr_port *port, char *mac)
+{
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_MAC,
+		.port = port->hw_id,
+		.dev = port->dev_id
+	};
+	memcpy(&req.param.mac, mac, sizeof(req.param.mac));
+
+	return mvsw_pr_hw_send_req(port->sw, MVSW_MSG_TYPE_PORT_ATTR_SET, &req);
+}
+
+int mvsw_pr_hw_port_mac_get(const struct mvsw_pr_port *port, char *mac)
+{
+	int err;
+	struct mvsw_msg_port_attr_ret resp;
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_MAC,
+		.port = port->hw_id,
+		.dev = port->dev_id
+	};
+
+	err = mvsw_pr_hw_send_req_resp(port->sw, MVSW_MSG_TYPE_PORT_ATTR_GET,
+				       &req, &resp);
+	if (err)
+		return err;
+
+	memcpy(mac, resp.param.mac, sizeof(resp.param.mac));
+
+	return err;
+}
+
+int mvsw_pr_hw_port_accept_frame_type_set(const struct mvsw_pr_port *port,
+					  enum mvsw_pr_accept_frame_type type)
+{
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_ACCEPT_FRAME_TYPE,
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.param = {.accept_frm_type = type}
+	};
+
+	return mvsw_pr_hw_send_req(port->sw, MVSW_MSG_TYPE_PORT_ATTR_SET, &req);
+}
+
+int mvsw_pr_hw_port_learning_set(const struct mvsw_pr_port *port, bool enable)
+{
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_LEARNING,
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.param = {.learning = enable ? 1 : 0}
+	};
+
+	return mvsw_pr_hw_send_req(port->sw, MVSW_MSG_TYPE_PORT_ATTR_SET, &req);
+}
+
+static struct mvsw_pr_event_handler_item *
+__find_event_handler_item(const struct mvsw_pr_switch *sw,
+			  const struct mvsw_pr_event_handler *eh)
+{
+	struct mvsw_pr_event_handler_item *item;
+
+	list_for_each_entry(item, &sw->event_handlers, list) {
+		if (eh->func == item->eh.func &&
+		    eh->type == item->eh.type)
+			return item;
+	}
+
+	return NULL;
+}
+
+int mvsw_pr_hw_event_handler_register(struct mvsw_pr_switch *sw,
+				      const struct mvsw_pr_event_handler *eh)
+{
+	struct mvsw_pr_event_handler_item *item;
+
+	item = __find_event_handler_item(sw, eh);
+	if (item)
+		return -EEXIST;
+	item = kmalloc(sizeof(*item), GFP_KERNEL);
+	if (!item)
+		return -ENOMEM;
+
+	item->eh = *eh;
+	INIT_LIST_HEAD(&item->list);
+
+	list_add_rcu(&item->list, &sw->event_handlers);
+
+	return 0;
+}
+
+void mvsw_pr_hw_event_handler_unregister(struct mvsw_pr_switch *sw,
+					 const struct mvsw_pr_event_handler *eh)
+{
+	struct mvsw_pr_event_handler_item *item;
+
+	item = __find_event_handler_item(sw, eh);
+	if (!item)
+		return;
+
+	list_del_rcu(&item->list);
+	synchronize_rcu();
+	kfree(item);
+}
+
+int mvsw_pr_hw_vlan_create(const struct mvsw_pr_switch *sw, u16 vid)
+{
+	struct mvsw_msg_vlan_cmd req = {
+		.vid = vid,
+	};
+
+	return mvsw_pr_hw_send_req(sw, MVSW_MSG_TYPE_VLAN_CREATE, &req);
+}
+
+int mvsw_pr_hw_vlan_delete(const struct mvsw_pr_switch *sw, u16 vid)
+{
+	struct mvsw_msg_vlan_cmd req = {
+		.vid = vid,
+	};
+
+	return mvsw_pr_hw_send_req(sw, MVSW_MSG_TYPE_VLAN_DELETE, &req);
+}
+
+int mvsw_pr_hw_vlan_port_set(const struct mvsw_pr_port *port,
+			     u16 vid, bool is_member, bool untagged)
+{
+	struct mvsw_msg_vlan_cmd req = {
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.vid = vid,
+		.is_member = is_member ? 1 : 0,
+		.is_tagged = untagged ? 0 : 1
+	};
+
+	return mvsw_pr_hw_send_req(port->sw, MVSW_MSG_TYPE_VLAN_PORT_SET, &req);
+}
+
+int mvsw_pr_hw_vlan_port_vid_set(const struct mvsw_pr_port *port, u16 vid)
+{
+	struct mvsw_msg_vlan_cmd req = {
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.vid = vid
+	};
+
+	return mvsw_pr_hw_send_req(port->sw, MVSW_MSG_TYPE_VLAN_PVID_SET, &req);
+}
+
+int mvsw_pr_hw_port_speed_get(const struct mvsw_pr_port *port, u32 *speed)
+{
+	int err;
+	struct mvsw_msg_port_attr_ret resp;
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_SPEED,
+		.port = port->hw_id,
+		.dev = port->dev_id
+	};
+
+	err = mvsw_pr_hw_send_req_resp(port->sw, MVSW_MSG_TYPE_PORT_ATTR_GET,
+				       &req, &resp);
+	if (err)
+		return err;
+
+	*speed = resp.param.speed;
+
+	return err;
+}
+
+int mvsw_pr_hw_port_flood_set(const struct mvsw_pr_port *port, bool flood)
+{
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_FLOOD,
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.param = {.flood = flood ? 1 : 0}
+	};
+
+	return mvsw_pr_hw_send_req(port->sw, MVSW_MSG_TYPE_PORT_ATTR_SET, &req);
+}
+
+int mvsw_pr_hw_fdb_add(const struct mvsw_pr_port *port,
+		       const unsigned char *mac, u16 vid, bool dynamic)
+{
+	struct mvsw_msg_fdb_cmd req = {
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.vid = vid,
+		.dynamic = dynamic ? 1 : 0
+	};
+
+	memcpy(req.mac, mac, sizeof(req.mac));
+
+	return mvsw_pr_hw_send_req(port->sw, MVSW_MSG_TYPE_FDB_ADD, &req);
+}
+
+int mvsw_pr_hw_fdb_del(const struct mvsw_pr_port *port,
+		       const unsigned char *mac, u16 vid)
+{
+	struct mvsw_msg_fdb_cmd req = {
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.vid = vid
+	};
+
+	memcpy(req.mac, mac, sizeof(req.mac));
+
+	return mvsw_pr_hw_send_req(port->sw, MVSW_MSG_TYPE_FDB_DELETE, &req);
+}
+
+int mvsw_pr_hw_port_cap_get(struct mvsw_pr_port *port)
+{
+	int err;
+	struct mvsw_msg_port_attr_ret resp;
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_CAPABILITY,
+		.port = port->hw_id,
+		.dev = port->dev_id
+	};
+
+	err = mvsw_pr_hw_send_req_resp(port->sw, MVSW_MSG_TYPE_PORT_ATTR_GET,
+				       &req, &resp);
+	if (err)
+		return err;
+
+	port->supp_link_modes = resp.param.cap.link_mode;
+	port->supp_fec = resp.param.cap.fec;
+	port->type = resp.param.cap.type;
+	port->transceiver = resp.param.cap.transceiver;
+
+	return err;
+}
+
+int mvsw_pr_hw_port_remote_cap_get(const struct mvsw_pr_port *port,
+				   u64 *link_mode_bitmap)
+{
+	int err;
+	struct mvsw_msg_port_attr_ret resp;
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_REMOTE_CAPABILITY,
+		.port = port->hw_id,
+		.dev = port->dev_id
+	};
+
+	err = mvsw_pr_hw_send_req_resp(port->sw, MVSW_MSG_TYPE_PORT_ATTR_GET,
+				       &req, &resp);
+	if (err)
+		return err;
+
+	*link_mode_bitmap = resp.param.cap.link_mode;
+
+	return err;
+}
+
+int mvsw_pr_hw_port_mdix_get(const struct mvsw_pr_port *port, u8 *mode)
+{
+	int err;
+	struct mvsw_msg_port_attr_ret resp;
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_MDIX,
+		.port = port->hw_id,
+		.dev = port->dev_id
+	};
+
+	err = mvsw_pr_hw_send_req_resp(port->sw, MVSW_MSG_TYPE_PORT_ATTR_GET,
+				       &req, &resp);
+	if (err)
+		return err;
+
+	switch (resp.param.mdix) {
+	case MVSW_MODE_FORCED_MDI:
+	case MVSW_MODE_AUTO_MDI:
+		*mode = ETH_TP_MDI;
+		break;
+
+	case MVSW_MODE_FORCED_MDIX:
+	case MVSW_MODE_AUTO_MDIX:
+		*mode = ETH_TP_MDI_X;
+		break;
+
+	default:
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+int mvsw_pr_hw_port_mdix_set(const struct mvsw_pr_port *port, u8 mode)
+{
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_MDIX,
+		.port = port->hw_id,
+		.dev = port->dev_id
+	};
+
+	switch (mode) {
+	case ETH_TP_MDI:
+		req.param.mdix = MVSW_MODE_FORCED_MDI;
+		break;
+
+	case ETH_TP_MDI_X:
+		req.param.mdix = MVSW_MODE_FORCED_MDIX;
+		break;
+
+	case ETH_TP_MDI_AUTO:
+		req.param.mdix = MVSW_MODE_AUTO;
+		break;
+
+	default:
+		return -EINVAL;
+	}
+
+	return mvsw_pr_hw_send_req(port->sw, MVSW_MSG_TYPE_PORT_ATTR_SET, &req);
+}
+
+int mvsw_pr_hw_port_type_get(const struct mvsw_pr_port *port, u8 *type)
+{
+	int err;
+	struct mvsw_msg_port_attr_ret resp;
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_TYPE,
+		.port = port->hw_id,
+		.dev = port->dev_id
+	};
+
+	err = mvsw_pr_hw_send_req_resp(port->sw, MVSW_MSG_TYPE_PORT_ATTR_GET,
+				       &req, &resp);
+	if (err)
+		return err;
+
+	*type = resp.param.type;
+
+	return err;
+}
+
+int mvsw_pr_hw_port_fec_get(const struct mvsw_pr_port *port, u8 *fec)
+{
+	int err;
+	struct mvsw_msg_port_attr_ret resp;
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_FEC,
+		.port = port->hw_id,
+		.dev = port->dev_id
+	};
+
+	err = mvsw_pr_hw_send_req_resp(port->sw, MVSW_MSG_TYPE_PORT_ATTR_GET,
+				       &req, &resp);
+	if (err)
+		return err;
+
+	*fec = resp.param.fec;
+
+	return err;
+}
+
+int mvsw_pr_hw_port_fec_set(const struct mvsw_pr_port *port, u8 fec)
+{
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_FEC,
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.param = {.fec = fec}
+	};
+
+	return mvsw_pr_hw_send_req(port->sw, MVSW_MSG_TYPE_PORT_ATTR_SET, &req);
+}
+
+int mvsw_pr_hw_fw_log_level_set(const struct mvsw_pr_switch *sw,
+				u32 lib_name, u32 log_level)
+{
+	struct mvsw_msg_log_cmd req = {
+		.lib = lib_name,
+		.level = log_level
+	};
+
+	return mvsw_pr_hw_send_req(sw, MVSW_MSG_TYPE_LOG_LEVEL_SET, &req);
+}
+
+int mvsw_pr_hw_port_autoneg_set(const struct mvsw_pr_port *port,
+				bool autoneg, u64 link_modes, u8 fec)
+{
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_AUTONEG,
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.param = {.autoneg = {.link_mode = link_modes,
+				      .enable = autoneg ? 1 : 0,
+				      .fec = fec}
+		}
+	};
+
+	return mvsw_pr_hw_send_req(port->sw, MVSW_MSG_TYPE_PORT_ATTR_SET, &req);
+}
+
+int mvsw_pr_hw_port_duplex_get(const struct mvsw_pr_port *port, u8 *duplex)
+{
+	int err;
+	struct mvsw_msg_port_attr_ret resp;
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_DUPLEX,
+		.port = port->hw_id,
+		.dev = port->dev_id
+	};
+
+	err = mvsw_pr_hw_send_req_resp(port->sw, MVSW_MSG_TYPE_PORT_ATTR_GET,
+				       &req, &resp);
+	if (err)
+		return err;
+
+	*duplex = resp.param.duplex;
+
+	return err;
+}
+
+int mvsw_pr_hw_port_stats_get(const struct mvsw_pr_port *port,
+			      u64 *cnt, size_t size)
+{
+	int err;
+	struct mvsw_msg_port_stats_ret resp;
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_STATS,
+		.port = port->hw_id,
+		.dev = port->dev_id
+	};
+
+	err = mvsw_pr_hw_send_req_resp(port->sw, MVSW_MSG_TYPE_PORT_ATTR_GET,
+				       &req, &resp);
+	if (err)
+		return err;
+
+	memcpy(cnt, resp.stats, size);
+	return 0;
+}
+
+int mvsw_pr_hw_bridge_create(const struct mvsw_pr_switch *sw, u16 *bridge_id)
+{
+	struct mvsw_msg_bridge_cmd req;
+	struct mvsw_msg_bridge_ret resp;
+	int err;
+
+	err = mvsw_pr_hw_send_req_resp(sw, MVSW_MSG_TYPE_BRIDGE_CREATE,
+				       &req, &resp);
+	if (err)
+		return err;
+
+	*bridge_id = resp.bridge;
+	return err;
+}
+
+int mvsw_pr_hw_bridge_delete(const struct mvsw_pr_switch *sw, u16 bridge_id)
+{
+	struct mvsw_msg_bridge_cmd req = {
+		.bridge = bridge_id
+	};
+
+	return mvsw_pr_hw_send_req(sw, MVSW_MSG_TYPE_BRIDGE_DELETE, &req);
+}
+
+int mvsw_pr_hw_bridge_port_add(const struct mvsw_pr_port *port, u16 bridge_id)
+{
+	struct mvsw_msg_bridge_cmd req = {
+		.bridge = bridge_id,
+		.port = port->hw_id,
+		.dev = port->dev_id
+	};
+
+	return mvsw_pr_hw_send_req(port->sw, MVSW_MSG_TYPE_BRIDGE_PORT_ADD,
+				   &req);
+}
+
+int mvsw_pr_hw_bridge_port_delete(const struct mvsw_pr_port *port,
+				  u16 bridge_id)
+{
+	struct mvsw_msg_bridge_cmd req = {
+		.bridge = bridge_id,
+		.port = port->hw_id,
+		.dev = port->dev_id
+	};
+
+	return mvsw_pr_hw_send_req(port->sw, MVSW_MSG_TYPE_BRIDGE_PORT_DELETE,
+				   &req);
+}
+
+int mvsw_pr_hw_fdb_flush_port(const struct mvsw_pr_port *port, u32 mode)
+{
+	struct mvsw_msg_fdb_cmd req = {
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.flush_mode = mode,
+	};
+
+	return  mvsw_pr_hw_send_req(port->sw, MVSW_MSG_TYPE_FDB_FLUSH_PORT,
+				    &req);
+}
+
+int mvsw_pr_hw_fdb_flush_vlan(const struct mvsw_pr_switch *sw, u16 vid,
+			      u32 mode)
+{
+	struct mvsw_msg_fdb_cmd req = {
+		.vid = vid,
+		.flush_mode = mode,
+	};
+
+	return  mvsw_pr_hw_send_req(sw, MVSW_MSG_TYPE_FDB_FLUSH_VLAN, &req);
+}
+
+int mvsw_pr_hw_fdb_flush_port_vlan(const struct mvsw_pr_port *port, u16 vid,
+				   u32 mode)
+{
+	struct mvsw_msg_fdb_cmd req = {
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.vid = vid,
+		.flush_mode = mode,
+	};
+
+	return  mvsw_pr_hw_send_req(port->sw, MVSW_MSG_TYPE_FDB_FLUSH_PORT_VLAN,
+				    &req);
+}
+
+int mvsw_pr_hw_port_link_mode_get(const struct mvsw_pr_port *port,
+				  u32 *mode)
+{
+	int err;
+	struct mvsw_msg_port_attr_ret resp;
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_LINK_MODE,
+		.port = port->hw_id,
+		.dev = port->dev_id
+	};
+
+	err = mvsw_pr_hw_send_req_resp(port->sw, MVSW_MSG_TYPE_PORT_ATTR_GET,
+				       &req, &resp);
+	if (err)
+		return err;
+
+	*mode = resp.param.link_mode;
+
+	return err;
+}
+
+int mvsw_pr_hw_port_link_mode_set(const struct mvsw_pr_port *port,
+				  u32 mode)
+{
+	struct mvsw_msg_port_attr_cmd req = {
+		.attr = MVSW_MSG_PORT_ATTR_LINK_MODE,
+		.port = port->hw_id,
+		.dev = port->dev_id,
+		.param = {.link_mode = mode}
+	};
+
+	return mvsw_pr_hw_send_req(port->sw, MVSW_MSG_TYPE_PORT_ATTR_SET, &req);
+}
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_hw.h b/drivers/net/ethernet/marvell/prestera_sw/prestera_hw.h
new file mode 100644
index 0000000..36b0b54
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_hw.h
@@ -0,0 +1,190 @@
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+
+#ifndef _MVSW_PRESTERA_HW_H_
+#define _MVSW_PRESTERA_HW_H_
+
+#include <linux/types.h>
+
+enum mvsw_pr_accept_frame_type {
+	MVSW_ACCEPT_FRAME_TYPE_TAGGED,
+	MVSW_ACCEPT_FRAME_TYPE_UNTAGGED,
+	MVSW_ACCEPT_FRAME_TYPE_ALL
+};
+
+enum {
+	MVSW_LINK_MODE_10baseT_Half_BIT,
+	MVSW_LINK_MODE_10baseT_Full_BIT,
+	MVSW_LINK_MODE_100baseT_Half_BIT,
+	MVSW_LINK_MODE_100baseT_Full_BIT,
+	MVSW_LINK_MODE_1000baseT_Half_BIT,
+	MVSW_LINK_MODE_1000baseT_Full_BIT,
+	MVSW_LINK_MODE_1000baseX_Full_BIT,
+	MVSW_LINK_MODE_1000baseKX_Full_BIT,
+	MVSW_LINK_MODE_10GbaseKR_Full_BIT,
+	MVSW_LINK_MODE_10GbaseSR_Full_BIT,
+	MVSW_LINK_MODE_10GbaseLR_Full_BIT,
+	MVSW_LINK_MODE_20GbaseKR2_Full_BIT,
+	MVSW_LINK_MODE_25GbaseCR_Full_BIT,
+	MVSW_LINK_MODE_25GbaseKR_Full_BIT,
+	MVSW_LINK_MODE_25GbaseSR_Full_BIT,
+	MVSW_LINK_MODE_40GbaseKR4_Full_BIT,
+	MVSW_LINK_MODE_40GbaseCR4_Full_BIT,
+	MVSW_LINK_MODE_40GbaseSR4_Full_BIT,
+	MVSW_LINK_MODE_50GbaseCR2_Full_BIT,
+	MVSW_LINK_MODE_50GbaseKR2_Full_BIT,
+	MVSW_LINK_MODE_50GbaseSR2_Full_BIT,
+	MVSW_LINK_MODE_100GbaseKR4_Full_BIT,
+	MVSW_LINK_MODE_100GbaseSR4_Full_BIT,
+	MVSW_LINK_MODE_100GbaseCR4_Full_BIT,
+	MVSW_LINK_MODE_MAX,
+};
+
+enum {
+	MVSW_PORT_TYPE_NONE,
+	MVSW_PORT_TYPE_TP,
+	MVSW_PORT_TYPE_AUI,
+	MVSW_PORT_TYPE_MII,
+	MVSW_PORT_TYPE_FIBRE,
+	MVSW_PORT_TYPE_BNC,
+	MVSW_PORT_TYPE_DA,
+	MVSW_PORT_TYPE_OTHER,
+	MVSW_PORT_TYPE_MAX,
+};
+
+enum {
+	MVSW_PORT_TRANSCEIVER_COPPER,
+	MVSW_PORT_TRANSCEIVER_SFP,
+	MVSW_PORT_TRANSCEIVER_MAX,
+};
+
+enum {
+	MVSW_PORT_FEC_OFF_BIT,
+	MVSW_PORT_FEC_BASER_BIT,
+	MVSW_PORT_FEC_RS_BIT,
+	MVSW_PORT_FEC_MAX,
+};
+
+enum {
+	MVSW_PORT_DUPLEX_HALF,
+	MVSW_PORT_DUPLEX_FULL
+};
+
+enum {
+	MVSW_PORT_GOOD_OCTETS_RCV_CNT,
+	MVSW_PORT_BAD_OCTETS_RCV_CNT,
+	MVSW_PORT_MAC_TRANSMIT_ERR_CNT,
+	MVSW_PORT_BRDC_PKTS_RCV_CNT,
+	MVSW_PORT_MC_PKTS_RCV_CNT,
+	MVSW_PORT_PKTS_64_OCTETS_CNT,
+	MVSW_PORT_PKTS_65TO127_OCTETS_CNT,
+	MVSW_PORT_PKTS_128TO255_OCTETS_CNT,
+	MVSW_PORT_PKTS_256TO511_OCTETS_CNT,
+	MVSW_PORT_PKTS_512TO1023_OCTETS_CNT,
+	MVSW_PORT_PKTS_1024TOMAX_OCTETS_CNT,
+	MVSW_PORT_EXCESSIVE_COLLISIONS_CNT,
+	MVSW_PORT_MC_PKTS_SENT_CNT,
+	MVSW_PORT_BRDC_PKTS_SENT_CNT,
+	MVSW_PORT_FC_SENT_CNT,
+	MVSW_PORT_GOOD_FC_RCV_CNT,
+	MVSW_PORT_DROP_EVENTS_CNT,
+	MVSW_PORT_UNDERSIZE_PKTS_CNT,
+	MVSW_PORT_FRAGMENTS_PKTS_CNT,
+	MVSW_PORT_OVERSIZE_PKTS_CNT,
+	MVSW_PORT_JABBER_PKTS_CNT,
+	MVSW_PORT_MAC_RCV_ERROR_CNT,
+	MVSW_PORT_BAD_CRC_CNT,
+	MVSW_PORT_COLLISIONS_CNT,
+	MVSW_PORT_LATE_COLLISIONS_CNT,
+	MVSW_PORT_GOOD_UC_PKTS_RCV_CNT,
+	MVSW_PORT_GOOD_UC_PKTS_SENT_CNT,
+	MVSW_PORT_MULTIPLE_PKTS_SENT_CNT,
+	MVSW_PORT_DEFERRED_PKTS_SENT_CNT,
+	MVSW_PORT_PKTS_1024TO1518_OCTETS_CNT,
+	MVSW_PORT_PKTS_1519TOMAX_OCTETS_CNT,
+	MVSW_PORT_GOOD_OCTETS_SENT_CNT,
+	MVSW_PORT_CNT_MAX,
+};
+
+struct mvsw_pr_switch;
+struct mvsw_pr_port;
+struct mvsw_pr_event_handler;
+
+/* Switch API */
+int mvsw_pr_hw_switch_init(struct mvsw_pr_switch *sw);
+int mvsw_pr_hw_switch_ageing_set(const struct mvsw_pr_switch *sw,
+				 u32 ageing_time);
+
+/* Port API */
+int mvsw_pr_hw_port_info_get(const struct mvsw_pr_port *port,
+			     u16 *fp_id, u32 *hw_id, u32 *dev_id);
+int mvsw_pr_hw_port_state_set(const struct mvsw_pr_port *port,
+			      bool admin_state);
+int mvsw_pr_hw_port_state_get(const struct mvsw_pr_port *port,
+			      bool *admin_state, bool *oper_state);
+int mvsw_pr_hw_port_mtu_set(const struct mvsw_pr_port *port, u32 mtu);
+int mvsw_pr_hw_port_mtu_get(const struct mvsw_pr_port *port, u32 *mtu);
+int mvsw_pr_hw_port_mac_set(const struct mvsw_pr_port *port, char *mac);
+int mvsw_pr_hw_port_mac_get(const struct mvsw_pr_port *port, char *mac);
+int mvsw_pr_hw_port_accept_frame_type_set(const struct mvsw_pr_port *port,
+					  enum mvsw_pr_accept_frame_type type);
+int mvsw_pr_hw_port_learning_set(const struct mvsw_pr_port *port, bool enable);
+int mvsw_pr_hw_port_speed_get(const struct mvsw_pr_port *port, u32 *speed);
+int mvsw_pr_hw_port_flood_set(const struct mvsw_pr_port *port, bool flood);
+int mvsw_pr_hw_port_cap_get(struct mvsw_pr_port *port);
+int mvsw_pr_hw_port_remote_cap_get(const struct mvsw_pr_port *port,
+				   u64 *link_mode_bitmap);
+int mvsw_pr_hw_port_type_get(const struct mvsw_pr_port *port, u8 *type);
+int mvsw_pr_hw_port_fec_get(const struct mvsw_pr_port *port, u8 *fec);
+int mvsw_pr_hw_port_fec_set(const struct mvsw_pr_port *port, u8 fec);
+int mvsw_pr_hw_port_autoneg_set(const struct mvsw_pr_port *port,
+				bool autoneg, u64 link_modes, u8 fec);
+int mvsw_pr_hw_port_duplex_get(const struct mvsw_pr_port *port, u8 *duplex);
+int mvsw_pr_hw_port_stats_get(const struct mvsw_pr_port *port,
+			      u64 *cnt, size_t size);
+int mvsw_pr_hw_port_link_mode_get(const struct mvsw_pr_port *port,
+				  u32 *mode);
+int mvsw_pr_hw_port_link_mode_set(const struct mvsw_pr_port *port,
+				  u32 mode);
+int mvsw_pr_hw_port_mdix_get(const struct mvsw_pr_port *port, u8 *mode);
+int mvsw_pr_hw_port_mdix_set(const struct mvsw_pr_port *port, u8 mode);
+
+/* Vlan API */
+int mvsw_pr_hw_vlan_create(const struct mvsw_pr_switch *sw, u16 vid);
+int mvsw_pr_hw_vlan_delete(const struct mvsw_pr_switch *sw, u16 vid);
+int mvsw_pr_hw_vlan_port_set(const struct mvsw_pr_port *port,
+			     u16 vid, bool is_member, bool untagged);
+int mvsw_pr_hw_vlan_port_vid_set(const struct mvsw_pr_port *port, u16 vid);
+
+/* FDB API */
+int mvsw_pr_hw_fdb_add(const struct mvsw_pr_port *port,
+		       const unsigned char *mac, u16 vid, bool dynamic);
+int mvsw_pr_hw_fdb_del(const struct mvsw_pr_port *port,
+		       const unsigned char *mac, u16 vid);
+int mvsw_pr_hw_fdb_flush_port(const struct mvsw_pr_port *port, u32 mode);
+int mvsw_pr_hw_fdb_flush_vlan(const struct mvsw_pr_switch *sw, u16 vid,
+			      u32 mode);
+int mvsw_pr_hw_fdb_flush_port_vlan(const struct mvsw_pr_port *port, u16 vid,
+				   u32 mode);
+
+/* Bridge API */
+int mvsw_pr_hw_bridge_create(const struct mvsw_pr_switch *sw, u16 *bridge_id);
+int mvsw_pr_hw_bridge_delete(const struct mvsw_pr_switch *sw, u16 bridge_id);
+int mvsw_pr_hw_bridge_port_add(const struct mvsw_pr_port *port, u16 bridge_id);
+int mvsw_pr_hw_bridge_port_delete(const struct mvsw_pr_port *port,
+				  u16 bridge_id);
+
+/* Event handlers */
+int mvsw_pr_hw_event_handler_register(struct mvsw_pr_switch *sw,
+				      const struct mvsw_pr_event_handler *eh);
+void mvsw_pr_hw_event_handler_unregister(struct mvsw_pr_switch *sw,
+					 const struct mvsw_pr_event_handler
+					 *eh);
+/* SW Dev Log API */
+int mvsw_pr_hw_fw_log_level_set(const struct mvsw_pr_switch *sw,
+				u32 lib_name, u32 log_level);
+
+#endif /* _MVSW_PRESTERA_HW_H_ */
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_log.c b/drivers/net/ethernet/marvell/prestera_sw/prestera_log.c
new file mode 100644
index 0000000..ff65732
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_log.c
@@ -0,0 +1,133 @@
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+#include "prestera_log.h"
+
+static const char unknown[] = "UNKNOWN";
+
+DEF_ENUM_MAP(netdev_cmd) = {
+	[NETDEV_UP] = "NETDEV_UP",
+	[NETDEV_DOWN] = "NETDEV_DOWN",
+	[NETDEV_REBOOT] = "NETDEV_REBOOT",
+	[NETDEV_CHANGE] = "NETDEV_CHANGE",
+	[NETDEV_REGISTER] = "NETDEV_REGISTER",
+	[NETDEV_UNREGISTER] = "NETDEV_UNREGISTER",
+	[NETDEV_CHANGEMTU] = "NETDEV_CHANGEMTU",
+	[NETDEV_CHANGEADDR] = "NETDEV_CHANGEADDR",
+	[NETDEV_PRE_CHANGEADDR] = "NETDEV_PRE_CHANGEADDR",
+	[NETDEV_GOING_DOWN] = "NETDEV_GOING_DOWN",
+	[NETDEV_CHANGENAME] = "NETDEV_CHANGENAME",
+	[NETDEV_FEAT_CHANGE] = "NETDEV_FEAT_CHANGE",
+	[NETDEV_BONDING_FAILOVER] = "NETDEV_BONDING_FAILOVER",
+	[NETDEV_PRE_UP] = "NETDEV_PRE_UP",
+	[NETDEV_PRE_TYPE_CHANGE] = "NETDEV_PRE_TYPE_CHANGE",
+	[NETDEV_POST_TYPE_CHANGE] = "NETDEV_POST_TYPE_CHANGE",
+	[NETDEV_POST_INIT] = "NETDEV_POST_INIT",
+	[NETDEV_RELEASE] = "NETDEV_RELEASE",
+	[NETDEV_NOTIFY_PEERS] = "NETDEV_NOTIFY_PEERS",
+	[NETDEV_JOIN] = "NETDEV_JOIN",
+	[NETDEV_CHANGEUPPER] = "NETDEV_CHANGEUPPER",
+	[NETDEV_RESEND_IGMP] = "NETDEV_RESEND_IGMP",
+	[NETDEV_PRECHANGEMTU] = "NETDEV_PRECHANGEMTU",
+	[NETDEV_CHANGEINFODATA] = "NETDEV_CHANGEINFODATA",
+	[NETDEV_BONDING_INFO] = "NETDEV_BONDING_INFO",
+	[NETDEV_PRECHANGEUPPER] = "NETDEV_PRECHANGEUPPER",
+	[NETDEV_CHANGELOWERSTATE] = "NETDEV_CHANGELOWERSTATE",
+	[NETDEV_UDP_TUNNEL_PUSH_INFO] = "NETDEV_UDP_TUNNEL_PUSH_INFO",
+	[NETDEV_UDP_TUNNEL_DROP_INFO] = "NETDEV_UDP_TUNNEL_DROP_INFO",
+	[NETDEV_CHANGE_TX_QUEUE_LEN] = "NETDEV_CHANGE_TX_QUEUE_LEN",
+	[NETDEV_CVLAN_FILTER_PUSH_INFO] = "NETDEV_CVLAN_FILTER_PUSH_INFO",
+	[NETDEV_CVLAN_FILTER_DROP_INFO] = "NETDEV_CVLAN_FILTER_DROP_INFO",
+	[NETDEV_SVLAN_FILTER_PUSH_INFO] = "NETDEV_SVLAN_FILTER_PUSH_INFO",
+	[NETDEV_SVLAN_FILTER_DROP_INFO] = "NETDEV_SVLAN_FILTER_DROP_INFO"
+};
+
+DEF_ENUM_MAP(switchdev_notifier_type) = {
+	[SWITCHDEV_FDB_ADD_TO_BRIDGE] = "SWITCHDEV_FDB_ADD_TO_BRIDGE",
+	[SWITCHDEV_FDB_DEL_TO_BRIDGE] = "SWITCHDEV_FDB_DEL_TO_BRIDGE",
+	[SWITCHDEV_FDB_ADD_TO_DEVICE] = "SWITCHDEV_FDB_ADD_TO_DEVICE",
+	[SWITCHDEV_FDB_DEL_TO_DEVICE] = "SWITCHDEV_FDB_DEL_TO_DEVICE",
+	[SWITCHDEV_FDB_OFFLOADED] = "SWITCHDEV_FDB_OFFLOADED",
+	[SWITCHDEV_PORT_OBJ_ADD] = "SWITCHDEV_PORT_OBJ_ADD",
+	[SWITCHDEV_PORT_OBJ_DEL] = "SWITCHDEV_PORT_OBJ_DEL",
+	[SWITCHDEV_PORT_ATTR_SET] = "SWITCHDEV_PORT_ATTR_SET",
+	[SWITCHDEV_VXLAN_FDB_ADD_TO_BRIDGE] =
+		"SWITCHDEV_VXLAN_FDB_ADD_TO_BRIDGE",
+	[SWITCHDEV_VXLAN_FDB_DEL_TO_BRIDGE] =
+		"SWITCHDEV_VXLAN_FDB_DEL_TO_BRIDGE",
+	[SWITCHDEV_VXLAN_FDB_ADD_TO_DEVICE] =
+		"SWITCHDEV_VXLAN_FDB_ADD_TO_DEVICE",
+	[SWITCHDEV_VXLAN_FDB_DEL_TO_DEVICE] =
+		"SWITCHDEV_VXLAN_FDB_DEL_TO_DEVICE",
+	[SWITCHDEV_VXLAN_FDB_OFFLOADED] = "SWITCHDEV_VXLAN_FDB_OFFLOADED"
+};
+
+DEF_ENUM_MAP(switchdev_attr_id) = {
+	[SWITCHDEV_ATTR_ID_UNDEFINED] =
+		"SWITCHDEV_ATTR_ID_UNDEFINED",
+	[SWITCHDEV_ATTR_ID_PORT_STP_STATE] =
+		"SWITCHDEV_ATTR_ID_PORT_STP_STATE",
+	[SWITCHDEV_ATTR_ID_PORT_BRIDGE_FLAGS] =
+		"SWITCHDEV_ATTR_ID_PORT_BRIDGE_FLAGS",
+	[SWITCHDEV_ATTR_ID_PORT_PRE_BRIDGE_FLAGS] =
+		"SWITCHDEV_ATTR_ID_PORT_PRE_BRIDGE_FLAGS",
+	[SWITCHDEV_ATTR_ID_PORT_MROUTER] =
+		"SWITCHDEV_ATTR_ID_PORT_MROUTER",
+	[SWITCHDEV_ATTR_ID_BRIDGE_AGEING_TIME] =
+		"SWITCHDEV_ATTR_ID_BRIDGE_AGEING_TIME",
+	[SWITCHDEV_ATTR_ID_BRIDGE_VLAN_FILTERING] =
+		"SWITCHDEV_ATTR_ID_BRIDGE_VLAN_FILTERING",
+	[SWITCHDEV_ATTR_ID_BRIDGE_MC_DISABLED] =
+		"SWITCHDEV_ATTR_ID_BRIDGE_MC_DISABLED",
+	[SWITCHDEV_ATTR_ID_BRIDGE_MROUTER] =
+		"SWITCHDEV_ATTR_ID_BRIDGE_MROUTER"
+};
+
+DEF_ENUM_MAP(switchdev_obj_id) = {
+	[SWITCHDEV_OBJ_ID_UNDEFINED] = "SWITCHDEV_OBJ_ID_UNDEFINED",
+	[SWITCHDEV_OBJ_ID_PORT_VLAN] = "SWITCHDEV_OBJ_ID_PORT_VLAN",
+	[SWITCHDEV_OBJ_ID_PORT_MDB] = "SWITCHDEV_OBJ_ID_PORT_MDB",
+	[SWITCHDEV_OBJ_ID_HOST_MDB] = "SWITCHDEV_OBJ_ID_HOST_MDB",
+};
+
+DEF_ENUM_MAP(fib_event_type) = {
+	[FIB_EVENT_ENTRY_REPLACE] = "FIB_EVENT_ENTRY_REPLACE",
+	[FIB_EVENT_ENTRY_APPEND] = "FIB_EVENT_ENTRY_APPEND",
+	[FIB_EVENT_ENTRY_ADD] = "FIB_EVENT_ENTRY_ADD",
+	[FIB_EVENT_ENTRY_DEL] = "FIB_EVENT_ENTRY_DEL",
+	[FIB_EVENT_RULE_ADD] = "FIB_EVENT_RULE_ADD",
+	[FIB_EVENT_RULE_DEL] = "FIB_EVENT_RULE_DEL",
+	[FIB_EVENT_NH_ADD] = "FIB_EVENT_NH_ADD",
+	[FIB_EVENT_NH_DEL] = "FIB_EVENT_NH_DEL",
+	[FIB_EVENT_VIF_ADD] = "FIB_EVENT_VIF_ADD",
+	[FIB_EVENT_VIF_DEL] = "FIB_EVENT_VIF_DEL",
+};
+
+DEF_ENUM_MAP(netevent_notif_type) = {
+	[NETEVENT_NEIGH_UPDATE] = "NETEVENT_NEIGH_UPDATE",
+	[NETEVENT_REDIRECT] = "NETEVENT_REDIRECT",
+	[NETEVENT_DELAY_PROBE_TIME_UPDATE] =
+		"NETEVENT_DELAY_PROBE_TIME_UPDATE",
+	[NETEVENT_IPV4_MPATH_HASH_UPDATE] =
+		"NETEVENT_IPV4_MPATH_HASH_UPDATE",
+	[NETEVENT_IPV6_MPATH_HASH_UPDATE] =
+		"NETEVENT_IPV6_MPATH_HASH_UPDATE",
+	[NETEVENT_IPV4_FWD_UPDATE_PRIORITY_UPDATE] =
+		"NETEVENT_IPV4_FWD_UPDATE_PRIORITY_UPDATE",
+};
+
+DEF_ENUM_FUNC(netdev_cmd, NETDEV_UP, NETDEV_SVLAN_FILTER_DROP_INFO)
+
+DEF_ENUM_FUNC(switchdev_notifier_type, SWITCHDEV_FDB_ADD_TO_BRIDGE,
+	      SWITCHDEV_VXLAN_FDB_OFFLOADED)
+DEF_ENUM_FUNC(switchdev_attr_id, SWITCHDEV_ATTR_ID_UNDEFINED,
+	      SWITCHDEV_ATTR_ID_BRIDGE_MROUTER)
+DEF_ENUM_FUNC(switchdev_obj_id, SWITCHDEV_OBJ_ID_UNDEFINED,
+	      SWITCHDEV_OBJ_ID_HOST_MDB)
+
+DEF_ENUM_FUNC(fib_event_type, FIB_EVENT_ENTRY_REPLACE, FIB_EVENT_VIF_DEL)
+
+DEF_ENUM_FUNC(netevent_notif_type, NETEVENT_NEIGH_UPDATE,
+	      NETEVENT_IPV4_FWD_UPDATE_PRIORITY_UPDATE)
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_log.h b/drivers/net/ethernet/marvell/prestera_sw/prestera_log.h
new file mode 100644
index 0000000..b75ca94
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_log.h
@@ -0,0 +1,52 @@
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+
+#ifndef _MVSW_PRESTERA_LOG_H_
+#define _MVSW_PRESTERA_LOG_H_
+
+#ifdef CONFIG_MRVL_PRESTERA_DEBUG
+
+#include <linux/netdevice.h>
+#include <linux/version.h>
+#include <net/switchdev.h>
+#include <net/fib_notifier.h>
+#include <net/netevent.h>
+
+#define DEF_ENUM_MAP(enum_name) \
+static const char *enum_name##_map[]
+
+#define DEF_ENUM_FUNC(enum_name, enum_min, enum_max) \
+const char *enum_name##_to_name(enum enum_name val) \
+{ \
+	if (val < enum_min || val > enum_max) \
+		return unknown; \
+	return enum_name##_map[val]; \
+}
+
+#define DEC_ENUM_FUNC(enum_name) \
+const char *enum_name##_to_name(enum enum_name)
+
+#define ENUM_TO_NAME(enum_name, val) enum_name##_to_name(val)
+
+#define MVSW_LOG_INFO(fmt, ...) \
+	pr_info("%s:%d: " fmt "\n", __func__, __LINE__, ##__VA_ARGS__)
+
+#define MVSW_LOG_ERROR(fmt, ...) \
+	pr_err("%s:%d: " fmt "\n", __func__, __LINE__, ##__VA_ARGS__)
+
+DEC_ENUM_FUNC(netdev_cmd);
+DEC_ENUM_FUNC(switchdev_notifier_type);
+DEC_ENUM_FUNC(switchdev_attr_id);
+DEC_ENUM_FUNC(switchdev_obj_id);
+DEC_ENUM_FUNC(fib_event_type);
+DEC_ENUM_FUNC(netevent_notif_type);
+
+#else /* CONFIG_MRVL_PRESTERA_DEBUG */
+#define MVSW_LOG_INFO(...)
+#define MVSW_LOG_ERROR(...)
+#endif /* CONFIG_MRVL_PRESTERA_DEBUG */
+
+#endif /* _MVSW_PRESTERA_LOG_H_ */
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_pci.c b/drivers/net/ethernet/marvell/prestera_sw/prestera_pci.c
new file mode 100644
index 0000000..3142d1c
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_pci.c
@@ -0,0 +1,903 @@
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/device.h>
+#include <linux/pci.h>
+#include <linux/circ_buf.h>
+#include <linux/firmware.h>
+
+#include "prestera.h"
+#include "prestera_log.h"
+
+#define MVSW_PR_FW_FILENAME	"marvell/mvsw_prestera_fw.img"
+
+#define MVSW_PCI_ALIGNTO	4
+
+#define MVSW_PCI_ALIGN(len) \
+	(((len) + (MVSW_PCI_ALIGNTO - 1)) & ~(MVSW_PCI_ALIGNTO - 1))
+
+#define MVSW_PCI_ROUND_DOWN(len)	((len) & ~(MVSW_PCI_ALIGNTO - 1))
+
+#define MVSW_PR_SUPPORTED_FW_MAJOR_VERSION 1
+#define MVSW_PR_SUPPORTED_FW_MINOR_VERSION 0
+#define MVSW_PR_SUPPORTED_FW_PATCH_VERSION 0
+
+#define mvsw_pr_pci_wait_timeout(cond, waitms) \
+({ \
+	unsigned long __wait_end = jiffies + msecs_to_jiffies(waitms); \
+	bool __wait_ret = false; \
+	do { \
+		if (cond) { \
+			__wait_ret = true; \
+			break; \
+		} \
+		cond_resched(); \
+	} while (time_before(jiffies, __wait_end)); \
+	__wait_ret; \
+})
+
+#define VERSION_MAJOR_MULTIPLIER 1000000
+#define VERSION_MINOR_MULTIPLIER 1000
+
+#define MVSW_PR_FW_HEADER_MAGIC_NUMBER 0x351D9D06
+#define MVSW_PR_FW_DOWNLOAD_WAIT_TIMEOUT 50000
+#define MVSW_PR_FW_DOWNLOAD_BLOCK_SIZE 1024
+
+/* Version consists of: MAJOR, MINOR and PATCH fields
+ * calculated as following:
+ * MAJOR  = fw_header_field_version.value * 1 000 000
+ * MINOR  = (fw_header_field_version.value - MAJOR) / 1000
+ * PATCH = fw_header_field_version.value - MINOR
+ */
+#define VERSION_GET_MAJOR(_v)						\
+	((_v) / VERSION_MAJOR_MULTIPLIER)
+
+#define VERSION_GET_MINOR(_v)						\
+	({								\
+		u32 versm = (_v);					\
+		((((versm) - (VERSION_GET_MAJOR(versm) *		\
+		VERSION_MAJOR_MULTIPLIER)) / VERSION_MINOR_MULTIPLIER));\
+	})
+
+#define VERSION_GET_PATCH(_v)						\
+	({								\
+		u32 versp = (_v);					\
+		((versp - (VERSION_GET_MAJOR(versp) * VERSION_MAJOR_MULTIPLIER)\
+		- (VERSION_GET_MINOR(versp) * VERSION_MINOR_MULTIPLIER)));\
+	})
+
+struct mvsw_pr_fw_header {
+	u32 magic_number;
+	u32 version_value;
+	u8 reserved[8];
+} __packed;
+
+struct mvsw_fw_ldr_regs {
+	u32 ldr_ready;
+	u32 pad1;
+
+	u32 ldr_img_size;
+	u32 ldr_ctl_flags;
+
+	u32 ldr_buf_offs;
+	u32 ldr_buf_size;
+
+	u32 ldr_buf_rd;
+	u32 pad2;
+	u32 ldr_buf_wr;
+
+	u32 ldr_status;
+} __packed __aligned(4);
+
+#define MVSW_LDR_REG_OFFSET(f)	offsetof(struct mvsw_fw_ldr_regs, f)
+
+#define MVSW_LDR_READY_MAGIC	0xf00dfeed
+
+#define MVSW_LDR_STATUS_IMG_DOWNLOADING		0x1
+#define MVSW_LDR_STATUS_STARTING_FW		0x2
+#define MVSW_LDR_STATUS_INVALID_IMG		0x4
+#define MVSW_LDR_STATUS_NOMEM			0x8
+
+#define mvsw_ldr_reg_write32(dev, reg, val) \
+	writel(val, dev->ldr_regs + (reg))
+#define mvsw_ldr_reg_read32(dev, reg) \
+	readl(dev->ldr_regs + (reg))
+
+/* fw loader registers */
+#define MVSW_LDR_READY_REG	MVSW_LDR_REG_OFFSET(ldr_ready)
+#define MVSW_LDR_IMG_SIZE_REG	MVSW_LDR_REG_OFFSET(ldr_img_size)
+#define MVSW_LDR_CTL_REG	MVSW_LDR_REG_OFFSET(ldr_ctl_flags)
+#define MVSW_LDR_BUF_SIZE_REG	MVSW_LDR_REG_OFFSET(ldr_buf_size)
+#define MVSW_LDR_BUF_OFFS_REG	MVSW_LDR_REG_OFFSET(ldr_buf_offs)
+#define MVSW_LDR_BUF_RD_REG	MVSW_LDR_REG_OFFSET(ldr_buf_rd)
+#define MVSW_LDR_BUF_WR_REG	MVSW_LDR_REG_OFFSET(ldr_buf_wr)
+#define MVSW_LDR_STATUS_REG	MVSW_LDR_REG_OFFSET(ldr_status)
+
+#define MVSW_LDR_CTL_DL_START	BIT(0)
+
+#define MVSW_LDR_WR_IDX_MOVE(dev, n) \
+do { \
+	typeof(dev) __dev = (dev); \
+	(__dev)->ldr_wr_idx = ((__dev)->ldr_wr_idx + (n)) & \
+				((__dev)->ldr_buf_len - 1); \
+} while (0)
+
+#define MVSW_LDR_WR_IDX_COMMIT(dev) \
+({ \
+	typeof(dev) __dev = (dev); \
+	mvsw_ldr_reg_write32((__dev), MVSW_LDR_BUF_WR_REG, \
+			     (__dev)->ldr_wr_idx); \
+})
+
+#define MVSW_LDR_WR_PTR(dev) \
+({ \
+	typeof(dev) __dev = (dev); \
+	((__dev)->ldr_ring_buf + (__dev)->ldr_wr_idx); \
+})
+
+#define MVSW_EVT_QNUM_MAX	4
+
+struct mvsw_pr_fw_evtq_regs {
+	u32 rd_idx;
+	u32 pad1;
+	u32 wr_idx;
+	u32 pad2;
+	u32 offs;
+	u32 len;
+};
+
+struct mvsw_pr_fw_regs {
+	u32 fw_ready;
+	u32 pad;
+	u32 cmd_offs;
+	u32 cmd_len;
+	u32 evt_offs;
+	u32 evt_qnum;
+
+	u32 cmd_req_ctl;
+	u32 cmd_req_len;
+	u32 cmd_rcv_ctl;
+	u32 cmd_rcv_len;
+
+	u32 fw_status;
+
+	struct mvsw_pr_fw_evtq_regs evtq_list[MVSW_EVT_QNUM_MAX];
+};
+
+#define MVSW_FW_REG_OFFSET(f)	offsetof(struct mvsw_pr_fw_regs, f)
+
+#define MVSW_FW_READY_MAGIC	0xcafebabe
+
+/* fw registers */
+#define MVSW_FW_READY_REG		MVSW_FW_REG_OFFSET(fw_ready)
+
+#define MVSW_CMD_BUF_OFFS_REG		MVSW_FW_REG_OFFSET(cmd_offs)
+#define MVSW_CMD_BUF_LEN_REG		MVSW_FW_REG_OFFSET(cmd_len)
+#define MVSW_EVT_BUF_OFFS_REG		MVSW_FW_REG_OFFSET(evt_offs)
+#define MVSW_EVT_QNUM_REG		MVSW_FW_REG_OFFSET(evt_qnum)
+
+#define MVSW_CMD_REQ_CTL_REG		MVSW_FW_REG_OFFSET(cmd_req_ctl)
+#define MVSW_CMD_REQ_LEN_REG		MVSW_FW_REG_OFFSET(cmd_req_len)
+
+#define MVSW_CMD_RCV_CTL_REG		MVSW_FW_REG_OFFSET(cmd_rcv_ctl)
+#define MVSW_CMD_RCV_LEN_REG		MVSW_FW_REG_OFFSET(cmd_rcv_len)
+#define MVSW_FW_STATUS_REG		MVSW_FW_REG_OFFSET(fw_status)
+
+/* MVSW_CMD_REQ_CTL_REG flags */
+#define MVSW_CMD_F_REQ_SENT		BIT(0)
+#define MVSW_CMD_F_REPL_RCVD		BIT(1)
+
+/* MVSW_CMD_RCV_CTL_REG flags */
+#define MVSW_CMD_F_REPL_SENT		BIT(0)
+
+#define MVSW_EVTQ_REG_OFFSET(q, f)			\
+	(MVSW_FW_REG_OFFSET(evtq_list) +		\
+	 (q) * sizeof(struct mvsw_pr_fw_evtq_regs) +	\
+	 offsetof(struct mvsw_pr_fw_evtq_regs, f))
+
+#define MVSW_EVTQ_RD_IDX_REG(q)		MVSW_EVTQ_REG_OFFSET(q, rd_idx)
+#define MVSW_EVTQ_WR_IDX_REG(q)		MVSW_EVTQ_REG_OFFSET(q, wr_idx)
+#define MVSW_EVTQ_OFFS_REG(q)		MVSW_EVTQ_REG_OFFSET(q, offs)
+#define MVSW_EVTQ_LEN_REG(q)		MVSW_EVTQ_REG_OFFSET(q, len)
+
+#define mvsw_pr_reg_write32(dev, reg, val) \
+	writel(val, dev->hw_regs + (reg))
+#define mvsw_pr_reg_read32(dev, reg) \
+	readl(dev->hw_regs + (reg))
+
+struct mvsw_pr_pci_evtq {
+	/* protect concurrent access to the ring buffer */
+	struct mutex mtx;
+	u8 __iomem *addr;
+	size_t len;
+};
+
+struct mvsw_pr_pci_device {
+	struct mvsw_pr_device dev;
+	struct pci_dev *pci_dev;
+	u8 __iomem *mem_addr;
+	size_t mem_len;
+
+	u8 __iomem *ldr_regs;
+	u8 __iomem *hw_regs;
+
+	u8 __iomem *ldr_ring_buf;
+	u32 ldr_buf_len;
+	u32 ldr_wr_idx;
+
+	/* TODO: add more descriptive comment */
+	struct mutex cmd_mtx;
+	size_t cmd_mbox_len;
+	u8 __iomem *cmd_mbox;
+	struct mvsw_pr_pci_evtq evt_queue[MVSW_EVT_QNUM_MAX];
+	u8 evt_qnum;
+	struct work_struct evt_work;
+	u8 __iomem *evt_buf;
+	u8 *evt_msg;
+};
+
+#define PRESTERA_DEVICE(id) PCI_VDEVICE(MARVELL, (id))
+
+static struct mvsw_pr_pci_match {
+	struct pci_driver driver;
+	const struct pci_device_id id;
+	bool registered;
+} mvsw_pci_devices[] = {
+	{
+		.driver = { .name = "Cetus", },
+		.id = { PRESTERA_DEVICE(0xbe00), 0 },
+	},
+	{
+		.driver = { .name = "AC3x B2B", },
+		.id = { PRESTERA_DEVICE(0x6820), 0 },
+	},
+	{{ }, { },}
+};
+
+static int mvsw_pr_pci_fw_load(struct mvsw_pr_pci_device *mvsw_pci);
+
+static u32 mvsw_pr_pci_evtq_len(struct mvsw_pr_pci_device *mvsw_pci, u8 qid)
+{
+	return mvsw_pci->evt_queue[qid].len;
+}
+
+static u32 mvsw_pr_pci_evtq_avail(struct mvsw_pr_pci_device *mvsw_pci, u8 qid)
+{
+	u32 wr_idx = mvsw_pr_reg_read32(mvsw_pci, MVSW_EVTQ_WR_IDX_REG(qid));
+	u32 rd_idx = mvsw_pr_reg_read32(mvsw_pci, MVSW_EVTQ_RD_IDX_REG(qid));
+
+	return CIRC_CNT(wr_idx, rd_idx, mvsw_pr_pci_evtq_len(mvsw_pci, qid));
+}
+
+static void mvsw_pr_pci_evtq_rd_set(struct mvsw_pr_pci_device *mvsw_pci,
+				    u8 qid, u32 idx)
+{
+	u32 rd_idx = idx & (mvsw_pr_pci_evtq_len(mvsw_pci, qid) - 1);
+
+	mvsw_pr_reg_write32(mvsw_pci, MVSW_EVTQ_RD_IDX_REG(qid), rd_idx);
+}
+
+static u8 __iomem *mvsw_pr_pci_evtq_buf(struct mvsw_pr_pci_device *mvsw_pci,
+					u8 qid)
+{
+	return mvsw_pci->evt_queue[qid].addr;
+}
+
+static u32 mvsw_pr_pci_evtq_read32(struct mvsw_pr_pci_device *mvsw_pci, u8 qid)
+{
+	u32 rd_idx = mvsw_pr_reg_read32(mvsw_pci, MVSW_EVTQ_RD_IDX_REG(qid));
+	u32 val;
+
+	val = readl(mvsw_pr_pci_evtq_buf(mvsw_pci, qid) + rd_idx);
+	mvsw_pr_pci_evtq_rd_set(mvsw_pci, qid, rd_idx + 4);
+	return val;
+}
+
+static ssize_t mvsw_pr_pci_evtq_read_buf(struct mvsw_pr_pci_device *mvsw_pci,
+					 u8 qid, u8 *buf, size_t len)
+{
+	u32 idx = mvsw_pr_reg_read32(mvsw_pci, MVSW_EVTQ_RD_IDX_REG(qid));
+	u8 __iomem *evtq_addr = mvsw_pr_pci_evtq_buf(mvsw_pci, qid);
+	u32 *buf32 = (u32 *)buf;
+	int i;
+
+	for (i = 0; i < len / 4; buf32++, i++) {
+		*buf32 = readl_relaxed(evtq_addr + idx);
+		idx = (idx + 4) & (mvsw_pr_pci_evtq_len(mvsw_pci, qid) - 1);
+	}
+
+	mvsw_pr_pci_evtq_rd_set(mvsw_pci, qid, idx);
+
+	return i;
+}
+
+static void mvsw_pr_pci_evtq_lock(struct mvsw_pr_pci_device *mvsw_pci, u8 qid)
+{
+	mutex_lock(&mvsw_pci->evt_queue[qid].mtx);
+}
+
+static void mvsw_pr_pci_evtq_unlock(struct mvsw_pr_pci_device *mvsw_pci, u8 qid)
+{
+	mutex_unlock(&mvsw_pci->evt_queue[qid].mtx);
+}
+
+static u8 mvsw_pr_pci_evtq_pick(struct mvsw_pr_pci_device *mvsw_pci)
+{
+	int qid;
+
+	for (qid = 0; qid < mvsw_pci->evt_qnum; qid++) {
+		if (mvsw_pr_pci_evtq_avail(mvsw_pci, qid) >= 4)
+			return qid;
+	}
+
+	return MVSW_EVT_QNUM_MAX;
+}
+
+static void mvsw_pr_pci_evt_work_fn(struct work_struct *work)
+{
+	struct mvsw_pr_pci_device *mvsw_pci;
+	u8 *msg;
+	u8 qid;
+
+	mvsw_pci = container_of(work, struct mvsw_pr_pci_device, evt_work);
+	msg = mvsw_pci->evt_msg;
+
+	while ((qid = mvsw_pr_pci_evtq_pick(mvsw_pci)) < MVSW_EVT_QNUM_MAX) {
+		u32 idx;
+		u32 len;
+
+		mvsw_pr_pci_evtq_lock(mvsw_pci, qid);
+
+		len = mvsw_pr_pci_evtq_read32(mvsw_pci, qid);
+		idx = mvsw_pr_reg_read32(mvsw_pci, MVSW_EVTQ_RD_IDX_REG(qid));
+
+		WARN_ON(mvsw_pr_pci_evtq_avail(mvsw_pci, qid) < len);
+
+		if (len > MVSW_MSG_MAX_SIZE) {
+			MVSW_LOG_ERROR("too big event msg len(%u)\n", len);
+			mvsw_pr_pci_evtq_rd_set(mvsw_pci, qid, idx + len);
+			mvsw_pr_pci_evtq_unlock(mvsw_pci, qid);
+			continue;
+		}
+
+		mvsw_pr_pci_evtq_read_buf(mvsw_pci, qid, msg, len);
+
+		if (mvsw_pci->dev.recv_msg)
+			mvsw_pci->dev.recv_msg(&mvsw_pci->dev, msg, len);
+
+		mvsw_pr_pci_evtq_unlock(mvsw_pci, qid);
+	}
+}
+
+static int mvsw_pr_pci_wait_reg32(struct mvsw_pr_pci_device *dev,
+				  u32 reg, u32 val,
+				  unsigned int wait)
+{
+	if (mvsw_pr_pci_wait_timeout(mvsw_pr_reg_read32(dev, reg) == val, wait))
+		return 0;
+
+	return -EBUSY;
+}
+
+static void mvsw_pr_mbox_copy_to(u8 __iomem *dst, u8 *src, size_t len)
+{
+	u32 __iomem *dst32 = (u32 __iomem *)dst;
+	u32 *src32 = (u32 *)src;
+	int i;
+
+	for (i = 0; i < (len / 4); dst32++, src32++, i++)
+		writel_relaxed(*src32, dst32);
+}
+
+static void mvsw_pr_mbox_copy_from(u8 *dst, u8 __iomem *src, size_t len)
+{
+	u32 *dst32 = (u32 *)dst;
+	u32 __iomem *src32 = (u32 __iomem *)src;
+	int i;
+
+	for (i = 0; i < (len / 4); dst32++, src32++, i++)
+		*dst32 = readl_relaxed(src32);
+}
+
+static ssize_t mvsw_pr_pci_cmd_send(struct mvsw_pr_pci_device *mvsw_pci,
+				    u8 *in_msg, size_t in_size,
+				    u8 *out_msg, size_t out_size)
+{
+	u32 ret_size = 0;
+	int err = 0;
+
+	if (MVSW_PCI_ALIGN(in_size) > mvsw_pci->cmd_mbox_len) {
+		pr_err("cmd msg is too long (%zu), max(%zu)\n",
+		       in_size, mvsw_pci->cmd_mbox_len);
+		return -EMSGSIZE;
+	}
+
+	/* wait for finish previous reply from FW */
+	err = mvsw_pr_pci_wait_reg32(mvsw_pci, MVSW_CMD_RCV_CTL_REG, 0, 30);
+	if (err) {
+		pr_err("finish reply from FW is timed out\n");
+		return err;
+	}
+
+	mvsw_pr_reg_write32(mvsw_pci, MVSW_CMD_REQ_LEN_REG, in_size);
+	mvsw_pr_mbox_copy_to(mvsw_pci->cmd_mbox, in_msg, in_size);
+
+	mvsw_pr_reg_write32(mvsw_pci, MVSW_CMD_REQ_CTL_REG,
+			    MVSW_CMD_F_REQ_SENT);
+
+	/* wait for reply from FW */
+	err = mvsw_pr_pci_wait_reg32(mvsw_pci,
+				     MVSW_CMD_RCV_CTL_REG, MVSW_CMD_F_REPL_SENT,
+				     30000);
+	if (err) {
+		pr_err("reply from FW is timed out\n");
+		goto cmd_exit;
+	}
+
+	ret_size = mvsw_pr_reg_read32(mvsw_pci, MVSW_CMD_RCV_LEN_REG);
+	if (!ret_size) {
+		goto cmd_exit;
+	} else if (ret_size > out_size) {
+		pr_err("ret_size (%u) > out_len(%zu)\n", ret_size, out_size);
+		err = -EMSGSIZE;
+		goto cmd_exit;
+	}
+
+	mvsw_pr_mbox_copy_from(out_msg, mvsw_pci->cmd_mbox + in_size, ret_size);
+
+cmd_exit:
+	mvsw_pr_reg_write32(mvsw_pci, MVSW_CMD_REQ_CTL_REG,
+			    MVSW_CMD_F_REPL_RCVD);
+	return ret_size;
+}
+
+static int mvsw_pr_pci_send_req(struct mvsw_pr_device *dev, int mode,
+				u8 *in_msg, size_t in_size, u8 *out_msg,
+				size_t out_size, size_t *out_data_size)
+{
+	struct mvsw_pr_pci_device *mvsw_pci;
+	ssize_t ret;
+
+	mvsw_pci = container_of(dev, struct mvsw_pr_pci_device, dev);
+
+	mutex_lock(&mvsw_pci->cmd_mtx);
+
+	ret = mvsw_pr_pci_cmd_send(mvsw_pci, in_msg, in_size,
+				   out_msg, out_size);
+	if (ret < 0) {
+		pr_err("cmd msg sent failed, err=%ld\n", ret);
+		*out_data_size = 0;
+	} else {
+		*out_data_size = ret;
+	}
+
+	mutex_unlock(&mvsw_pci->cmd_mtx);
+
+	return ret < 0 ? ret : 0;
+}
+
+static struct mvsw_pr_bus mvsw_pci_bus = {
+	.timeout = 500000,
+	.send_req = mvsw_pr_pci_send_req,
+};
+
+static int mvsw_pr_pci_fw_init(struct mvsw_pr_pci_device *dev)
+{
+	u8 __iomem *base;
+	int err;
+	u8 qid;
+
+	err = mvsw_pr_pci_fw_load(dev);
+	if (err && err != -ETIMEDOUT) {
+		MVSW_LOG_ERROR("failed to load fw image\n");
+		return err;
+	}
+
+	err = mvsw_pr_pci_wait_reg32(dev, MVSW_FW_READY_REG,
+				     MVSW_FW_READY_MAGIC, 20000);
+	if (err) {
+		pr_err("FW is loaded, but failed to start\n");
+		return err;
+	}
+
+	base = dev->mem_addr;
+
+	dev->cmd_mbox = base + mvsw_pr_reg_read32(dev, MVSW_CMD_BUF_OFFS_REG);
+	dev->cmd_mbox_len = mvsw_pr_reg_read32(dev, MVSW_CMD_BUF_LEN_REG);
+	mutex_init(&dev->cmd_mtx);
+
+	dev->evt_buf = base + mvsw_pr_reg_read32(dev, MVSW_EVT_BUF_OFFS_REG);
+	dev->evt_qnum = mvsw_pr_reg_read32(dev, MVSW_EVT_QNUM_REG);
+	dev->evt_msg = kmalloc(MVSW_MSG_MAX_SIZE, GFP_KERNEL);
+	if (!dev->evt_msg)
+		return -ENOMEM;
+
+	for (qid = 0; qid < dev->evt_qnum; qid++) {
+		u32 offs = mvsw_pr_reg_read32(dev, MVSW_EVTQ_OFFS_REG(qid));
+		struct mvsw_pr_pci_evtq *evtq = &dev->evt_queue[qid];
+
+		evtq->len = mvsw_pr_reg_read32(dev, MVSW_EVTQ_LEN_REG(qid));
+		evtq->addr = dev->evt_buf + offs;
+		mutex_init(&evtq->mtx);
+	}
+
+	return 0;
+}
+
+static void mvsw_pr_pci_fw_uninit(struct mvsw_pr_pci_device *dev)
+{
+	kfree(dev->evt_msg);
+}
+
+static irqreturn_t mvsw_pci_irq_handler(int irq, void *dev_id)
+{
+	struct mvsw_pr_pci_device *mvsw_pci = dev_id;
+
+	queue_work(mvsw_pci->dev.dev_wq, &mvsw_pci->evt_work);
+
+	return IRQ_HANDLED;
+}
+
+static u32 mvsw_ldr_buf_avail(struct mvsw_pr_pci_device *dev)
+{
+	u32 rd_idx = mvsw_ldr_reg_read32(dev, MVSW_LDR_BUF_RD_REG);
+
+	return CIRC_SPACE(dev->ldr_wr_idx, rd_idx, dev->ldr_buf_len);
+}
+
+static int mvsw_pr_pci_fw_send_buf(struct mvsw_pr_pci_device *dev,
+				   const u8 *buf, size_t len)
+{
+	int i;
+
+	if (!mvsw_pr_pci_wait_timeout(mvsw_ldr_buf_avail(dev) >= len, 100)) {
+		MVSW_LOG_ERROR("failed wait while sending firmware\n");
+		return -EBUSY;
+	}
+
+	for (i = 0; i < len; i += 4) {
+		writel_relaxed(*(u32 *)(buf + i), MVSW_LDR_WR_PTR(dev));
+		MVSW_LDR_WR_IDX_MOVE(dev, 4);
+	}
+
+	MVSW_LDR_WR_IDX_COMMIT(dev);
+	return 0;
+}
+
+static int mvsw_pr_pci_fw_send(struct mvsw_pr_pci_device *dev,
+			       const char *fw, u32 fw_size)
+{
+	u32 pos;
+	int err;
+	u32 status_reg;
+	u32 dl_result_mask;
+
+	if (mvsw_pr_pci_wait_reg32(dev, MVSW_LDR_STATUS_REG,
+				   MVSW_LDR_STATUS_IMG_DOWNLOADING, 1000)) {
+		pr_err("FW LDR is ready, but not ready to dl FW IMG");
+		return -EBUSY;
+	}
+
+	for (pos = 0; pos < fw_size; pos += MVSW_PR_FW_DOWNLOAD_BLOCK_SIZE) {
+		if (pos + MVSW_PR_FW_DOWNLOAD_BLOCK_SIZE > fw_size)
+			break;
+		err = mvsw_pr_pci_fw_send_buf(dev, fw + pos,
+					      MVSW_PR_FW_DOWNLOAD_BLOCK_SIZE);
+		if (err)
+			return err;
+	}
+
+	if (pos < fw_size) {
+		err = mvsw_pr_pci_fw_send_buf(dev, fw + pos, fw_size - pos);
+		if (err)
+			return err;
+	}
+
+	/* Waiting for status IMG_DOWNLOADING to change to something else */
+	dl_result_mask = ~(MVSW_LDR_STATUS_IMG_DOWNLOADING);
+
+	if (!mvsw_pr_pci_wait_timeout(mvsw_ldr_reg_read32(dev,
+							  MVSW_LDR_STATUS_REG)
+				      & dl_result_mask,
+				      MVSW_PR_FW_DOWNLOAD_WAIT_TIMEOUT)) {
+		pr_err("Timeout to download FW img [state=%d]",
+		       mvsw_pr_reg_read32(dev, MVSW_LDR_STATUS_REG));
+		return -ETIMEDOUT;
+	}
+
+	status_reg = mvsw_ldr_reg_read32(dev, MVSW_LDR_STATUS_REG);
+	if (status_reg != MVSW_LDR_STATUS_STARTING_FW) {
+		switch (status_reg) {
+		case MVSW_LDR_STATUS_INVALID_IMG:
+			pr_err("Downloaded FW img is invalid: bad crc\n");
+			return -EINVAL;
+		case MVSW_LDR_STATUS_NOMEM:
+			pr_err("Image download failed: LDR has no mem left\n");
+			return -ENOMEM;
+		default:
+			break;
+		}
+	}
+
+	return 0;
+}
+
+static bool mvsw_pr_pci_fw_ldr_is_ready(struct mvsw_pr_pci_device *mvsw_pci)
+{
+	return mvsw_ldr_reg_read32(mvsw_pci, MVSW_LDR_READY_REG) ==
+		MVSW_LDR_READY_MAGIC;
+}
+
+static int mvsw_pr_pci_check_fw_header(const struct firmware *fw)
+{
+	struct mvsw_pr_fw_header *hdr = (struct mvsw_pr_fw_header *)fw->data;
+	u32 major, minor, patch, magic, version;
+
+	magic = be32_to_cpu((__force __be32)hdr->magic_number);
+	version = be32_to_cpu((__force __be32)hdr->version_value);
+
+	MVSW_LOG_INFO("Got hdr, size %ld", sizeof(struct mvsw_pr_fw_header));
+
+	if (magic != MVSW_PR_FW_HEADER_MAGIC_NUMBER) {
+		pr_err("Magic number of file header is invalid");
+		return -EINVAL;
+	}
+
+	major = VERSION_GET_MAJOR(version);
+	minor = VERSION_GET_MINOR(version);
+	patch = VERSION_GET_PATCH(version);
+
+	if (major != MVSW_PR_SUPPORTED_FW_MAJOR_VERSION ||
+	    minor < MVSW_PR_SUPPORTED_FW_MINOR_VERSION) {
+		pr_err("FW version mismatch:(drv '%d.%d.%d', FW '%d.%d.%d')",
+		       MVSW_PR_SUPPORTED_FW_MAJOR_VERSION,
+		       MVSW_PR_SUPPORTED_FW_MINOR_VERSION,
+		       MVSW_PR_SUPPORTED_FW_PATCH_VERSION,
+		       major,
+		       minor,
+		       patch);
+		return -EINVAL;
+	}
+
+	pr_info("FW version '%u.%u.%u'\n", major, minor, patch);
+
+	return 0;
+}
+
+static int mvsw_pr_pci_fw_load(struct mvsw_pr_pci_device *mvsw_pci)
+{
+	const struct firmware *fw;
+	bool is_ldr_ready;
+	int err;
+
+	is_ldr_ready = mvsw_pr_pci_wait_timeout
+	    (mvsw_pr_pci_fw_ldr_is_ready(mvsw_pci), 1000);
+	if (!is_ldr_ready)
+		return -ETIMEDOUT;
+
+	mvsw_pci->ldr_ring_buf = mvsw_pci->ldr_regs +
+		mvsw_ldr_reg_read32(mvsw_pci, MVSW_LDR_BUF_OFFS_REG);
+
+	mvsw_pci->ldr_buf_len =
+		mvsw_ldr_reg_read32(mvsw_pci, MVSW_LDR_BUF_SIZE_REG);
+
+	mvsw_pci->ldr_wr_idx = 0;
+
+	MVSW_LOG_INFO("Trying to load FW '%s'", MVSW_PR_FW_FILENAME);
+	err = request_firmware_direct(&fw, MVSW_PR_FW_FILENAME,
+				      &mvsw_pci->pci_dev->dev);
+
+	if (err) {
+		MVSW_LOG_ERROR("failed request firmware file\n");
+		return err;
+	}
+
+	/* In case if user fed us invalid file (it's unaligned)
+	 * return error: pci bus is 4-bytes msg aligned
+	 */
+	if (MVSW_PCI_ALIGN(fw->size) - fw->size) {
+		pr_err("FW image file is invalid");
+		release_firmware(fw);
+		return -EINVAL;
+	}
+
+	err = mvsw_pr_pci_check_fw_header(fw);
+	if (err) {
+		pr_err("FW image file header is invalid\n");
+		release_firmware(fw);
+		return err;
+	}
+
+	mvsw_ldr_reg_write32(mvsw_pci, MVSW_LDR_IMG_SIZE_REG,
+			     (fw->size - sizeof(struct mvsw_pr_fw_header)));
+
+	mvsw_ldr_reg_write32(mvsw_pci, MVSW_LDR_CTL_REG, MVSW_LDR_CTL_DL_START);
+
+	MVSW_LOG_INFO("Loading prestera fw image ...");
+
+	/* header is only used for the driver; skip it */
+	err = mvsw_pr_pci_fw_send(mvsw_pci,
+				  fw->data + sizeof(struct mvsw_pr_fw_header),
+				  fw->size - sizeof(struct mvsw_pr_fw_header));
+
+	release_firmware(fw);
+	return err;
+}
+
+static int mvsw_pr_pci_probe(struct pci_dev *pdev,
+			     const struct pci_device_id *id)
+{
+	const char *driver_name = pdev->driver->name;
+	struct mvsw_pr_pci_device *mvsw_pci;
+	u8 __iomem *mem_addr;
+	size_t mem_len;
+	int err;
+
+	err = pci_enable_device(pdev);
+	if (err) {
+		dev_err(&pdev->dev, "pci_enable_device failed\n");
+		goto err_pci_enable_device;
+	}
+
+	err = pci_request_regions(pdev, driver_name);
+	if (err) {
+		dev_err(&pdev->dev, "pci_request_regions failed\n");
+		goto err_pci_request_regions;
+	}
+
+	mem_len = pci_resource_len(pdev, 2);
+
+	mem_addr = ioremap(pci_resource_start(pdev, 2), mem_len);
+	if (!mem_addr) {
+		dev_err(&pdev->dev, "ioremap failed\n");
+		err = -EIO;
+		goto err_ioremap;
+	}
+
+	pci_set_master(pdev);
+
+	mvsw_pci = kzalloc(sizeof(*mvsw_pci), GFP_KERNEL);
+	if (!mvsw_pci) {
+		err = -ENOMEM;
+		goto err_pci_dev_alloc;
+	}
+
+	mvsw_pci->pci_dev = pdev;
+	mvsw_pci->dev.dev = &pdev->dev;
+	mvsw_pci->dev.bus = &mvsw_pci_bus;
+	mvsw_pci->dev.name = driver_name;
+	mvsw_pci->mem_addr = mem_addr;
+	mvsw_pci->mem_len = mem_len;
+	mvsw_pci->ldr_regs = mem_addr;
+	mvsw_pci->hw_regs = mem_addr;
+
+	mvsw_pci->dev.dev_wq = alloc_workqueue("mvsw_pci_wq", WQ_HIGHPRI, 1);
+	if (!mvsw_pci->dev.dev_wq)
+		goto err_wq_alloc;
+
+	INIT_WORK(&mvsw_pci->evt_work, mvsw_pr_pci_evt_work_fn);
+
+	err = pci_alloc_irq_vectors(pdev, 1, 1, PCI_IRQ_MSI);
+	if (err < 0) {
+		dev_err(&pdev->dev, "MSI IRQ init failed\n");
+		goto err_irq_alloc;
+	}
+
+	err = request_irq(pci_irq_vector(pdev, 0), mvsw_pci_irq_handler,
+			  0, driver_name, mvsw_pci);
+	if (err) {
+		dev_err(&pdev->dev, "fail to request IRQ\n");
+		goto err_request_irq;
+	}
+
+	pci_set_drvdata(pdev, mvsw_pci);
+
+	pr_info("Waiting for Marvell's Prestera Switch FW ...\n");
+
+	err = mvsw_pr_pci_fw_init(mvsw_pci);
+	if (err)
+		goto err_mvsw_fw_init;
+
+	pr_info("Marvell's Prestera Switch FW is ready\n");
+
+	err = mvsw_pr_device_register(&mvsw_pci->dev);
+	if (err)
+		goto err_mvsw_dev_register;
+
+	return 0;
+
+err_mvsw_dev_register:
+	mvsw_pr_pci_fw_uninit(mvsw_pci);
+err_mvsw_fw_init:
+	free_irq(pci_irq_vector(pdev, 0), mvsw_pci);
+err_request_irq:
+	pci_free_irq_vectors(pdev);
+err_irq_alloc:
+	destroy_workqueue(mvsw_pci->dev.dev_wq);
+err_wq_alloc:
+	kfree(mvsw_pci);
+err_pci_dev_alloc:
+	iounmap(mem_addr);
+err_ioremap:
+	pci_release_regions(pdev);
+err_pci_request_regions:
+	pci_disable_device(pdev);
+err_pci_enable_device:
+	return err;
+}
+
+static void mvsw_pr_pci_remove(struct pci_dev *pdev)
+{
+	struct mvsw_pr_pci_device *mvsw_pci = pci_get_drvdata(pdev);
+
+	free_irq(pci_irq_vector(pdev, 0), mvsw_pci);
+	pci_free_irq_vectors(pdev);
+	mvsw_pr_device_unregister(&mvsw_pci->dev);
+	destroy_workqueue(mvsw_pci->dev.dev_wq);
+	mvsw_pr_pci_fw_uninit(mvsw_pci);
+	iounmap(mvsw_pci->mem_addr);
+	pci_release_regions(pdev);
+	pci_disable_device(pdev);
+	kfree(mvsw_pci);
+}
+
+static int __init mvsw_pr_pci_init(void)
+{
+	struct mvsw_pr_pci_match *match;
+	int err;
+
+	for (match = mvsw_pci_devices; match->driver.name; match++) {
+		match->driver.probe = mvsw_pr_pci_probe;
+		match->driver.remove = mvsw_pr_pci_remove;
+		match->driver.id_table = &match->id;
+
+		err = pci_register_driver(&match->driver);
+		if (err) {
+			pr_err("%s: failed to register %s\n", __func__,
+			       match->driver.name);
+			break;
+		}
+
+		match->registered = true;
+	}
+
+	if (err) {
+		for (match = mvsw_pci_devices; match->driver.name; match++) {
+			if (!match->registered)
+				break;
+
+			pci_unregister_driver(&match->driver);
+		}
+
+		return err;
+	}
+
+	pr_info("Initialized Marvell Prestera PCI interface\n");
+	return 0;
+}
+
+static void __exit mvsw_pr_pci_exit(void)
+{
+	struct mvsw_pr_pci_match *match;
+
+	for (match = mvsw_pci_devices; match->driver.name; match++) {
+		if (!match->registered)
+			break;
+
+		pci_unregister_driver(&match->driver);
+	}
+
+	pr_info("Un-initialized Marvell Prestera PCI interface\n");
+}
+
+module_init(mvsw_pr_pci_init);
+module_exit(mvsw_pr_pci_exit);
+
+MODULE_AUTHOR("Marvell Semi.");
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("Marvell Prestera switch PCI interface");
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx.c b/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx.c
new file mode 100644
index 0000000..345d2cc
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx.c
@@ -0,0 +1,224 @@
+// SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+/*
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+#include "prestera.h"
+#include "prestera_rxtx_priv.h"
+#include "prestera_dsa.h"
+
+#include <linux/if_vlan.h>
+#include <net/ip.h>
+
+struct mvsw_pr_rxtx;
+
+enum mvsw_pr_rxtx_type {
+	MVSW_PR_RXTX_MVPP,
+	MVSW_PR_RXTX_ETH,
+};
+
+static struct mvsw_pr_rxtx *rxtx_registered;
+
+netdev_tx_t mvsw_pr_rxtx_xmit(struct sk_buff *skb,
+			      struct mvsw_pr_rxtx_info *info)
+{
+	struct mvsw_pr_dsa dsa;
+	struct mvsw_pr_dsa_from_cpu *from_cpu;
+	struct net_device *dev = skb->dev;
+	struct mvsw_pr_port *port = netdev_priv(dev);
+	size_t dsa_resize_len = MVSW_PR_DSA_HLEN;
+
+	if (!rxtx_registered)
+		return NET_XMIT_DROP;
+
+	/* common DSA tag fill-up */
+	memset(&dsa, 0, sizeof(dsa));
+	dsa.dsa_cmd = MVSW_NET_DSA_CMD_FROM_CPU_E;
+
+	from_cpu = &dsa.dsa_info.from_cpu;
+	from_cpu->egr_filter_en = false;
+	from_cpu->egr_filter_registered = false;
+	from_cpu->dst_eport = port->hw_id;
+
+	from_cpu->dst_iface.dev_port.port_num = port->hw_id;
+	from_cpu->dst_iface.dev_port.hw_dev_num = port->dev_id;
+	from_cpu->dst_iface.type = MVSW_IF_PORT_E;
+
+	if (skb->protocol == htons(ETH_P_8021Q)) {
+		/* 802.1q packet tag size is 4 bytes, so DSA len would
+		 * need only allocation of MVSW_PR_DSA_HLEN - size of
+		 * 802.1q tag
+		 */
+		dsa_resize_len = MVSW_PR_DSA_HLEN - VLAN_HLEN;
+		dsa.common_params.vpt = skb_vlan_tag_get_prio(skb);
+		dsa.common_params.cfi_bit = skb_vlan_tag_get_cfi(skb);
+		dsa.common_params.vid = skb_vlan_tag_get_id(skb);
+	}
+
+	if (skb_cow_head(skb, dsa_resize_len) < 0)
+		return NET_XMIT_DROP;
+
+	/* expects skb->data at mac header */
+	skb_push_rcsum(skb, dsa_resize_len);
+	memmove(skb->data, skb->data + dsa_resize_len, 2 * ETH_ALEN);
+
+	if (mvsw_pr_dsa_build(&dsa, skb->data + 2 * ETH_ALEN) != 0)
+		return NET_XMIT_DROP;
+
+	return rxtx_registered->ops->rxtx_xmit(rxtx_registered, skb);
+}
+
+struct sk_buff *mvsw_pr_rxtx_recv_skb(struct mvsw_pr_rxtx *rxtx,
+				      struct sk_buff *skb)
+{
+	const struct mvsw_pr_port *port;
+	const struct ethhdr *eth;
+	const u8 *orig_mac_hdr;
+	unsigned int ip_len = 0;
+	u32 hw_port, hw_id;
+	struct mvsw_pr_dsa dsa;
+
+	/* parse/process DSA tag
+	 * ethertype field is part of the dsa header
+	 */
+	if (mvsw_pr_dsa_parse(skb->data - ETH_TLEN, &dsa))
+		return NULL;
+
+	/* get switch port */
+	hw_port = dsa.dsa_info.to_cpu.iface.port_num;
+	hw_id = dsa.dsa_info.to_cpu.hw_dev_num;
+	port = mvsw_pr_port_find(hw_id, hw_port);
+	if (unlikely(!port)) {
+		dev_warn_ratelimited(&skb->dev->dev,
+				     "skb received for non-existent port\n");
+		return NULL;
+	}
+
+	/* check skb */
+	skb = skb_share_check(skb, GFP_ATOMIC);
+	if (unlikely(!skb))
+		return NULL;
+
+	if (unlikely(!pskb_may_pull(skb, MVSW_PR_DSA_HLEN)))
+		return NULL;
+
+	/* remove DSA tag and update checksum */
+	skb_pull_rcsum(skb, MVSW_PR_DSA_HLEN);
+
+	/* reset mac header */
+	orig_mac_hdr = skb_mac_header(skb);
+	skb_set_mac_header(skb, (-skb->mac_len));
+	memmove(skb_mac_header(skb), orig_mac_hdr, skb->mac_len - ETH_TLEN);
+
+	/* reset network header */
+	skb_reset_network_header(skb);
+
+	/* update protocol type */
+	eth = (struct ethhdr *)skb_mac_header(skb);
+	skb->protocol = eth->h_proto;
+
+	/* update packet type */
+	if (skb->pkt_type != PACKET_BROADCAST &&
+	    skb->pkt_type != PACKET_MULTICAST)
+		skb->pkt_type = PACKET_HOST;
+
+	/* reset transport header */
+	if (likely(skb->protocol == htons(ETH_P_IP))) {
+		ip_len = ip_hdrlen(skb);
+		if (unlikely(ip_len < sizeof(struct iphdr)))
+			return NULL;
+	}
+	skb_set_transport_header(skb, ip_len);
+
+	/* change net device to our port */
+	skb->dev = port->net_dev;
+	return skb;
+}
+
+static struct mvsw_pr_rxtx_ops rxtx_driver_ops[] = {
+	[MVSW_PR_RXTX_MVPP] = {
+		.rxtx_init = mvsw_pr_rxtx_mvpp_init,
+		.rxtx_fini = mvsw_pr_rxtx_mvpp_fini,
+		.rxtx_xmit = mvsw_pr_rxtx_mvpp_xmit,
+	},
+	[MVSW_PR_RXTX_ETH] = {
+		.rxtx_init = mvsw_pr_rxtx_eth_init,
+		.rxtx_fini = mvsw_pr_rxtx_eth_fini,
+		.rxtx_xmit = mvsw_pr_rxtx_eth_xmit,
+	},
+};
+
+static int mvsw_pr_rxtx_probe(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+	enum mvsw_pr_rxtx_type rxtx_type;
+	struct mvsw_pr_rxtx *rxtx;
+	int err;
+
+	rxtx_type = (enum mvsw_pr_rxtx_type)of_device_get_match_data(dev);
+
+	rxtx = devm_kzalloc(dev, sizeof(*rxtx), GFP_KERNEL);
+	if (!rxtx)
+		return -ENOMEM;
+
+	rxtx->ops = &rxtx_driver_ops[rxtx_type];
+	rxtx->pdev = pdev;
+	rxtx->dev = dev;
+
+	platform_set_drvdata(pdev, rxtx);
+
+	if (rxtx->ops->rxtx_init) {
+		err = rxtx->ops->rxtx_init(rxtx);
+		if (err)
+			return err;
+	}
+
+	rxtx_registered = rxtx;
+
+	pr_info("Registered mvsw prestera rxtx driver\n");
+
+	return 0;
+}
+
+static int mvsw_pr_rxtx_remove(struct platform_device *pdev)
+{
+	struct mvsw_pr_rxtx *rxtx = platform_get_drvdata(pdev);
+	int err = 0;
+
+	if (rxtx->ops->rxtx_fini)
+		err = rxtx->ops->rxtx_fini(rxtx);
+
+	rxtx_registered = NULL;
+	return err;
+}
+
+static const struct of_device_id mvsw_pr_rxtx_match[] = {
+	{
+		.compatible = "marvell,prestera-switch-rxtx-mvpp",
+		.data = (void *)MVSW_PR_RXTX_MVPP,
+	},
+	{
+		.compatible = "marvell,prestera-switch-rxtx-eth",
+		.data = (void *)MVSW_PR_RXTX_ETH,
+	},
+	{ }
+};
+
+static struct platform_driver mvsw_pr_rxtx_driver = {
+	.probe = mvsw_pr_rxtx_probe,
+	.remove = mvsw_pr_rxtx_remove,
+	.driver = {
+		.name = "mvsw_pr_rxtx",
+		.of_match_table = mvsw_pr_rxtx_match,
+	},
+};
+
+int mvsw_pr_rxtx_init(void)
+{
+	return platform_driver_register(&mvsw_pr_rxtx_driver);
+}
+
+void mvsw_pr_rxtx_fini(void)
+{
+	platform_driver_unregister(&mvsw_pr_rxtx_driver);
+}
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx.h b/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx.h
new file mode 100644
index 0000000..6b2f76a
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx.h
@@ -0,0 +1,23 @@
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+
+#ifndef _MVSW_PRESTERA_RXTX_H_
+#define _MVSW_PRESTERA_RXTX_H_
+
+#include <linux/netdevice.h>
+
+struct mvsw_pr_rxtx_info {
+	u32 port_id;
+	u32 dev_id;
+};
+
+int mvsw_pr_rxtx_init(void);
+void mvsw_pr_rxtx_fini(void);
+
+netdev_tx_t mvsw_pr_rxtx_xmit(struct sk_buff *skb,
+			      struct mvsw_pr_rxtx_info *info);
+
+#endif /* _MVSW_PRESTERA_RXTX_H_ */
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx_eth.c b/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx_eth.c
new file mode 100644
index 0000000..ac28338
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx_eth.c
@@ -0,0 +1,89 @@
+// SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+/*
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+
+#include "prestera_rxtx_priv.h"
+
+#include <linux/rtnetlink.h>
+#include <linux/of_net.h>
+
+struct mvsw_pr_rxtx_eth {
+	struct mvsw_pr_rxtx *rxtx;
+	struct net_device *dev;
+};
+
+static rx_handler_result_t mvsw_pr_rxtx_eth_rx(struct sk_buff **pskb)
+{
+	struct mvsw_pr_rxtx_eth *rxtx_eth;
+	struct sk_buff *skb = *pskb;
+
+	rxtx_eth = rcu_dereference(skb->dev->rx_handler_data);
+	skb = mvsw_pr_rxtx_recv_skb(rxtx_eth->rxtx, skb);
+	if (likely(skb)) {
+		*pskb = skb;
+		return RX_HANDLER_ANOTHER;
+	}
+	return RX_HANDLER_PASS;
+}
+
+int mvsw_pr_rxtx_eth_init(struct mvsw_pr_rxtx *rxtx)
+{
+	struct device_node *rxtx_dn = rxtx->dev->of_node;
+	struct device_node *eth_dn = of_parse_phandle(rxtx_dn, "ethernet", 0);
+	unsigned int rxtx_flags = IFF_NOARP | IFF_PROMISC | IFF_UP;
+	struct mvsw_pr_rxtx_eth *rxtx_eth;
+	struct net_device *dev;
+	int err;
+
+	if (!eth_dn)
+		return -EINVAL;
+
+	dev = of_find_net_device_by_node(eth_dn);
+	if (!dev)
+		return -EPROBE_DEFER;
+
+	rxtx_eth = devm_kzalloc(rxtx->dev, sizeof(*rxtx_eth), GFP_KERNEL);
+	if (!rxtx_eth)
+		return -ENOMEM;
+
+	rxtx->priv = rxtx_eth;
+	rxtx_eth->rxtx = rxtx;
+	rxtx_eth->dev = dev;
+
+	rtnl_lock();
+
+	dev_change_flags(dev, dev->flags | rxtx_flags, NULL);
+
+	err = netdev_rx_handler_register(dev, mvsw_pr_rxtx_eth_rx, rxtx_eth);
+	if (err) {
+		pr_err("failed to register rx handler for the cpu interface\n");
+		rtnl_unlock();
+		return err;
+	}
+
+	rtnl_unlock();
+
+	return 0;
+}
+
+int mvsw_pr_rxtx_eth_fini(struct mvsw_pr_rxtx *rxtx)
+{
+	struct mvsw_pr_rxtx_eth *rxtx_eth = rxtx->priv;
+
+	rtnl_lock();
+	/* Can be called only while holding rtnl lock */
+	netdev_rx_handler_unregister(rxtx_eth->dev);
+	rtnl_unlock();
+
+	return 0;
+}
+
+netdev_tx_t mvsw_pr_rxtx_eth_xmit(struct mvsw_pr_rxtx *rxtx,
+				  struct sk_buff *skb)
+{
+	struct mvsw_pr_rxtx_eth *rxtx_eth = rxtx->priv;
+
+	return rxtx_eth->dev->netdev_ops->ndo_start_xmit(skb, rxtx_eth->dev);
+}
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx_mvpp.c b/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx_mvpp.c
new file mode 100644
index 0000000..5f598db
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx_mvpp.c
@@ -0,0 +1,129 @@
+// SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+/*
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+#include <linux/kernel.h>
+#include <linux/netdevice.h>
+#include <linux/rtnetlink.h>
+#include <linux/platform_device.h>
+#include <linux/of.h>
+#include <linux/of_net.h>
+#include <linux/of_device.h>
+#ifdef CONFIG_MRVL_PRESTERA_SW_RXTX_MVPP2
+#include <linux/platform_data/mvpp2.h>
+#endif
+
+#include "prestera_rxtx_priv.h"
+
+#ifdef CONFIG_MRVL_PRESTERA_SW_RXTX_MVPP2
+struct mvsw_pr_rxtx_mvpp {
+	struct mvsw_pr_rxtx *rxtx;
+	struct device_node *mvpp_dn;
+	void *port_priv;
+};
+
+static int mvsw_pr_rxtx_mvpp_rx_hook(u8 port_id, struct sk_buff *skb, void *arg)
+{
+	struct mvsw_pr_rxtx_mvpp *rxtx_mvpp = arg;
+
+	return mvsw_pr_rxtx_recv_skb(rxtx_mvpp->rxtx, skb);
+}
+
+int mvsw_pr_rxtx_mvpp_init(struct mvsw_pr_rxtx *rxtx)
+{
+	const struct fwnode_handle *fwnode = rxtx->pdev->dev.fwnode;
+	struct mvsw_pr_rxtx_mvpp *rxtx_mvpp;
+	struct mvpp2_port_platform_data pdata;
+	struct fwnode_handle *port_fwnode;
+	struct platform_device *mvpp_pd;
+	struct device_node *mvpp_dn;
+	struct device_node *port_dn;
+	int err;
+
+	rxtx_mvpp = devm_kzalloc(rxtx->dev, sizeof(*rxtx_mvpp), GFP_KERNEL);
+	if (!rxtx_mvpp)
+		return -ENOMEM;
+
+	rxtx->priv = rxtx_mvpp;
+
+	mvpp_dn = of_find_compatible_node(NULL, NULL, "marvell,armada-7k-pp22");
+	if (!mvpp_dn) {
+		pr_err("failed to find mvpp2 node\n");
+		return -EEXIST;
+	}
+
+	mvpp_pd = of_find_device_by_node(mvpp_dn);
+	if (!mvpp_pd) {
+		pr_err("failed to find mvpp2 platform device\n");
+		err = -EINVAL;
+		goto put_node;
+	}
+
+	pdata.rx_hook.fn = mvsw_pr_rxtx_mvpp_rx_hook;
+	pdata.rx_hook.arg = rxtx_mvpp;
+
+	port_fwnode = fwnode_get_named_child_node(fwnode, "cpu_port");
+	if (!port_fwnode) {
+		pr_err("failed to find cpu port node\n");
+		err = -EINVAL;
+		goto put_node;
+	}
+
+	/* re-assign parent as mvpp2 node for irq lookup */
+	port_dn = to_of_node(port_fwnode);
+	if (port_dn)
+		port_dn->parent = mvpp_dn;
+
+	err = mvpp2_port_probe(mvpp_pd, port_fwnode, &pdata);
+	if (err) {
+		pr_err("mvsw prestera rxtx: mvpp2 probe failed\n");
+		goto err_port_probe;
+	}
+
+	rxtx_mvpp->port_priv = pdata.priv;
+	rxtx_mvpp->mvpp_dn = mvpp_dn;
+
+	pr_info("mvsw prestera rxtx: mvpp2 successfully probed\n");
+
+	return 0;
+
+err_port_probe:
+put_node:
+	of_node_put(mvpp_dn);
+	return err;
+}
+
+netdev_tx_t mvsw_pr_rxtx_mvpp_xmit(struct mvsw_pr_rxtx *rxtx,
+				   struct sk_buff *skb)
+{
+	return -EINVAL;
+}
+
+int mvsw_pr_rxtx_mvpp_fini(struct mvsw_pr_rxtx *rxtx)
+{
+	struct mvsw_pr_rxtx_mvpp *rxtx_mvpp = rxtx->priv;
+
+	of_node_put(rxtx_mvpp->mvpp_dn);
+
+	mvpp2_port_remove(rxtx_mvpp->port_priv);
+
+	return 0;
+}
+#else
+int mvsw_pr_rxtx_mvpp_init(struct mvsw_pr_rxtx *rxtx)
+{
+	return -EINVAL;
+}
+
+int mvsw_pr_rxtx_mvpp_fini(struct mvsw_pr_rxtx *rxtx)
+{
+	return -EINVAL;
+}
+
+netdev_tx_t mvsw_pr_rxtx_mvpp_xmit(struct mvsw_pr_rxtx *rxtx,
+				   struct sk_buff *skb)
+{
+	return -EINVAL;
+}
+#endif /* CONFIG_MRVL_PRESTERA_SW_RXTX_MVPP2 */
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx_priv.h b/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx_priv.h
new file mode 100644
index 0000000..ee1e978
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_rxtx_priv.h
@@ -0,0 +1,44 @@
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+#include <linux/kernel.h>
+#include <linux/netdevice.h>
+#include <linux/rtnetlink.h>
+#include <linux/platform_device.h>
+#include <linux/of.h>
+#include <linux/of_device.h>
+
+#include "prestera_rxtx.h"
+
+struct mvsw_pr_rxtx;
+
+struct mvsw_pr_rxtx_ops {
+	int (*rxtx_init)(struct mvsw_pr_rxtx *rxtx);
+	int (*rxtx_fini)(struct mvsw_pr_rxtx *rxtx);
+
+	netdev_tx_t (*rxtx_xmit)(struct mvsw_pr_rxtx *rxtx,
+				 struct sk_buff *skb);
+};
+
+struct mvsw_pr_rxtx {
+	struct platform_device *pdev;
+	struct device *dev;
+
+	const struct mvsw_pr_rxtx_ops *ops;
+	void *priv;
+};
+
+struct sk_buff *mvsw_pr_rxtx_recv_skb(struct mvsw_pr_rxtx *rxtx,
+				      struct sk_buff *skb);
+
+int mvsw_pr_rxtx_eth_init(struct mvsw_pr_rxtx *rxtx);
+int mvsw_pr_rxtx_eth_fini(struct mvsw_pr_rxtx *rxtx);
+netdev_tx_t mvsw_pr_rxtx_eth_xmit(struct mvsw_pr_rxtx *rxtx,
+				  struct sk_buff *skb);
+
+int mvsw_pr_rxtx_mvpp_init(struct mvsw_pr_rxtx *rxtx);
+int mvsw_pr_rxtx_mvpp_fini(struct mvsw_pr_rxtx *rxtx);
+netdev_tx_t mvsw_pr_rxtx_mvpp_xmit(struct mvsw_pr_rxtx *rxtx,
+				   struct sk_buff *skb);
diff --git a/drivers/net/ethernet/marvell/prestera_sw/prestera_switchdev.c b/drivers/net/ethernet/marvell/prestera_sw/prestera_switchdev.c
new file mode 100644
index 0000000..c72f5cd
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/prestera_switchdev.c
@@ -0,0 +1,1456 @@
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/if_vlan.h>
+#include <linux/if_bridge.h>
+#include <linux/notifier.h>
+#include <net/switchdev.h>
+#include <net/netevent.h>
+#include <net/vxlan.h>
+
+#include "prestera.h"
+#include "prestera_log.h"
+
+struct mvsw_pr_bridge {
+	struct mvsw_pr_switch *sw;
+	u32 ageing_time;
+	struct list_head bridge_list;
+	bool bridge_8021q_exists;
+};
+
+struct mvsw_pr_bridge_device {
+	struct net_device *dev;
+	struct list_head switch_node;
+	struct list_head port_list; /* list of mvsw_pr_bridge_port */
+	u16 bridge_id;
+	u8 vlan_enabled:1, multicast_enabled:1, mrouter:1;
+};
+
+struct mvsw_pr_bridge_port {
+	struct net_device *dev;
+	struct mvsw_pr_bridge_device *bridge_device;
+	struct list_head bridge_device_node;
+	struct list_head vlan_list; /* list of mvsw_pr_bridge_vlan */
+	unsigned int ref_count;
+	u8 stp_state;
+	unsigned long flags;
+};
+
+struct mvsw_pr_bridge_vlan {
+	struct list_head bridge_port_node;
+	struct list_head port_vlan_list; /* list of mvsw_pr_port_vlan */
+	u16 vid;
+};
+
+struct mvsw_pr_event_work {
+	struct work_struct work;
+	struct switchdev_notifier_fdb_info fdb_info;
+	struct net_device *dev;
+	unsigned long event;
+};
+
+/* ordered work queue */
+static struct workqueue_struct *mvsw_owq;
+
+static struct mvsw_pr_bridge_port *
+mvsw_pr_bridge_port_get(struct mvsw_pr_bridge *bridge,
+			struct net_device *brport_dev);
+
+static void mvsw_pr_bridge_port_put(struct mvsw_pr_bridge *bridge,
+				    struct mvsw_pr_bridge_port *bridge_port);
+
+static struct mvsw_pr_bridge_device *
+mvsw_pr_bridge_device_find(const struct mvsw_pr_bridge *bridge,
+			   const struct net_device *br_dev)
+{
+	struct mvsw_pr_bridge_device *bridge_device;
+
+	list_for_each_entry(bridge_device, &bridge->bridge_list,
+			    switch_node)
+		if (bridge_device->dev == br_dev)
+			return bridge_device;
+
+	return NULL;
+}
+
+static bool
+mvsw_pr_bridge_device_is_offloaded(const struct mvsw_pr_switch *sw,
+				   const struct net_device *br_dev)
+{
+	return !!mvsw_pr_bridge_device_find(sw->bridge, br_dev);
+}
+
+static struct mvsw_pr_bridge_port *
+__mvsw_pr_bridge_port_find(const struct mvsw_pr_bridge_device *bridge_device,
+			   const struct net_device *brport_dev)
+{
+	struct mvsw_pr_bridge_port *bridge_port;
+
+	list_for_each_entry(bridge_port, &bridge_device->port_list,
+			    bridge_device_node) {
+		if (bridge_port->dev == brport_dev)
+			return bridge_port;
+	}
+
+	return NULL;
+}
+
+static struct mvsw_pr_bridge_port *
+mvsw_pr_bridge_port_find(struct mvsw_pr_bridge *bridge,
+			 struct net_device *brport_dev)
+{
+	struct net_device *br_dev = netdev_master_upper_dev_get(brport_dev);
+	struct mvsw_pr_bridge_device *bridge_device;
+
+	if (!br_dev)
+		return NULL;
+
+	bridge_device = mvsw_pr_bridge_device_find(bridge, br_dev);
+	if (!bridge_device)
+		return NULL;
+
+	return __mvsw_pr_bridge_port_find(bridge_device, brport_dev);
+}
+
+static struct mvsw_pr_bridge_vlan *
+mvsw_pr_bridge_vlan_find(const struct mvsw_pr_bridge_port *bridge_port, u16 vid)
+{
+	struct mvsw_pr_bridge_vlan *bridge_vlan;
+
+	list_for_each_entry(bridge_vlan, &bridge_port->vlan_list,
+			    bridge_port_node) {
+		if (bridge_vlan->vid == vid)
+			return bridge_vlan;
+	}
+
+	return NULL;
+}
+
+static struct mvsw_pr_bridge_vlan *
+mvsw_pr_bridge_vlan_create(struct mvsw_pr_bridge_port *bridge_port, u16 vid)
+{
+	struct mvsw_pr_bridge_vlan *bridge_vlan;
+
+	bridge_vlan = kzalloc(sizeof(*bridge_vlan), GFP_KERNEL);
+	if (!bridge_vlan)
+		return NULL;
+
+	INIT_LIST_HEAD(&bridge_vlan->port_vlan_list);
+	bridge_vlan->vid = vid;
+	list_add(&bridge_vlan->bridge_port_node, &bridge_port->vlan_list);
+
+	return bridge_vlan;
+}
+
+static void
+mvsw_pr_bridge_vlan_destroy(struct mvsw_pr_bridge_vlan *bridge_vlan)
+{
+	list_del(&bridge_vlan->bridge_port_node);
+	WARN_ON(!list_empty(&bridge_vlan->port_vlan_list));
+	kfree(bridge_vlan);
+}
+
+static struct mvsw_pr_bridge_vlan *
+mvsw_pr_bridge_vlan_get(struct mvsw_pr_bridge_port *bridge_port, u16 vid)
+{
+	struct mvsw_pr_bridge_vlan *bridge_vlan;
+
+	bridge_vlan = mvsw_pr_bridge_vlan_find(bridge_port, vid);
+	if (bridge_vlan)
+		return bridge_vlan;
+
+	return mvsw_pr_bridge_vlan_create(bridge_port, vid);
+}
+
+static void mvsw_pr_bridge_vlan_put(struct mvsw_pr_bridge_vlan *bridge_vlan)
+{
+	if (list_empty(&bridge_vlan->port_vlan_list))
+		mvsw_pr_bridge_vlan_destroy(bridge_vlan);
+}
+
+static int
+mvsw_pr_port_vlan_bridge_join(struct mvsw_pr_port_vlan *mvsw_pr_port_vlan,
+			      struct mvsw_pr_bridge_port *bridge_port,
+			      struct netlink_ext_ack *extack)
+{
+	struct mvsw_pr_port *mvsw_pr_port = mvsw_pr_port_vlan->mvsw_pr_port;
+	struct mvsw_pr_bridge_vlan *bridge_vlan;
+	u16 vid = mvsw_pr_port_vlan->vid;
+	int err;
+
+	if (mvsw_pr_port_vlan->bridge_port)
+		return 0;
+
+	err = mvsw_pr_port_flood_set(mvsw_pr_port,
+				     bridge_port->flags & BR_FLOOD);
+	if (err)
+		return err;
+
+	err = mvsw_pr_port_learning_set(mvsw_pr_port,
+					bridge_port->flags & BR_LEARNING);
+	if (err)
+		goto err_port_learning_set;
+
+	bridge_vlan = mvsw_pr_bridge_vlan_get(bridge_port, vid);
+	if (!bridge_vlan) {
+		err = -ENOMEM;
+		goto err_bridge_vlan_get;
+	}
+
+	list_add(&mvsw_pr_port_vlan->bridge_vlan_node,
+		 &bridge_vlan->port_vlan_list);
+
+	mvsw_pr_bridge_port_get(mvsw_pr_port->sw->bridge, bridge_port->dev);
+	mvsw_pr_port_vlan->bridge_port = bridge_port;
+
+	return 0;
+
+err_bridge_vlan_get:
+	mvsw_pr_port_learning_set(mvsw_pr_port, false);
+err_port_learning_set:
+	return err;
+}
+
+static int
+mvsw_pr_bridge_vlan_port_count_get(struct mvsw_pr_bridge_device *bridge_device,
+				   u16 vid)
+{
+	int count = 0;
+	struct mvsw_pr_bridge_port *bridge_port;
+	struct mvsw_pr_bridge_vlan *bridge_vlan;
+
+	list_for_each_entry(bridge_port, &bridge_device->port_list,
+			    bridge_device_node) {
+		list_for_each_entry(bridge_vlan, &bridge_port->vlan_list,
+				    bridge_port_node) {
+			if (bridge_vlan->vid == vid) {
+				count += 1;
+				break;
+			}
+		}
+	}
+
+	return count;
+}
+
+void
+mvsw_pr_port_vlan_bridge_leave(struct mvsw_pr_port_vlan *mvsw_pr_port_vlan)
+{
+	struct mvsw_pr_port *mvsw_pr_port = mvsw_pr_port_vlan->mvsw_pr_port;
+	struct mvsw_pr_bridge_vlan *bridge_vlan;
+	struct mvsw_pr_bridge_port *bridge_port;
+	int port_count;
+	u16 vid = mvsw_pr_port_vlan->vid;
+	bool last_port, last_vlan;
+
+	bridge_port = mvsw_pr_port_vlan->bridge_port;
+	last_vlan = list_is_singular(&bridge_port->vlan_list);
+	port_count =
+	    mvsw_pr_bridge_vlan_port_count_get(bridge_port->bridge_device, vid);
+	bridge_vlan = mvsw_pr_bridge_vlan_find(bridge_port, vid);
+	last_port = port_count == 1;
+	if (last_vlan) {
+		mvsw_pr_fdb_flush_port(mvsw_pr_port,
+				       MVSW_PR_FDB_FLUSH_MODE_DYNAMIC);
+	} else if (last_port) {
+		mvsw_pr_fdb_flush_vlan(mvsw_pr_port->sw, vid,
+				       MVSW_PR_FDB_FLUSH_MODE_DYNAMIC);
+	} else {
+		mvsw_pr_fdb_flush_port_vlan(mvsw_pr_port, vid,
+					    MVSW_PR_FDB_FLUSH_MODE_DYNAMIC);
+	}
+
+	list_del(&mvsw_pr_port_vlan->bridge_vlan_node);
+	mvsw_pr_bridge_vlan_put(bridge_vlan);
+	mvsw_pr_bridge_port_put(mvsw_pr_port->sw->bridge, bridge_port);
+	mvsw_pr_port_vlan->bridge_port = NULL;
+}
+
+static int
+mvsw_pr_bridge_port_vlan_add(struct mvsw_pr_port *mvsw_pr_port,
+			     struct mvsw_pr_bridge_port *bridge_port,
+			     u16 vid, bool is_untagged, bool is_pvid,
+			     struct netlink_ext_ack *extack)
+{
+	u16 pvid;
+	struct mvsw_pr_port_vlan *mvsw_pr_port_vlan;
+	u16 old_pvid = mvsw_pr_port->pvid;
+	int err;
+
+	MVSW_LOG_INFO("[port=%u, vid=%u]", mvsw_pr_port->id, vid);
+
+	if (is_pvid)
+		pvid = vid;
+	else
+		pvid = mvsw_pr_port->pvid == vid ? 0 : mvsw_pr_port->pvid;
+
+	mvsw_pr_port_vlan = mvsw_pr_port_vlan_find_by_vid(mvsw_pr_port, vid);
+	if (mvsw_pr_port_vlan &&
+	    mvsw_pr_port_vlan->bridge_port != bridge_port)
+		return -EEXIST;
+
+	if (!mvsw_pr_port_vlan) {
+		mvsw_pr_port_vlan = mvsw_pr_port_vlan_create(mvsw_pr_port, vid);
+		if (IS_ERR(mvsw_pr_port_vlan))
+			return PTR_ERR(mvsw_pr_port_vlan);
+	}
+
+	err = mvsw_pr_port_vlan_set(mvsw_pr_port, vid, true, is_untagged);
+	if (err)
+		goto err_port_vlan_set;
+
+	err = mvsw_pr_port_pvid_set(mvsw_pr_port, pvid);
+	if (err)
+		goto err_port_pvid_set;
+
+	err = mvsw_pr_port_vlan_bridge_join(mvsw_pr_port_vlan, bridge_port,
+					    extack);
+	if (err)
+		goto err_port_vlan_bridge_join;
+
+	return 0;
+
+err_port_vlan_bridge_join:
+	mvsw_pr_port_pvid_set(mvsw_pr_port, old_pvid);
+err_port_pvid_set:
+	mvsw_pr_port_vlan_set(mvsw_pr_port, vid, false, false);
+err_port_vlan_set:
+	mvsw_pr_port_vlan_destroy(mvsw_pr_port_vlan);
+
+	return err;
+}
+
+static int mvsw_pr_port_vlans_add(struct mvsw_pr_port *mvsw_pr_port,
+				  const struct switchdev_obj_port_vlan *vlan,
+				  struct switchdev_trans *trans,
+				  struct netlink_ext_ack *extack)
+{
+	bool flag_untagged = vlan->flags & BRIDGE_VLAN_INFO_UNTAGGED;
+	bool flag_pvid = vlan->flags & BRIDGE_VLAN_INFO_PVID;
+	struct mvsw_pr_switch *sw = mvsw_pr_port->sw;
+	struct net_device *orig_dev = vlan->obj.orig_dev;
+	struct mvsw_pr_bridge_port *bridge_port;
+	u16 vid;
+
+	if (netif_is_bridge_master(orig_dev))
+		return 0;
+
+	if (switchdev_trans_ph_commit(trans))
+		return 0;
+
+	MVSW_LOG_INFO("[port=%u, orig_dev=%s]", mvsw_pr_port->id,
+		      orig_dev->name);
+
+	bridge_port = mvsw_pr_bridge_port_find(sw->bridge, orig_dev);
+	if (WARN_ON(!bridge_port))
+		return -EINVAL;
+
+	if (!bridge_port->bridge_device->vlan_enabled)
+		return 0;
+
+	for (vid = vlan->vid_begin; vid <= vlan->vid_end; vid++) {
+		int err;
+
+		err = mvsw_pr_bridge_port_vlan_add(mvsw_pr_port, bridge_port,
+						   vid, flag_untagged,
+						   flag_pvid, extack);
+		if (err)
+			return err;
+	}
+
+	return 0;
+}
+
+static int mvsw_pr_port_obj_add(struct net_device *dev,
+				const struct switchdev_obj *obj,
+				struct switchdev_trans *trans,
+				struct netlink_ext_ack *extack)
+{
+	int err = 0;
+	struct mvsw_pr_port *mvsw_pr_port = netdev_priv(dev);
+	const struct switchdev_obj_port_vlan *vlan;
+
+	MVSW_LOG_INFO("Processing switchdev port add event [port=%d, attr=%s]",
+		      mvsw_pr_port->id,
+		      ENUM_TO_NAME(switchdev_obj_id, obj->id));
+
+	switch (obj->id) {
+	case SWITCHDEV_OBJ_ID_PORT_VLAN:
+		vlan = SWITCHDEV_OBJ_PORT_VLAN(obj);
+		err = mvsw_pr_port_vlans_add(mvsw_pr_port, vlan, trans, extack);
+		break;
+	case SWITCHDEV_OBJ_ID_PORT_MDB:
+		err = -EOPNOTSUPP;
+		break;
+	default:
+		err = -EOPNOTSUPP;
+	}
+
+	return err;
+}
+
+static void
+mvsw_pr_bridge_port_vlan_del(struct mvsw_pr_port *mvsw_pr_port,
+			     struct mvsw_pr_bridge_port *bridge_port, u16 vid)
+{
+	u16 pvid = mvsw_pr_port->pvid == vid ? 0 : mvsw_pr_port->pvid;
+	struct mvsw_pr_port_vlan *mvsw_pr_port_vlan;
+
+	mvsw_pr_port_vlan = mvsw_pr_port_vlan_find_by_vid(mvsw_pr_port, vid);
+	if (WARN_ON(!mvsw_pr_port_vlan))
+		return;
+
+	mvsw_pr_port_vlan_bridge_leave(mvsw_pr_port_vlan);
+	mvsw_pr_port_pvid_set(mvsw_pr_port, pvid);
+	mvsw_pr_port_vlan_destroy(mvsw_pr_port_vlan);
+}
+
+static int mvsw_pr_port_vlans_del(struct mvsw_pr_port *mvsw_pr_port,
+				  const struct switchdev_obj_port_vlan *vlan)
+{
+	struct mvsw_pr_switch *sw = mvsw_pr_port->sw;
+	struct net_device *orig_dev = vlan->obj.orig_dev;
+	struct mvsw_pr_bridge_port *bridge_port;
+	u16 vid;
+
+	if (netif_is_bridge_master(orig_dev))
+		return -EOPNOTSUPP;
+
+	bridge_port = mvsw_pr_bridge_port_find(sw->bridge, orig_dev);
+	if (WARN_ON(!bridge_port))
+		return -EINVAL;
+
+	if (!bridge_port->bridge_device->vlan_enabled)
+		return 0;
+
+	for (vid = vlan->vid_begin; vid <= vlan->vid_end; vid++)
+		mvsw_pr_bridge_port_vlan_del(mvsw_pr_port, bridge_port, vid);
+
+	return 0;
+}
+
+static int mvsw_pr_port_obj_del(struct net_device *dev,
+				const struct switchdev_obj *obj)
+{
+	int err = 0;
+	struct mvsw_pr_port *mvsw_pr_port = netdev_priv(dev);
+
+	MVSW_LOG_INFO("Processing switchdev port del event [port=%d, attr=%s]",
+		      mvsw_pr_port->id,
+		      ENUM_TO_NAME(switchdev_obj_id, obj->id));
+
+	/* TO DO: process port event */
+	switch (obj->id) {
+	case SWITCHDEV_OBJ_ID_PORT_VLAN:
+		err = mvsw_pr_port_vlans_del(mvsw_pr_port,
+					     SWITCHDEV_OBJ_PORT_VLAN(obj));
+		break;
+	case SWITCHDEV_OBJ_ID_PORT_MDB:
+		err = -EOPNOTSUPP;
+		break;
+	default:
+		err = -EOPNOTSUPP;
+		break;
+	}
+
+	return err;
+}
+
+static int mvsw_pr_port_attr_br_vlan_set(struct mvsw_pr_port *mvsw_pr_port,
+					 struct switchdev_trans *trans,
+					 struct net_device *orig_dev,
+					 bool vlan_enabled)
+{
+	struct mvsw_pr_switch *sw = mvsw_pr_port->sw;
+	struct mvsw_pr_bridge_device *bridge_device;
+
+	if (!switchdev_trans_ph_prepare(trans))
+		return 0;
+
+	bridge_device = mvsw_pr_bridge_device_find(sw->bridge, orig_dev);
+	if (WARN_ON(!bridge_device))
+		return -EINVAL;
+
+	if (bridge_device->vlan_enabled == vlan_enabled)
+		return 0;
+
+	netdev_err(bridge_device->dev,
+		   "VLAN filtering can't be changed for existing bridge\n");
+	return -EINVAL;
+}
+
+static int mvsw_pr_port_attr_br_flags_set(struct mvsw_pr_port *mvsw_pr_port,
+					  struct switchdev_trans *trans,
+					  struct net_device *orig_dev,
+					  unsigned long brport_flags)
+{
+	struct mvsw_pr_bridge_port *bridge_port;
+	int err;
+
+	if (switchdev_trans_ph_prepare(trans))
+		return 0;
+
+	bridge_port = mvsw_pr_bridge_port_find(mvsw_pr_port->sw->bridge,
+					       orig_dev);
+	if (!bridge_port)
+		return 0;
+
+	err = mvsw_pr_port_flood_set(mvsw_pr_port, brport_flags & BR_FLOOD);
+	if (err)
+		return err;
+
+	err = mvsw_pr_port_learning_set(mvsw_pr_port,
+					brport_flags & BR_LEARNING);
+	if (err)
+		return err;
+
+	memcpy(&bridge_port->flags, &brport_flags, sizeof(brport_flags));
+	return 0;
+}
+
+static int mvsw_pr_port_attr_br_ageing_set(struct mvsw_pr_port *mvsw_pr_port,
+					   struct switchdev_trans *trans,
+					   unsigned long ageing_clock_t)
+{
+	int err;
+	struct mvsw_pr_switch *sw = mvsw_pr_port->sw;
+	unsigned long ageing_jiffies = clock_t_to_jiffies(ageing_clock_t);
+	u32 ageing_time = jiffies_to_msecs(ageing_jiffies) / 1000;
+
+	if (switchdev_trans_ph_prepare(trans)) {
+		if (ageing_time < MVSW_PR_MIN_AGEING_TIME ||
+		    ageing_time > MVSW_PR_MAX_AGEING_TIME)
+			return -ERANGE;
+		else
+			return 0;
+	}
+
+	err = mvsw_pr_switch_ageing_set(sw, ageing_time);
+	if (!err)
+		sw->bridge->ageing_time = ageing_time;
+
+	return err;
+}
+
+static int mvsw_pr_port_obj_attr_set(struct net_device *dev,
+				     const struct switchdev_attr *attr,
+				     struct switchdev_trans *trans)
+{
+	int err = 0;
+	struct mvsw_pr_port *mvsw_pr_port = netdev_priv(dev);
+
+	MVSW_LOG_INFO("Processing port attr set event [port=%d, attr=%s]",
+		      mvsw_pr_port->id,
+		      ENUM_TO_NAME(switchdev_attr_id, attr->id));
+
+	switch (attr->id) {
+	case SWITCHDEV_ATTR_ID_PORT_STP_STATE:
+		err = -EOPNOTSUPP;
+		break;
+	case SWITCHDEV_ATTR_ID_PORT_PRE_BRIDGE_FLAGS:
+		if (attr->u.brport_flags &
+		    ~(BR_LEARNING | BR_FLOOD | BR_MCAST_FLOOD))
+			err = -EINVAL;
+		break;
+	case SWITCHDEV_ATTR_ID_PORT_BRIDGE_FLAGS:
+		err = mvsw_pr_port_attr_br_flags_set(mvsw_pr_port, trans,
+						     attr->orig_dev,
+						     attr->u.brport_flags);
+		break;
+	case SWITCHDEV_ATTR_ID_BRIDGE_AGEING_TIME:
+		err = mvsw_pr_port_attr_br_ageing_set(mvsw_pr_port, trans,
+						      attr->u.ageing_time);
+		break;
+	case SWITCHDEV_ATTR_ID_BRIDGE_VLAN_FILTERING:
+		err = mvsw_pr_port_attr_br_vlan_set(mvsw_pr_port, trans,
+						    attr->orig_dev,
+						    attr->u.vlan_filtering);
+		break;
+	case SWITCHDEV_ATTR_ID_PORT_MROUTER:
+		err = -EOPNOTSUPP;
+		break;
+	case SWITCHDEV_ATTR_ID_BRIDGE_MC_DISABLED:
+		err = -EOPNOTSUPP;
+		break;
+	case SWITCHDEV_ATTR_ID_BRIDGE_MROUTER:
+		err = -EOPNOTSUPP;
+		break;
+	default:
+		err = -EOPNOTSUPP;
+	}
+
+	return err;
+}
+
+static void mvsw_fdb_offload_notify(struct mvsw_pr_port *port,
+				    struct switchdev_notifier_fdb_info *info)
+{
+	struct switchdev_notifier_fdb_info send_info;
+
+	send_info.addr = info->addr;
+	send_info.vid = info->vid;
+	send_info.offloaded = true;
+	call_switchdev_notifiers(SWITCHDEV_FDB_OFFLOADED,
+				 port->net_dev, &send_info.info, NULL);
+}
+
+static int
+mvsw_pr_port_fdb_set(struct mvsw_pr_port *mvsw_pr_port,
+		     struct switchdev_notifier_fdb_info *fdb_info, bool adding)
+{
+	struct mvsw_pr_switch *sw = mvsw_pr_port->sw;
+	struct mvsw_pr_bridge_port *bridge_port;
+	struct mvsw_pr_bridge_device *bridge_device;
+	struct net_device *orig_dev = fdb_info->info.dev;
+	int err;
+	u16 vid;
+
+	bridge_port = mvsw_pr_bridge_port_find(sw->bridge, orig_dev);
+	if (!bridge_port)
+		return -EINVAL;
+
+	bridge_device = bridge_port->bridge_device;
+
+	if (bridge_device->vlan_enabled)
+		vid = fdb_info->vid;
+	else
+		vid = bridge_device->bridge_id;
+
+	if (adding)
+		err = mvsw_pr_fdb_add(mvsw_pr_port, fdb_info->addr, vid, false);
+	else
+		err = mvsw_pr_fdb_del(mvsw_pr_port, fdb_info->addr, vid);
+
+	return err;
+}
+
+static void mvsw_pr_bridge_fdb_event_work(struct work_struct *work)
+{
+	int err = 0;
+	struct mvsw_pr_event_work *switchdev_work =
+	    container_of(work, struct mvsw_pr_event_work, work);
+	struct net_device *dev = switchdev_work->dev;
+	struct switchdev_notifier_fdb_info *fdb_info;
+	struct mvsw_pr_port *mvsw_pr_port;
+
+	rtnl_lock();
+	if (netif_is_vxlan(dev))
+		goto out;
+
+	mvsw_pr_port = mvsw_pr_port_dev_lower_find(dev);
+	if (!mvsw_pr_port)
+		goto out;
+
+	switch (switchdev_work->event) {
+	case SWITCHDEV_FDB_ADD_TO_DEVICE:
+		fdb_info = &switchdev_work->fdb_info;
+		if (!fdb_info->added_by_user)
+			break;
+		err = mvsw_pr_port_fdb_set(mvsw_pr_port, fdb_info, true);
+		if (err)
+			break;
+		mvsw_fdb_offload_notify(mvsw_pr_port, fdb_info);
+		break;
+	case SWITCHDEV_FDB_DEL_TO_DEVICE:
+		fdb_info = &switchdev_work->fdb_info;
+		mvsw_pr_port_fdb_set(mvsw_pr_port, fdb_info, false);
+		break;
+	case SWITCHDEV_FDB_ADD_TO_BRIDGE:	/* fall through */
+	case SWITCHDEV_FDB_DEL_TO_BRIDGE:
+		break;
+	}
+
+out:
+	rtnl_unlock();
+	kfree(switchdev_work->fdb_info.addr);
+	kfree(switchdev_work);
+	dev_put(dev);
+}
+
+static int mvsw_pr_switchdev_event(struct notifier_block *unused,
+				   unsigned long event, void *ptr)
+{
+	int err = 0;
+	struct net_device *net_dev = switchdev_notifier_info_to_dev(ptr);
+	struct mvsw_pr_event_work *switchdev_work;
+	struct switchdev_notifier_fdb_info *fdb_info;
+	struct switchdev_notifier_info *info = ptr;
+	struct net_device *upper_br;
+
+	MVSW_LOG_INFO("Processing switchdev event [event=%s]",
+		      ENUM_TO_NAME(switchdev_notifier_type, event));
+
+	if (event == SWITCHDEV_PORT_ATTR_SET) {
+		err = switchdev_handle_port_attr_set(net_dev, ptr,
+						     mvsw_pr_netdev_check,
+						     mvsw_pr_port_obj_attr_set);
+		return notifier_from_errno(err);
+	}
+
+	upper_br = netdev_master_upper_dev_get_rcu(net_dev);
+	if (!upper_br)
+		return NOTIFY_DONE;
+
+	if (!netif_is_bridge_master(upper_br))
+		return NOTIFY_DONE;
+
+	switchdev_work = kzalloc(sizeof(*switchdev_work), GFP_ATOMIC);
+	if (!switchdev_work)
+		return NOTIFY_BAD;
+
+	switchdev_work->dev = net_dev;
+	switchdev_work->event = event;
+
+	switch (event) {
+	case SWITCHDEV_FDB_ADD_TO_DEVICE:
+	case SWITCHDEV_FDB_DEL_TO_DEVICE:
+	case SWITCHDEV_FDB_ADD_TO_BRIDGE:
+	case SWITCHDEV_FDB_DEL_TO_BRIDGE:
+		fdb_info = container_of(info,
+					struct switchdev_notifier_fdb_info,
+					info);
+
+		INIT_WORK(&switchdev_work->work, mvsw_pr_bridge_fdb_event_work);
+		memcpy(&switchdev_work->fdb_info, ptr,
+		       sizeof(switchdev_work->fdb_info));
+		switchdev_work->fdb_info.addr = kzalloc(ETH_ALEN, GFP_ATOMIC);
+		if (!switchdev_work->fdb_info.addr)
+			goto out;
+		ether_addr_copy((u8 *)switchdev_work->fdb_info.addr,
+				fdb_info->addr);
+		dev_hold(net_dev);
+
+		break;
+	case SWITCHDEV_VXLAN_FDB_ADD_TO_DEVICE:
+	case SWITCHDEV_VXLAN_FDB_DEL_TO_DEVICE:
+	default:
+		kfree(switchdev_work);
+		return NOTIFY_DONE;
+	}
+
+	queue_work(mvsw_owq, &switchdev_work->work);
+	return NOTIFY_DONE;
+out:
+	kfree(switchdev_work);
+	return NOTIFY_BAD;
+}
+
+static int mvsw_pr_switchdev_blocking_event(struct notifier_block *unused,
+					    unsigned long event, void *ptr)
+{
+	int err = 0;
+	struct net_device *net_dev = switchdev_notifier_info_to_dev(ptr);
+
+	MVSW_LOG_INFO("Processing switchdev blocking event [event=%s]",
+		      ENUM_TO_NAME(switchdev_notifier_type, event));
+
+	switch (event) {
+	case SWITCHDEV_PORT_OBJ_ADD:
+		if (netif_is_vxlan(net_dev)) {
+			/* TO DO: process vxlan event */
+			err = -EOPNOTSUPP;
+		} else {
+			err = switchdev_handle_port_obj_add
+			    (net_dev, ptr, mvsw_pr_netdev_check,
+			     mvsw_pr_port_obj_add);
+		}
+		break;
+	case SWITCHDEV_PORT_OBJ_DEL:
+		if (netif_is_vxlan(net_dev)) {
+			/* TO DO: process vxlan event */
+			err = -EOPNOTSUPP;
+		} else {
+			err = switchdev_handle_port_obj_del
+			    (net_dev, ptr, mvsw_pr_netdev_check,
+			     mvsw_pr_port_obj_del);
+		}
+		break;
+	case SWITCHDEV_PORT_ATTR_SET:
+		err = switchdev_handle_port_attr_set
+		    (net_dev, ptr, mvsw_pr_netdev_check,
+		    mvsw_pr_port_obj_attr_set);
+		break;
+	default:
+		err = -EOPNOTSUPP;
+	}
+
+	return notifier_from_errno(err);
+}
+
+static struct mvsw_pr_bridge_device *
+mvsw_pr_bridge_device_create(struct mvsw_pr_bridge *bridge,
+			     struct net_device *br_dev)
+{
+	struct mvsw_pr_bridge_device *bridge_device;
+	bool vlan_enabled = br_vlan_enabled(br_dev);
+	u16 bridge_id;
+	int err;
+
+	if (vlan_enabled && bridge->bridge_8021q_exists) {
+		pr_err("Only one VLAN-aware bridge is supported\n");
+		return ERR_PTR(-EINVAL);
+	}
+
+	bridge_device = kzalloc(sizeof(*bridge_device), GFP_KERNEL);
+	if (!bridge_device)
+		return ERR_PTR(-ENOMEM);
+
+	if (vlan_enabled) {
+		bridge->bridge_8021q_exists = true;
+	} else {
+		err = mvsw_pr_8021d_bridge_create(bridge->sw, &bridge_id);
+		if (err) {
+			kfree(bridge_device);
+			return ERR_PTR(err);
+		}
+
+		bridge_device->bridge_id = bridge_id;
+
+		MVSW_LOG_INFO("Created .1D Bridge [bridge_id=%d]", bridge_id);
+	}
+
+	bridge_device->dev = br_dev;
+	bridge_device->vlan_enabled = vlan_enabled;
+	bridge_device->multicast_enabled = br_multicast_enabled(br_dev);
+	bridge_device->mrouter = br_multicast_router(br_dev);
+	INIT_LIST_HEAD(&bridge_device->port_list);
+
+	list_add(&bridge_device->switch_node, &bridge->bridge_list);
+
+	return bridge_device;
+}
+
+static void
+mvsw_pr_bridge_device_destroy(struct mvsw_pr_bridge *bridge,
+			      struct mvsw_pr_bridge_device *bridge_device)
+{
+	MVSW_LOG_INFO("[bridge_device=%s]", bridge_device->dev->name);
+	list_del(&bridge_device->switch_node);
+	if (bridge_device->vlan_enabled)
+		bridge->bridge_8021q_exists = false;
+	else
+		mvsw_pr_8021d_bridge_delete(bridge->sw,
+					    bridge_device->bridge_id);
+
+	WARN_ON(!list_empty(&bridge_device->port_list));
+	kfree(bridge_device);
+}
+
+static struct mvsw_pr_bridge_device *
+mvsw_pr_bridge_device_get(struct mvsw_pr_bridge *bridge,
+			  struct net_device *br_dev)
+{
+	struct mvsw_pr_bridge_device *bridge_device;
+
+	bridge_device = mvsw_pr_bridge_device_find(bridge, br_dev);
+	if (bridge_device)
+		return bridge_device;
+
+	return mvsw_pr_bridge_device_create(bridge, br_dev);
+}
+
+static void
+mvsw_pr_bridge_device_put(struct mvsw_pr_bridge *bridge,
+			  struct mvsw_pr_bridge_device *bridge_device)
+{
+	if (list_empty(&bridge_device->port_list))
+		mvsw_pr_bridge_device_destroy(bridge, bridge_device);
+}
+
+static struct mvsw_pr_bridge_port *
+mvsw_pr_bridge_port_create(struct mvsw_pr_bridge_device *bridge_device,
+			   struct net_device *brport_dev)
+{
+	struct mvsw_pr_bridge_port *bridge_port;
+	struct mvsw_pr_port *mvsw_pr_port;
+
+	bridge_port = kzalloc(sizeof(*bridge_port), GFP_KERNEL);
+	if (!bridge_port)
+		return NULL;
+
+	mvsw_pr_port = mvsw_pr_port_dev_lower_find(brport_dev);
+
+	bridge_port->dev = brport_dev;
+	bridge_port->bridge_device = bridge_device;
+	bridge_port->stp_state = BR_STATE_DISABLED;
+	bridge_port->flags = BR_LEARNING | BR_FLOOD | BR_LEARNING_SYNC |
+			     BR_MCAST_FLOOD;
+	INIT_LIST_HEAD(&bridge_port->vlan_list);
+	list_add(&bridge_port->bridge_device_node, &bridge_device->port_list);
+	bridge_port->ref_count = 1;
+
+	return bridge_port;
+}
+
+static void
+mvsw_pr_bridge_port_destroy(struct mvsw_pr_bridge_port *bridge_port)
+{
+	list_del(&bridge_port->bridge_device_node);
+	WARN_ON(!list_empty(&bridge_port->vlan_list));
+	kfree(bridge_port);
+}
+
+static struct mvsw_pr_bridge_port *
+mvsw_pr_bridge_port_get(struct mvsw_pr_bridge *bridge,
+			struct net_device *brport_dev)
+{
+	struct net_device *br_dev = netdev_master_upper_dev_get(brport_dev);
+	struct mvsw_pr_bridge_device *bridge_device;
+	struct mvsw_pr_bridge_port *bridge_port;
+	int err;
+
+	bridge_port = mvsw_pr_bridge_port_find(bridge, brport_dev);
+	if (bridge_port) {
+		bridge_port->ref_count++;
+		return bridge_port;
+	}
+
+	bridge_device = mvsw_pr_bridge_device_get(bridge, br_dev);
+	if (IS_ERR(bridge_device))
+		return ERR_CAST(bridge_device);
+
+	bridge_port = mvsw_pr_bridge_port_create(bridge_device, brport_dev);
+	if (!bridge_port) {
+		err = -ENOMEM;
+		goto err_bridge_port_create;
+	}
+
+	return bridge_port;
+
+err_bridge_port_create:
+	mvsw_pr_bridge_device_put(bridge, bridge_device);
+	return ERR_PTR(err);
+}
+
+static void mvsw_pr_bridge_port_put(struct mvsw_pr_bridge *bridge,
+				    struct mvsw_pr_bridge_port *bridge_port)
+{
+	struct mvsw_pr_bridge_device *bridge_device;
+
+	if (--bridge_port->ref_count != 0)
+		return;
+	bridge_device = bridge_port->bridge_device;
+	mvsw_pr_bridge_port_destroy(bridge_port);
+	mvsw_pr_bridge_device_put(bridge, bridge_device);
+}
+
+static int
+mvsw_pr_bridge_8021q_port_join(struct mvsw_pr_bridge_device *bridge_device,
+			       struct mvsw_pr_bridge_port *bridge_port,
+			       struct mvsw_pr_port *mvsw_pr_port,
+			       struct netlink_ext_ack *extack)
+{
+	if (is_vlan_dev(bridge_port->dev)) {
+		NL_SET_ERR_MSG_MOD(extack,
+				   "Can not enslave a VLAN device to a VLAN-aware bridge");
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static int
+mvsw_pr_bridge_8021d_port_join(struct mvsw_pr_bridge_device *bridge_device,
+			       struct mvsw_pr_bridge_port *bridge_port,
+			       struct mvsw_pr_port *mvsw_pr_port,
+			       struct netlink_ext_ack *extack)
+{
+	int err;
+
+	if (is_vlan_dev(bridge_port->dev)) {
+		NL_SET_ERR_MSG_MOD(extack,
+				   "Enslaving of a VLAN device is not supported");
+		return -ENOTSUPP;
+	}
+	err = mvsw_pr_8021d_bridge_port_add(mvsw_pr_port,
+					    bridge_device->bridge_id);
+	if (err)
+		return err;
+
+	err = mvsw_pr_port_flood_set(mvsw_pr_port,
+				     bridge_port->flags & BR_FLOOD);
+	if (err)
+		goto err_port_flood_set;
+
+	err = mvsw_pr_port_learning_set(mvsw_pr_port,
+					bridge_port->flags & BR_LEARNING);
+	if (err)
+		goto err_port_learning_set;
+
+	return err;
+
+err_port_learning_set:
+	mvsw_pr_port_flood_set(mvsw_pr_port, false);
+err_port_flood_set:
+	mvsw_pr_8021d_bridge_port_delete(mvsw_pr_port,
+					 bridge_device->bridge_id);
+	return err;
+}
+
+static int mvsw_pr_port_bridge_join(struct mvsw_pr_port *mvsw_pr_port,
+				    struct net_device *brport_dev,
+				    struct net_device *br_dev,
+				    struct netlink_ext_ack *extack)
+{
+	struct mvsw_pr_switch *sw = mvsw_pr_port->sw;
+	struct mvsw_pr_bridge_device *bridge_device;
+	struct mvsw_pr_bridge_port *bridge_port;
+	int err;
+
+	bridge_port = mvsw_pr_bridge_port_get(sw->bridge, brport_dev);
+	if (IS_ERR(bridge_port))
+		return PTR_ERR(bridge_port);
+
+	bridge_device = bridge_port->bridge_device;
+
+	MVSW_LOG_INFO("[bridge_device=%s, bridge_port=%s, port=%s]",
+		      bridge_device->dev->name, bridge_port->dev->name,
+		      mvsw_pr_port->net_dev->name);
+
+	if (bridge_device->vlan_enabled) {
+		err = mvsw_pr_bridge_8021q_port_join(bridge_device, bridge_port,
+						     mvsw_pr_port, extack);
+	} else {
+		err = mvsw_pr_bridge_8021d_port_join(bridge_device, bridge_port,
+						     mvsw_pr_port, extack);
+	}
+
+	if (err)
+		goto err_port_join;
+
+	return 0;
+
+err_port_join:
+	mvsw_pr_bridge_port_put(sw->bridge, bridge_port);
+	return err;
+}
+
+static void
+mvsw_pr_bridge_8021d_port_leave(struct mvsw_pr_bridge_device *bridge_device,
+				struct mvsw_pr_bridge_port *bridge_port,
+				struct mvsw_pr_port *mvsw_pr_port)
+{
+	mvsw_pr_fdb_flush_port(mvsw_pr_port, MVSW_PR_FDB_FLUSH_MODE_ALL);
+	mvsw_pr_8021d_bridge_port_delete(mvsw_pr_port,
+					 bridge_device->bridge_id);
+}
+
+static void
+mvsw_pr_bridge_8021q_port_leave(struct mvsw_pr_bridge_device *bridge_device,
+				struct mvsw_pr_bridge_port *bridge_port,
+				struct mvsw_pr_port *mvsw_pr_port)
+{
+	mvsw_pr_fdb_flush_port(mvsw_pr_port, MVSW_PR_FDB_FLUSH_MODE_ALL);
+	mvsw_pr_port_pvid_set(mvsw_pr_port, MVSW_PR_DEFAULT_VID);
+}
+
+static void mvsw_pr_port_bridge_leave(struct mvsw_pr_port *mvsw_pr_port,
+				      struct net_device *brport_dev,
+				      struct net_device *br_dev)
+{
+	struct mvsw_pr_switch *sw = mvsw_pr_port->sw;
+	struct mvsw_pr_bridge_device *bridge_device;
+	struct mvsw_pr_bridge_port *bridge_port;
+
+	bridge_device = mvsw_pr_bridge_device_find(sw->bridge, br_dev);
+	if (!bridge_device)
+		return;
+	bridge_port = __mvsw_pr_bridge_port_find(bridge_device, brport_dev);
+	if (!bridge_port)
+		return;
+
+	MVSW_LOG_INFO("[bridge_device=%s, bridge_port=%s, port=%s]",
+		      bridge_device->dev->name, bridge_port->dev->name,
+		      mvsw_pr_port->net_dev->name);
+
+	if (bridge_device->vlan_enabled)
+		mvsw_pr_bridge_8021q_port_leave(bridge_device, bridge_port,
+						mvsw_pr_port);
+	else
+		mvsw_pr_bridge_8021d_port_leave(bridge_device, bridge_port,
+						mvsw_pr_port);
+
+	mvsw_pr_port_learning_set(mvsw_pr_port, false);
+	mvsw_pr_port_flood_set(mvsw_pr_port, false);
+	mvsw_pr_bridge_port_put(sw->bridge, bridge_port);
+}
+
+static int mvsw_pr_netdevice_port_upper_event(struct net_device *lower_dev,
+					      struct net_device *dev,
+					      unsigned long event, void *ptr)
+{
+	struct netdev_notifier_changeupper_info *info;
+	struct mvsw_pr_port *mvsw_pr_port;
+	struct netlink_ext_ack *extack;
+	struct net_device *upper_dev;
+	struct mvsw_pr_switch *mvsw_pr;
+	int err = 0;
+
+	mvsw_pr_port = netdev_priv(dev);
+	mvsw_pr = mvsw_pr_port->sw;
+	info = ptr;
+	extack = netdev_notifier_info_to_extack(&info->info);
+
+	switch (event) {
+	case NETDEV_PRECHANGEUPPER:
+		upper_dev = info->upper_dev;
+		if (!netif_is_bridge_master(upper_dev)) {
+			NL_SET_ERR_MSG_MOD(extack, "Unknown upper device type");
+			return -EINVAL;
+		}
+		if (!info->linking)
+			break;
+		if (netdev_has_any_upper_dev(upper_dev) &&
+		    (!netif_is_bridge_master(upper_dev) ||
+		     !mvsw_pr_bridge_device_is_offloaded(mvsw_pr, upper_dev))) {
+			NL_SET_ERR_MSG_MOD(extack,
+					   "Enslaving a port to a device that already has an upper device is not supported");
+			return -EINVAL;
+		}
+		break;
+	case NETDEV_CHANGEUPPER:
+		upper_dev = info->upper_dev;
+		if (netif_is_bridge_master(upper_dev)) {
+			if (info->linking)
+				err = mvsw_pr_port_bridge_join(mvsw_pr_port,
+							       lower_dev,
+							       upper_dev,
+							       extack);
+			else
+				mvsw_pr_port_bridge_leave(mvsw_pr_port,
+							  lower_dev,
+							  upper_dev);
+		}
+		break;
+	}
+
+	return err;
+}
+
+static int mvsw_pr_netdevice_port_event(struct net_device *lower_dev,
+					struct net_device *port_dev,
+					unsigned long event, void *ptr)
+{
+	switch (event) {
+	case NETDEV_PRECHANGEUPPER:
+	case NETDEV_CHANGEUPPER:
+		return mvsw_pr_netdevice_port_upper_event(lower_dev, port_dev,
+							  event, ptr);
+	case NETDEV_CHANGELOWERSTATE:
+		break;
+	}
+
+	return 0;
+}
+
+static int mvsw_pr_netdevice_bridge_event(struct net_device *br_dev,
+					  unsigned long event, void *ptr)
+{
+	struct mvsw_pr_switch *mvsw_pr = mvsw_pr_lower_get(br_dev);
+	struct netdev_notifier_changeupper_info *info = ptr;
+	struct netlink_ext_ack *extack;
+
+	MVSW_LOG_INFO("%s: Processing bridge event... [dev_name=%s]",
+		      __func__, br_dev->name);
+
+	if (!mvsw_pr)
+		return 0;
+
+	extack = netdev_notifier_info_to_extack(&info->info);
+
+	switch (event) {
+	case NETDEV_PRECHANGEUPPER:
+		/* TODO:  */
+		break;
+	case NETDEV_CHANGEUPPER:
+		/* TODO:  */
+		break;
+	}
+
+	return 0;
+}
+
+static int mvsw_pr_netdevice_event(struct notifier_block *nb,
+				   unsigned long event, void *ptr)
+{
+	struct net_device *dev = netdev_notifier_info_to_dev(ptr);
+	struct mvsw_pr_switch *sw;
+	int err = 0;
+
+	sw = container_of(nb, struct mvsw_pr_switch, netdevice_nb);
+
+	MVSW_LOG_INFO("Processing netdevice event [event=%s]",
+		      netdev_cmd_to_name(event));
+
+	if (mvsw_pr_netdev_check(dev))
+		err = mvsw_pr_netdevice_port_event(dev, dev, event, ptr);
+	else if (netif_is_bridge_master(dev))
+		err = mvsw_pr_netdevice_bridge_event(dev, event, ptr);
+
+	return notifier_from_errno(err);
+}
+
+static int mvsw_pr_fib_event(struct notifier_block *nb,
+			     unsigned long event, void *ptr)
+{
+	struct fib_notifier_info *info = ptr;
+	struct mvsw_pr_fib *fib;
+
+	fib = container_of(nb, struct mvsw_pr_fib, fib_nb);
+	MVSW_LOG_INFO("Processing FIB event [event=%s]\n",
+		      ENUM_TO_NAME(fib_event_type, event));
+
+	if (!net_eq(info->net, &init_net) ||
+	    (info->family != AF_INET && info->family != AF_INET6 &&
+	     info->family != RTNL_FAMILY_IPMR &&
+	     info->family != RTNL_FAMILY_IP6MR))
+		return NOTIFY_DONE;
+
+	switch (event) {
+	case FIB_EVENT_RULE_ADD:
+	case FIB_EVENT_RULE_DEL:
+		/* TO DO: process rule */
+		break;
+	case FIB_EVENT_ENTRY_ADD:
+	case FIB_EVENT_ENTRY_REPLACE:
+	case FIB_EVENT_ENTRY_APPEND:
+		/* TO DO: process entry */
+		break;
+	}
+
+	return NOTIFY_DONE;
+}
+
+static void mvsw_pr_fib_dump_flush(struct notifier_block *nb)
+{
+	struct mvsw_pr_fib *fib;
+
+	fib = container_of(nb, struct mvsw_pr_fib, fib_nb);
+	MVSW_LOG_INFO("Processing FIB dump/flush event");
+
+	/* TO DO: implement fib handler */
+}
+
+static int mvsw_pr_fib_netevent_event(struct notifier_block *nb,
+				      unsigned long event, void *ptr)
+{
+	struct mvsw_pr_fib *fib;
+
+	fib = container_of(nb, struct mvsw_pr_fib, netevent_nb);
+	MVSW_LOG_INFO("Processing fib netevent event [event=%s]",
+		      ENUM_TO_NAME(netevent_notif_type, event));
+
+	switch (event) {
+	case NETEVENT_DELAY_PROBE_TIME_UPDATE:
+	case NETEVENT_NEIGH_UPDATE:
+	case NETEVENT_IPV4_MPATH_HASH_UPDATE:
+	case NETEVENT_IPV6_MPATH_HASH_UPDATE:
+	case NETEVENT_IPV4_FWD_UPDATE_PRIORITY_UPDATE:
+		/* TO DO: process the event */
+		break;
+	}
+
+	return NOTIFY_DONE;
+}
+
+static int mvsw_pr_fdb_init(struct mvsw_pr_switch *sw)
+{
+	int err;
+
+	err = mvsw_pr_switch_ageing_set(sw, MVSW_PR_DEFAULT_AGEING_TIME);
+	if (err) {
+		MVSW_LOG_ERROR("Could not apply default ageing time");
+		return err;
+	}
+
+	return 0;
+}
+
+static int mvsw_pr_switchdev_init(struct mvsw_pr_switch *sw)
+{
+	int err = 0;
+	struct mvsw_pr_switchdev *swdev;
+	struct mvsw_pr_bridge *bridge;
+
+	if (sw->switchdev) {
+		MVSW_LOG_ERROR("Double register of swdev notifier");
+		return -EPERM;
+	}
+
+	bridge = kzalloc(sizeof(*sw->bridge), GFP_KERNEL);
+	if (!bridge)
+		return -ENOMEM;
+
+	swdev = kzalloc(sizeof(*sw->switchdev), GFP_KERNEL);
+	if (!swdev) {
+		kfree(bridge);
+		return -ENOMEM;
+	}
+
+	sw->bridge = bridge;
+	bridge->sw = sw;
+	sw->switchdev = swdev;
+	swdev->sw = sw;
+
+	INIT_LIST_HEAD(&sw->bridge->bridge_list);
+
+	mvsw_owq = alloc_ordered_workqueue("%s_ordered", 0, "prestera_sw");
+	if (!mvsw_owq) {
+		MVSW_LOG_ERROR("Failed to alloc workqueue");
+		err = -ENOMEM;
+		goto err_alloc_workqueue;
+	}
+
+	swdev->swdev_n.notifier_call = mvsw_pr_switchdev_event;
+	err = register_switchdev_notifier(&swdev->swdev_n);
+	if (err) {
+		MVSW_LOG_ERROR("Failed to register switchdev notifier");
+		goto err_register_switchdev_notifier;
+	}
+
+	swdev->swdev_blocking_n.notifier_call =
+			mvsw_pr_switchdev_blocking_event;
+	err = register_switchdev_blocking_notifier(&swdev->swdev_blocking_n);
+	if (err) {
+		MVSW_LOG_ERROR
+		    ("Failed to register switchdev blocking notifier");
+		goto err_register_block_switchdev_notifier;
+	}
+
+	mvsw_pr_fdb_init(sw);
+
+	return 0;
+
+err_register_block_switchdev_notifier:
+	unregister_switchdev_notifier(&swdev->swdev_n);
+err_register_switchdev_notifier:
+	destroy_workqueue(mvsw_owq);
+err_alloc_workqueue:
+	kfree(swdev);
+	kfree(bridge);
+	return err;
+}
+
+static void mvsw_pr_switchdev_fini(struct mvsw_pr_switch *sw)
+{
+	if (!sw->switchdev)
+		return;
+
+	unregister_switchdev_notifier(&sw->switchdev->swdev_n);
+	unregister_switchdev_blocking_notifier
+	    (&sw->switchdev->swdev_blocking_n);
+	flush_workqueue(mvsw_owq);
+	destroy_workqueue(mvsw_owq);
+	kfree(sw->switchdev);
+	sw->switchdev = NULL;
+	kfree(sw->bridge);
+}
+
+static int mvsw_pr_netdev_init(struct mvsw_pr_switch *sw)
+{
+	int err = 0;
+
+	if (sw->netdevice_nb.notifier_call) {
+		MVSW_LOG_ERROR("Double register of netdev notifier");
+		return -EPERM;
+	}
+
+	sw->netdevice_nb.notifier_call = mvsw_pr_netdevice_event;
+	err = register_netdevice_notifier(&sw->netdevice_nb);
+	return err;
+}
+
+static void mvsw_pr_netdev_fini(struct mvsw_pr_switch *sw)
+{
+	if (sw->netdevice_nb.notifier_call)
+		unregister_netdevice_notifier(&sw->netdevice_nb);
+}
+
+static int mvsw_pr_fib_init(struct mvsw_pr_switch *sw)
+{
+	struct mvsw_pr_fib *fib;
+	int err = 0;
+
+	if (sw->fib) {
+		MVSW_LOG_ERROR("Double register of fib notifier");
+		return -EPERM;
+	}
+
+	fib = kzalloc(sizeof(*sw->fib), GFP_KERNEL);
+	if (!fib)
+		return -ENOMEM;
+
+	fib->sw = sw;
+
+	fib->netevent_nb.notifier_call = mvsw_pr_fib_netevent_event;
+	err = register_netevent_notifier(&fib->netevent_nb);
+	if (err)
+		goto err_register_netevent_notifier;
+
+	fib->fib_nb.notifier_call = mvsw_pr_fib_event;
+	err = register_fib_notifier(&fib->fib_nb, mvsw_pr_fib_dump_flush);
+	if (err)
+		goto err_register_fib_notifier;
+
+	sw->fib = fib;
+	return 0;
+
+err_register_fib_notifier:
+	unregister_netevent_notifier(&fib->netevent_nb);
+err_register_netevent_notifier:
+	kfree(fib);
+	return err;
+}
+
+static void mvsw_pr_fib_fini(struct mvsw_pr_switch *sw)
+{
+	if (!sw->fib)
+		return;
+
+	unregister_fib_notifier(&sw->fib->fib_nb);
+	unregister_netevent_notifier(&sw->fib->netevent_nb);
+	kfree(sw->fib);
+	sw->fib = NULL;
+}
+
+int mvsw_pr_switchdev_register(struct mvsw_pr_switch *sw)
+{
+	int err;
+
+	err = mvsw_pr_switchdev_init(sw);
+	if (err) {
+		pr_err("Failed to initialize switchdev\n");
+		return err;
+	}
+
+	err = mvsw_pr_fib_init(sw);
+	if (err) {
+		MVSW_LOG_ERROR("Failed to initialize fib notifier");
+		goto err_fib_notifier;
+	}
+
+	err = mvsw_pr_netdev_init(sw);
+	if (err) {
+		pr_err("Failed to register netdev notifier\n");
+		goto err_netdevice_notifier;
+	}
+
+	return 0;
+
+err_netdevice_notifier:
+	mvsw_pr_fib_fini(sw);
+err_fib_notifier:
+	mvsw_pr_switchdev_fini(sw);
+	return err;
+}
+
+void mvsw_pr_switchdev_unregister(struct mvsw_pr_switch *sw)
+{
+	mvsw_pr_netdev_fini(sw);
+	mvsw_pr_fib_fini(sw);
+	mvsw_pr_switchdev_fini(sw);
+}
diff --git a/drivers/net/ethernet/marvell/prestera_sw/tests/prestera_ipc_tests.c b/drivers/net/ethernet/marvell/prestera_sw/tests/prestera_ipc_tests.c
new file mode 100644
index 0000000..e389ef5
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/tests/prestera_ipc_tests.c
@@ -0,0 +1,529 @@
+// SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+/*
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+#include <linux/cpu.h>
+#include <linux/slab.h>
+#include <linux/random.h>
+#include <linux/atomic.h>
+#include <linux/completion.h>
+#include <linux/kthread.h>
+
+#include "prestera_tests.h"
+
+struct test_send_req_cmd {
+	struct mvsw_pr_test_cmd cmd;
+	u32 data_sum;
+	u32 data_num;
+	u32 data[0];
+} __packed __aligned(4);
+
+struct test_evt_gen_cmd {
+	struct mvsw_pr_test_cmd cmd;
+	u32 loop_num;
+	u32 data_num;
+	u32 evt_type;
+	u32 delay;
+	u32 flags;
+} __packed __aligned(4);
+
+struct test_ipc_evt {
+	u16 type;
+	u16 id;
+	u32 len;
+} __packed __aligned(4);
+
+struct test_ipc_evt_sum {
+	struct test_ipc_evt evt;
+	u32 data_sum;
+	u32 data_num;
+	u32 data[0];
+} __packed __aligned(4);
+
+#define MVSW_PR_TEST_SUM_NUM	512
+
+int test_ipc_send_req_init(struct mvsw_pr_test *test)
+{
+	struct test_send_req_cmd *cmd;
+	size_t cmd_size;
+
+	cmd_size = sizeof(struct test_send_req_cmd) + MVSW_PR_TEST_SUM_NUM * 4;
+
+	test->priv = kmalloc(cmd_size, GFP_KERNEL);
+	if (!test->priv)
+		return -ENOMEM;
+
+	cmd = test->priv;
+
+	cmd->cmd.type = MVSW_PR_TEST_CMD_RUN;
+	cmd->cmd.id = MVSW_PR_TEST_ID_SUM;
+	cmd->cmd.len = cmd_size;
+
+	return 0;
+}
+
+int test_ipc_send_req_fini(struct mvsw_pr_test *test)
+{
+	kfree(test->priv);
+	return 0;
+}
+
+static int mvsw_pr_test_calc_sum(struct mvsw_pr_test *test,
+				 struct test_send_req_cmd *cmd)
+{
+	int i;
+
+	cmd->data_sum = 0;
+	cmd->data_num = 0;
+
+	while (!cmd->data_num) {
+		cmd->data_num = prandom_u32();
+		cmd->data_num = cmd->data_num % MVSW_PR_TEST_SUM_NUM;
+	}
+
+	for (i = 0; i < cmd->data_num; i++) {
+		cmd->data[i] = prandom_u32();
+		cmd->data_sum += cmd->data[i];
+	}
+
+	return test_send_cmd(&cmd->cmd);
+}
+
+int test_ipc_send_req_run(struct mvsw_pr_test *test)
+{
+	struct test_send_req_cmd *sum_cmd = test->priv;
+	int err;
+	int i;
+
+	for (i = 0; i < 1000; i++) {
+		err = mvsw_pr_test_calc_sum(test, sum_cmd);
+		if (err)
+			return err;
+	}
+
+	return 0;
+}
+
+struct mvsw_pr_test_ipc_async {
+	struct test_send_req_cmd **data;
+	struct task_struct **tasks;
+	struct completion tasks_done;
+	atomic_t tasks_pending;
+	int num_tasks;
+};
+
+struct test_ipc_async_task_ctx {
+	struct mvsw_pr_test *test;
+	int task_id;
+};
+
+static int test_ipc_send_req_async_task(void *arg)
+{
+	struct test_ipc_async_task_ctx *ctx = arg;
+	struct mvsw_pr_test_ipc_async *test_priv = ctx->test->priv;
+	struct test_send_req_cmd *sum_cmd = test_priv->data[ctx->task_id];
+	int err = 0;
+	int i;
+
+	for (i = 0; i < 1000; i++) {
+		err = mvsw_pr_test_calc_sum(ctx->test, sum_cmd);
+		if (err)
+			goto done;
+	}
+
+done:
+	if (atomic_dec_return_relaxed(&test_priv->tasks_pending) == 0)
+		complete(&test_priv->tasks_done);
+
+	for (;;) {
+		set_current_state(TASK_INTERRUPTIBLE);
+		if (kthread_should_park())
+			break;
+
+		schedule();
+	}
+
+	kthread_parkme();
+	return err;
+}
+
+int test_ipc_send_req_async_init(struct mvsw_pr_test *test)
+{
+	struct mvsw_pr_test_ipc_async *test_priv;
+	int num_cpus = num_online_cpus();
+	struct task_struct *task;
+	size_t cmd_size;
+	int cpu;
+	int err;
+
+	test_priv = kmalloc(sizeof(*test_priv), GFP_KERNEL);
+	if (!test_priv)
+		return -ENOMEM;
+
+	test_priv->num_tasks = 0;
+
+	test_priv->data = kmalloc_array(num_cpus, sizeof(*test_priv->data),
+					GFP_KERNEL);
+	if (!test_priv->data) {
+		err = -ENOMEM;
+		goto err_malloc_array;
+	}
+
+	memset(test_priv->data, 0, sizeof(*test_priv->data) * num_cpus);
+
+	cmd_size = sizeof(struct test_send_req_cmd) + MVSW_PR_TEST_SUM_NUM * 4;
+
+	for (cpu = 0; cpu < num_cpus; cpu++) {
+		test_priv->data[cpu] = kmalloc(cmd_size, GFP_KERNEL);
+		if (!test_priv->data[cpu]) {
+			err = -ENOMEM;
+			goto err_malloc_data;
+		}
+
+		test_priv->data[cpu]->cmd.type = MVSW_PR_TEST_CMD_RUN;
+		test_priv->data[cpu]->cmd.id = MVSW_PR_TEST_ID_SUM;
+		test_priv->data[cpu]->cmd.len = cmd_size;
+	}
+
+	test_priv->tasks = kmalloc_array(num_cpus, sizeof(*test_priv->tasks),
+					 GFP_KERNEL);
+	if (!test_priv->tasks) {
+		err = -ENOMEM;
+		goto err_create_task;
+	}
+
+	for_each_online_cpu(cpu) {
+		struct test_ipc_async_task_ctx *ctx;
+
+		ctx = kmalloc(sizeof(*ctx), GFP_KERNEL);
+
+		ctx->task_id = test_priv->num_tasks;
+		ctx->test = test;
+
+		task = kthread_create(test_ipc_send_req_async_task, ctx,
+				      "test_ipc_send_req_async_task%d",
+				      cpu);
+
+		if (IS_ERR(task)) {
+			pr_err("failed to create test thread\n");
+			kfree(ctx);
+		} else {
+			test_priv->tasks[test_priv->num_tasks++] = task;
+			kthread_bind(task, cpu);
+		}
+	}
+
+	atomic_set(&test_priv->tasks_pending, test_priv->num_tasks);
+	init_completion(&test_priv->tasks_done);
+	test->priv = test_priv;
+	return 0;
+
+err_create_task:
+err_malloc_data:
+	for (cpu = 0; cpu < num_cpus; cpu++)
+		kfree(test_priv->data[cpu]);
+	kfree(test_priv->data);
+err_malloc_array:
+	kfree(test_priv);
+	test->priv = NULL;
+	return err;
+}
+
+int test_ipc_send_req_async_fini(struct mvsw_pr_test *test)
+{
+	struct mvsw_pr_test_ipc_async *test_priv = test->priv;
+	int i;
+
+	if (!test_priv)
+		return 0;
+
+	for (i = 0; i < test_priv->num_tasks; i++)
+		kfree(test_priv->data[i]);
+
+	kfree(test_priv->tasks);
+	kfree(test_priv->data);
+	kfree(test_priv);
+	return 0;
+}
+
+int test_ipc_send_req_async_run(struct mvsw_pr_test *test)
+{
+	struct mvsw_pr_test_ipc_async *test_priv = test->priv;
+	int err = 0;
+	int i;
+
+	for (i = 0; i < test_priv->num_tasks; i++)
+		wake_up_process(test_priv->tasks[i]);
+
+	wait_for_completion(&test_priv->tasks_done);
+
+	for (i = 0; i < test_priv->num_tasks; i++) {
+		err |= kthread_park(test_priv->tasks[i]);
+		err |= kthread_stop(test_priv->tasks[i]);
+	}
+
+	return err;
+}
+
+struct test_ipc_evt_recv_ctx {
+	u32 evts_count;
+	u32 evts_recvd;
+	u32 bytes_recvd;
+	u32 evt_type;
+	u32 data_num;
+	u32 err;
+};
+
+int test_ipc_evt_recv_init(struct mvsw_pr_test *test)
+{
+	struct test_ipc_evt_recv_ctx *ctx;
+
+	ctx = kmalloc(sizeof(*ctx), GFP_KERNEL);
+	if (!ctx)
+		return -ENOMEM;
+
+	memset(ctx, 0, sizeof(*ctx));
+	test->priv = ctx;
+	return 0;
+}
+
+int test_ipc_evt_recv_fini(struct mvsw_pr_test *test)
+{
+	kfree(test->priv);
+	return 0;
+}
+
+static void test_ipc_evt_recv_fn(u8 *evt, size_t len, void *arg)
+{
+	struct test_ipc_evt_sum *evt_sum = (struct test_ipc_evt_sum *)evt;
+	struct mvsw_pr_test *test = arg;
+	struct test_ipc_evt_recv_ctx *ctx = test->priv;
+	u32 sum = 0;
+	u32 i;
+
+	if (evt_sum->evt.type != ctx->evt_type) {
+		pr_err("evt(%u): err: type: exp(%u), act(%u)\n",
+		       ctx->evts_recvd, ctx->evt_type, evt_sum->evt.type);
+		ctx->err++;
+		goto done;
+	}
+	if (evt_sum->evt.len != len) {
+		pr_err("evt(%u): err: len: exp(%u), act(%u)\n",
+		       ctx->evts_recvd, len, evt_sum->evt.len);
+		ctx->err++;
+		goto done;
+	}
+	if (evt_sum->data_num != ctx->data_num) {
+		pr_err("evt(%u): err: count: exp(%u), act(%u)\n",
+		       ctx->evts_recvd, ctx->data_num, evt_sum->data_num);
+		ctx->err++;
+		goto done;
+	}
+
+	for (i = 0; i < evt_sum->data_num; i++)
+		sum += evt_sum->data[i];
+
+	if (sum != evt_sum->data_sum) {
+		pr_err("evt(%u): err: sum: exp(%u), act(%u)\n",
+		       ctx->evts_recvd, evt_sum->data_sum, sum);
+		ctx->err++;
+	}
+
+done:
+	ctx->bytes_recvd += len;
+	ctx->evts_recvd++;
+}
+
+static bool test_ipc_evt_recv_check(struct mvsw_pr_test *test)
+{
+	struct test_ipc_evt_recv_ctx *ctx = test->priv;
+
+	return ctx->err ||
+		(ctx->evts_recvd == ctx->evts_count);
+}
+
+static int test_ipc_evt_recv_run(struct mvsw_pr_test *test, u32 num)
+{
+	struct test_ipc_evt_recv_ctx *ctx = test->priv;
+	struct test_evt_gen_cmd evt_cmd;
+	int err = 0;
+
+	evt_cmd.cmd.type = MVSW_PR_TEST_CMD_RUN;
+	evt_cmd.cmd.id = MVSW_PR_TEST_ID_EVT;
+	evt_cmd.cmd.len = sizeof(evt_cmd);
+
+	evt_cmd.loop_num = num;
+	evt_cmd.data_num = 128;
+	evt_cmd.evt_type = 1;
+	evt_cmd.delay = 0;
+	evt_cmd.flags = 0;
+
+	ctx->evts_count = evt_cmd.loop_num;
+	ctx->evt_type = evt_cmd.evt_type;
+	ctx->data_num = evt_cmd.data_num;
+
+	test_evt_handler_set(test_ipc_evt_recv_fn, (void *)test);
+
+	err = test_send_cmd(&evt_cmd.cmd);
+	if (err)
+		goto done;
+
+	test_wait_timeout(test_ipc_evt_recv_check(test), 60 * 1000);
+
+	if (ctx->err)
+		err = -EINVAL;
+	if (ctx->evts_recvd != ctx->evts_count || !ctx->evts_recvd)
+		err = -EINVAL;
+
+done:
+	test_evt_handler_set(NULL, NULL);
+	return err;
+}
+
+int test_ipc_evt_recv_one_run(struct mvsw_pr_test *test)
+{
+	return test_ipc_evt_recv_run(test, 1);
+}
+
+int test_ipc_evt_recv_overflow_run(struct mvsw_pr_test *test)
+{
+	return test_ipc_evt_recv_run(test, 100000);
+}
+
+#define TEST_IPC_EVT_TYPE_MAX	4
+
+struct test_ipc_evt_type_recv_ctx {
+	struct test_ipc_evt_recv_ctx evts[TEST_IPC_EVT_TYPE_MAX];
+	int count;
+	int err;
+};
+
+int test_ipc_evt_type_recv_async_init(struct mvsw_pr_test *test)
+{
+	struct test_ipc_evt_type_recv_ctx *ctx;
+
+	ctx = kmalloc(sizeof(*ctx), GFP_KERNEL);
+	if (!ctx)
+		return -ENOMEM;
+
+	memset(ctx, 0, sizeof(*ctx));
+	test->priv = ctx;
+	return 0;
+}
+
+int test_ipc_evt_type_recv_async_fini(struct mvsw_pr_test *test)
+{
+	kfree(test->priv);
+	return 0;
+}
+
+static void test_ipc_evt_type_recv_fn(u8 *evt, size_t len, void *arg)
+{
+	struct test_ipc_evt_sum *evt_sum = (struct test_ipc_evt_sum *)evt;
+	struct mvsw_pr_test *test = arg;
+	struct test_ipc_evt_type_recv_ctx *ctx = test->priv;
+	struct test_ipc_evt_recv_ctx *evt_ctx;
+	u32 sum = 0;
+	u32 i;
+
+	if (evt_sum->evt.type >= ctx->count) {
+		pr_err("evt: err: type: greater than %u\n", ctx->count);
+		ctx->err++;
+		return;
+	}
+
+	evt_ctx = &ctx->evts[evt_sum->evt.type];
+
+	if (evt_sum->evt.len != len) {
+		pr_err("evt(%u): err: len: exp(%u), act(%u)\n",
+		       evt_ctx->evts_recvd, len, evt_sum->evt.len);
+		ctx->err++;
+		goto done;
+	}
+	if (evt_sum->data_num != evt_ctx->data_num) {
+		pr_err("evt(%u): err: count: exp(%u), act(%u)\n",
+		       evt_ctx->evts_recvd, evt_ctx->data_num,
+		       evt_sum->data_num);
+		ctx->err++;
+		goto done;
+	}
+
+	for (i = 0; i < evt_sum->data_num; i++)
+		sum += evt_sum->data[i];
+
+	if (sum != evt_sum->data_sum) {
+		pr_err("evt(%u): err: sum: exp(%u), act(%u)\n",
+		       evt_ctx->evts_recvd, evt_sum->data_sum, sum);
+		ctx->err++;
+	}
+
+done:
+	evt_ctx->bytes_recvd += len;
+	evt_ctx->evts_recvd++;
+}
+
+static bool test_ipc_evt_type_recv_check(struct mvsw_pr_test *test, u32 type)
+{
+	struct test_ipc_evt_type_recv_ctx *ctx = test->priv;
+	struct test_ipc_evt_recv_ctx *evt_ctx = &ctx->evts[type];
+
+	return ctx->err ||
+		(evt_ctx->evts_recvd == evt_ctx->evts_count);
+}
+
+int test_ipc_evt_type_recv_async_run(struct mvsw_pr_test *test)
+{
+	struct test_ipc_evt_type_recv_ctx *ctx = test->priv;
+	struct test_evt_gen_cmd evt_cmd;
+	int count = TEST_IPC_EVT_TYPE_MAX;
+	int err = 0;
+	int i;
+
+	evt_cmd.cmd.type = MVSW_PR_TEST_CMD_RUN;
+	evt_cmd.cmd.id = MVSW_PR_TEST_ID_EVT;
+	evt_cmd.cmd.len = sizeof(evt_cmd);
+
+	evt_cmd.loop_num = 1000;
+	evt_cmd.data_num = 128;
+	evt_cmd.delay = 0;
+	evt_cmd.flags = 0;
+
+	ctx->count = count;
+
+	for (i = 0; i < count; i++) {
+		struct test_ipc_evt_recv_ctx *evt_ctx = &ctx->evts[i];
+
+		evt_cmd.evt_type = i;
+
+		evt_ctx->evts_count = evt_cmd.loop_num;
+		evt_ctx->evt_type = evt_cmd.evt_type;
+		evt_ctx->data_num = evt_cmd.data_num;
+
+		test_evt_handler_set(test_ipc_evt_type_recv_fn, (void *)test);
+
+		err = test_send_cmd(&evt_cmd.cmd);
+		if (err)
+			goto done;
+	}
+
+	for (i = 0; i < count; i++) {
+		struct test_ipc_evt_recv_ctx *evt_ctx = &ctx->evts[i];
+
+		test_wait_timeout(test_ipc_evt_type_recv_check(test, i),
+				  60 * 1000);
+
+		if (evt_ctx->evts_recvd != evt_ctx->evts_count ||
+		    !evt_ctx->evts_recvd) {
+			pr_err("(%d)evts_recvd: exp(%u), act(%u)\n",
+			       i, evt_ctx->evts_count, evt_ctx->evts_recvd);
+			err = -EINVAL;
+		} else if (evt_ctx->err) {
+			err = -EINVAL;
+		}
+	}
+
+done:
+	test_evt_handler_set(NULL, NULL);
+	return err;
+}
diff --git a/drivers/net/ethernet/marvell/prestera_sw/tests/prestera_tests.c b/drivers/net/ethernet/marvell/prestera_sw/tests/prestera_tests.c
new file mode 100644
index 0000000..40c29ee
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/tests/prestera_tests.c
@@ -0,0 +1,429 @@
+// SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+/*
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/debugfs.h>
+#include <linux/uaccess.h>
+
+#include "../prestera.h"
+#include "prestera_tests.h"
+
+#define MVSW_TESTS_DIR "mvsw_tests"
+
+static ssize_t mvsw_pr_test_status_fs_read(struct file *file,
+					   char __user *ubuf,
+					   size_t count, loff_t *ppos);
+static ssize_t mvsw_pr_test_list_fs_read(struct file *file,
+					 char __user *ubuf,
+					 size_t count, loff_t *ppos);
+static ssize_t mvsw_pr_test_select_fs_read(struct file *file,
+					   char __user *ubuf,
+					   size_t count, loff_t *ppos);
+static ssize_t mvsw_pr_test_select_fs_write(struct file *file,
+					    const char __user *ubuf,
+					    size_t count, loff_t *ppos);
+static ssize_t mvsw_pr_test_run_fs_write(struct file *file,
+					 const char __user *ubuf,
+					 size_t count, loff_t *ppos);
+
+static const struct file_operations fs_status_ops = {
+	.read = mvsw_pr_test_status_fs_read,
+	.open = simple_open,
+	.llseek = default_llseek,
+};
+
+static const struct file_operations fs_select_ops = {
+	.read = mvsw_pr_test_select_fs_read,
+	.write = mvsw_pr_test_select_fs_write,
+	.open = simple_open,
+	.llseek = default_llseek,
+};
+
+static const struct file_operations fs_list_ops = {
+	.read = mvsw_pr_test_list_fs_read,
+	.open = simple_open,
+	.llseek = default_llseek,
+};
+
+static const struct file_operations fs_run_ops = {
+	.write = mvsw_pr_test_run_fs_write,
+	.open = simple_open,
+	.llseek = default_llseek,
+};
+
+static void (*evt_recv_func)(u8 *evt, size_t len, void *arg);
+static void *evt_recv_arg;
+static DEFINE_SPINLOCK(evt_recv_lock);
+
+static struct mvsw_pr_test tests[] = {
+	{
+		.name = "test_ipc_send_req",
+		.init = test_ipc_send_req_init,
+		.fini = test_ipc_send_req_fini,
+		.run = test_ipc_send_req_run,
+	},
+	{
+		.name = "test_ipc_send_req_async",
+		.init = test_ipc_send_req_async_init,
+		.fini = test_ipc_send_req_async_fini,
+		.run = test_ipc_send_req_async_run,
+	},
+	{
+		.name = "test_ipc_evt_one",
+		.init = test_ipc_evt_recv_init,
+		.fini = test_ipc_evt_recv_fini,
+		.run = test_ipc_evt_recv_one_run,
+	},
+	{
+		.name = "test_ipc_evt_overflow",
+		.init = test_ipc_evt_recv_init,
+		.fini = test_ipc_evt_recv_fini,
+		.run = test_ipc_evt_recv_overflow_run,
+	},
+	{
+		.name = "test_ipc_evt_type_async",
+		.init = test_ipc_evt_type_recv_async_init,
+		.fini = test_ipc_evt_type_recv_async_fini,
+		.run = test_ipc_evt_type_recv_async_run,
+	},
+	{},
+};
+
+#define mvsw_pr_for_each_test(t) \
+	for (t = &tests[0]; (t)->name; (t)++)
+
+#define mvsw_pr_for_each_selected_test(t) \
+	for (t = &tests[0]; (t)->name; (t)++) \
+		if ((t)->is_selected)
+
+static struct mvsw_pr_device *test_device;
+
+static struct dentry *fs_tests_dir;
+static struct dentry *fs_status;
+static struct dentry *fs_select;
+static struct dentry *fs_list;
+static struct dentry *fs_run;
+
+static int mvsw_pr_test_init(struct mvsw_pr_test *test)
+{
+	int err = 0;
+
+	test->err = 0;
+
+	if (test->init) {
+		err = test->init(test);
+		if (err)
+			pr_err("test: %s: failed init\n", test->name);
+	}
+
+	return err;
+}
+
+static int mvsw_pr_test_fini(struct mvsw_pr_test *test)
+{
+	int err = 0;
+
+	if (test->fini) {
+		err = test->fini(test);
+		if (err)
+			pr_err("test: %s: failed fini\n", test->name);
+	}
+
+	return err;
+}
+
+static int mvsw_pr_test_run(struct mvsw_pr_test *test)
+{
+	test->flags = MVSW_PR_TEST_F_RUNNING;
+	test->err = 0;
+
+	test->err = test->run(test);
+	if (test->err)
+		pr_err("test: %s: [FAILED]\n", test->name);
+	else
+		pr_info("test: %s: [PASSED]\n", test->name);
+
+	test->flags |= MVSW_PR_TEST_F_FINISHED;
+
+	return test->err;
+}
+
+static ssize_t mvsw_pr_test_status_fs_read(struct file *file, char __user *ubuf,
+					   size_t count, loff_t *ppos)
+{
+	struct mvsw_pr_test *t;
+	char buf[1024];
+
+	buf[0] = '\0';
+
+	mvsw_pr_for_each_selected_test(t) {
+		if (!(t->flags & MVSW_PR_TEST_F_FINISHED))
+			continue;
+
+		strcat(buf, t->name);
+		if (t->err)
+			strcat(buf, "    FAIL");
+		else
+			strcat(buf, "    OK");
+
+		strcat(buf, "\n");
+	}
+
+	return simple_read_from_buffer(ubuf, count, ppos, buf, strlen(buf));
+}
+
+static ssize_t mvsw_pr_test_select_fs_read(struct file *file, char __user *ubuf,
+					   size_t count, loff_t *ppos)
+{
+	struct mvsw_pr_test *test;
+	char buf[1024];
+
+	buf[0] = '\0';
+
+	mvsw_pr_for_each_selected_test(test) {
+		strcat(buf, test->name);
+		strcat(buf, "\n");
+	}
+
+	return simple_read_from_buffer(ubuf, count, ppos, buf, strlen(buf));
+}
+
+static ssize_t mvsw_pr_test_select_fs_write(struct file *file,
+					    const char __user *ubuf,
+					    size_t count, loff_t *ppos)
+{
+	size_t min_len = 1024;
+	size_t len = min(count, min_len);
+	struct mvsw_pr_test *t;
+	bool select_all;
+	char buf[1024];
+	int err;
+
+	err = copy_from_user(buf, ubuf, len);
+	if (err)
+		return err;
+
+	buf[len - 1] = '\0';
+
+	/* clear selected tests */
+	if (strcmp("reset", buf) == 0) {
+		mvsw_pr_for_each_selected_test(t) {
+			if (!t->is_selected)
+				continue;
+
+			t->is_selected = false;
+			t->flags = 0;
+		}
+
+		goto exit;
+	}
+
+	select_all = strcmp(buf, "all") == 0;
+
+	mvsw_pr_for_each_test(t) {
+		if (select_all || strcmp(buf, t->name) == 0) {
+			if (t->is_selected)
+				continue;
+
+			t->is_selected = true;
+			t->flags = 0;
+		}
+	}
+
+exit:
+	return len;
+}
+
+static ssize_t mvsw_pr_test_run_fs_write(struct file *file,
+					 const char __user *ubuf,
+					 size_t count, loff_t *ppos)
+{
+	size_t min_len = 2;
+	size_t len = min(count, min_len);
+	struct mvsw_pr_test *t;
+	char buf[2];
+	int err;
+
+	err = copy_from_user(buf, ubuf, len);
+	if (err)
+		return err;
+
+	if (buf[0] == '1') {
+		mvsw_pr_for_each_selected_test(t) {
+			pr_info("test: %s running ...\n", t->name);
+
+			err = mvsw_pr_test_init(t);
+			if (err)
+				continue;
+
+			mvsw_pr_test_run(t);
+			mvsw_pr_test_fini(t);
+		}
+	}
+
+	return len;
+}
+
+static ssize_t mvsw_pr_test_list_fs_read(struct file *file, char __user *ubuf,
+					 size_t count, loff_t *ppos)
+{
+	struct mvsw_pr_test *t;
+	char buf[1024];
+
+	buf[0] = '\0';
+
+	mvsw_pr_for_each_test(t) {
+		strcat(buf, t->name);
+		strcat(buf, "\n");
+	}
+
+	return simple_read_from_buffer(ubuf, count, ppos, buf, strlen(buf));
+}
+
+int test_send_cmd(struct mvsw_pr_test_cmd *cmd)
+{
+	struct mvsw_pr_test_ret ret;
+	size_t ret_len;
+	int err;
+
+	err = test_device->bus->send_req(test_device, 0, (u8 *)cmd, cmd->len,
+					 (u8 *)&ret, sizeof(ret), &ret_len);
+	if (err)
+		return err;
+	if (ret_len < sizeof(ret))
+		return -EMSGSIZE;
+	if (ret.status != MVSW_PR_TEST_RET_OK)
+		return -EINVAL;
+
+	return 0;
+}
+
+void test_evt_handler_set(void (*recv)(u8 *buf, size_t len, void *arg),
+			  void *arg)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&evt_recv_lock, flags);
+	evt_recv_arg = arg;
+	evt_recv_func = recv;
+	spin_unlock_irqrestore(&evt_recv_lock, flags);
+}
+
+static int mvsw_pr_tests_setup(void)
+{
+	struct mvsw_pr_test_cmd cmd;
+	int err;
+
+	cmd.type = MVSW_PR_TEST_CMD_INIT;
+	cmd.len = sizeof(cmd);
+
+	err = test_send_cmd(&cmd);
+	if (err) {
+		pr_err("error while sending test init cmd\n");
+		return err;
+	}
+
+	fs_tests_dir = debugfs_create_dir("mvsw_pr_tests", NULL);
+	if (!fs_tests_dir) {
+		pr_err("failed to create tests dir\n");
+		return -EINVAL;
+	}
+
+	fs_status = debugfs_create_file("status", 0644,
+					fs_tests_dir, NULL,
+					&fs_status_ops);
+	if (!fs_status) {
+		debugfs_remove(fs_tests_dir);
+		return -EINVAL;
+	}
+
+	fs_list = debugfs_create_file("list", 0644,
+				      fs_tests_dir, NULL,
+				      &fs_list_ops);
+	if (!fs_list) {
+		debugfs_remove(fs_status);
+		debugfs_remove(fs_tests_dir);
+		return -EINVAL;
+	}
+
+	fs_select = debugfs_create_file("select", 0644,
+					fs_tests_dir, NULL,
+					&fs_select_ops);
+	if (!fs_select) {
+		debugfs_remove(fs_list);
+		debugfs_remove(fs_status);
+		debugfs_remove(fs_tests_dir);
+		return -EINVAL;
+	}
+
+	fs_run = debugfs_create_file("run", 0644,
+				     fs_tests_dir, NULL,
+				     &fs_run_ops);
+	if (!fs_run) {
+		debugfs_remove(fs_status);
+		debugfs_remove(fs_select);
+		debugfs_remove(fs_tests_dir);
+	}
+
+	return 0;
+}
+
+static int test_evt_recv_msg(struct mvsw_pr_device *dev, u8 *msg, size_t size)
+{
+	void (*func)(u8 *evt, size_t len, void *arg);
+	unsigned long flags;
+	void *arg;
+
+	spin_lock_irqsave(&evt_recv_lock, flags);
+	func = evt_recv_func;
+	arg = evt_recv_arg;
+	spin_unlock_irqrestore(&evt_recv_lock, flags);
+
+	if (func)
+		func(msg, size, arg);
+
+	return 0;
+}
+
+int mvsw_pr_device_register(struct mvsw_pr_device *dev)
+{
+	dev->recv_msg = test_evt_recv_msg;
+	test_device = dev;
+
+	return mvsw_pr_tests_setup();
+}
+EXPORT_SYMBOL(mvsw_pr_device_register);
+
+void mvsw_pr_device_unregister(struct mvsw_pr_device *dev)
+{
+	test_device = NULL;
+}
+EXPORT_SYMBOL(mvsw_pr_device_unregister);
+
+static int __init mvsw_pr_tests_init(void)
+{
+	pr_info("Loading Marvell Prestera testing module\n");
+
+	return 0;
+}
+
+static void __exit mvsw_pr_tests_exit(void)
+{
+	pr_info("Un-loading Marvell Prestera testing module\n");
+
+	debugfs_remove(fs_run);
+	debugfs_remove(fs_list);
+	debugfs_remove(fs_status);
+	debugfs_remove(fs_select);
+	debugfs_remove(fs_tests_dir);
+}
+
+module_init(mvsw_pr_tests_init);
+module_exit(mvsw_pr_tests_exit);
+
+MODULE_AUTHOR("Marvell Semi.");
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("Marvell Prestera switch driver");
diff --git a/drivers/net/ethernet/marvell/prestera_sw/tests/prestera_tests.h b/drivers/net/ethernet/marvell/prestera_sw/tests/prestera_tests.h
new file mode 100644
index 0000000..6489215
--- /dev/null
+++ b/drivers/net/ethernet/marvell/prestera_sw/tests/prestera_tests.h
@@ -0,0 +1,100 @@
+/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0
+ *
+ * Copyright (c) 2019-2020 Marvell International Ltd. All rights reserved.
+ *
+ */
+
+#ifndef _MVSW_PRESTERA_TESTS_H_
+#define _MVSW_PRESTERA_TESTS_H_
+
+#include <linux/kernel.h>
+
+#define test_wait_timeout(cond, waitms) \
+({ \
+	unsigned long __wait_end = jiffies + msecs_to_jiffies(waitms); \
+	bool __wait_ret = false; \
+	do { \
+		if (cond) { \
+			__wait_ret = true; \
+			break; \
+		} \
+		cond_resched(); \
+	} while (time_before(jiffies, __wait_end)); \
+	__wait_ret; \
+})
+
+enum {
+	MVSW_PR_TEST_RUNNING,
+	MVSW_PR_TEST_SUCCESS,
+	MVSW_PR_TEST_FAILED,
+};
+
+enum {
+	MVSW_PR_TEST_CMD_INIT,
+	MVSW_PR_TEST_CMD_RUN,
+};
+
+enum {
+	MVSW_PR_TEST_ID_SUM,
+	MVSW_PR_TEST_ID_EVT,
+};
+
+enum {
+	MVSW_PR_TEST_RET_OK,
+	MVSW_PR_TEST_RET_FAIL,
+};
+
+enum {
+	MVSW_PR_TEST_F_FINISHED = BIT(0),
+	MVSW_PR_TEST_F_RUNNING	= BIT(1),
+};
+
+struct mvsw_pr_test_ret {
+	u32 status;
+	u32 value;
+} __packed __aligned(4);
+
+struct mvsw_pr_test_cmd {
+	u16 type;
+	u16 id;
+	u32 len;
+} __packed __aligned(4);
+
+struct mvsw_pr_test {
+	const char *name;
+	bool is_selected;
+	u32 flags;
+
+	int (*init)(struct mvsw_pr_test *test);
+	int (*fini)(struct mvsw_pr_test *test);
+	int (*run)(struct mvsw_pr_test *test);
+
+	void *priv;
+	int err;
+};
+
+int test_send_cmd(struct mvsw_pr_test_cmd *cmd);
+
+void test_evt_handler_set(void (*recv)(u8 *buf, size_t len, void *arg),
+			  void *arg);
+
+/* tests declaration */
+
+int test_ipc_send_req_init(struct mvsw_pr_test *test);
+int test_ipc_send_req_fini(struct mvsw_pr_test *test);
+int test_ipc_send_req_run(struct mvsw_pr_test *test);
+
+int test_ipc_send_req_async_init(struct mvsw_pr_test *test);
+int test_ipc_send_req_async_fini(struct mvsw_pr_test *test);
+int test_ipc_send_req_async_run(struct mvsw_pr_test *test);
+
+int test_ipc_evt_recv_init(struct mvsw_pr_test *test);
+int test_ipc_evt_recv_fini(struct mvsw_pr_test *test);
+int test_ipc_evt_recv_one_run(struct mvsw_pr_test *test);
+int test_ipc_evt_recv_overflow_run(struct mvsw_pr_test *test);
+
+int test_ipc_evt_type_recv_async_init(struct mvsw_pr_test *test);
+int test_ipc_evt_type_recv_async_fini(struct mvsw_pr_test *test);
+int test_ipc_evt_type_recv_async_run(struct mvsw_pr_test *test);
+
+#endif /* _MVSW_PRESTERA_TESTS_H_ */
